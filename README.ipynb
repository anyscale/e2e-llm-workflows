{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Recognition with LLMs\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/🚀 Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/e2e-llm-workflows\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An end-to-end tutorial where we'll fine-tune an LLM to perform batch inference and online serving at scale. While entity recognition is the main task, we can easily extend these end-to-end workflows to any use case.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/e2e_llm.png\" width=800>\n",
    "\n",
    "**Note**: the intent of this tutorial is to show how Ray can be use to implement end-to-end LLM workflows that can extend to any use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "### Compute\n",
    "This [Anyscale Workspace](https://docs.anyscale.com/platform/workspaces/) will automatically provision and autoscale the compute our workloads will need. If you're not on Anyscale, then you will need to provision `4xA10G:48CPU-192GB` for this tutorial.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/compute.png\" width=500>\n",
    "\n",
    "### Dependencies\n",
    "Let's start by downloading the dependencies required for this tutorial. You'll notice in our [`containerfile`](https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/containerfile) we have a base image `FROM anyscale/ray:2.44.1-py312-cu125` followed by a list of pip packages. If you're not on any [Anyscale](https://console.anyscale.com/), you can pull this docker image yourself and install the dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully registered `ray, vllm` and 5 other packages to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_kvedZWag2qA8i5BjxUevf5i7/prj_cz951f43jjdybtzkx1s5sjgz99/workspaces/expwrk_mp8cxvgle2yeumgcpu1yua2r3e?workspace-tab=dependencies\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install dependencies\n",
    "pip install -q \\\n",
    "    \"ray[serve,llm]>=2.44.0\" \\\n",
    "    \"vllm>=0.7.2\" \\\n",
    "    \"xgrammar==0.1.11\" \\\n",
    "    \"pynvml==12.0.0\" \\\n",
    "    \"hf_transfer==0.1.9\" \\\n",
    "    \"tensorboard\" \\\n",
    "    \"llamafactory @ git+https://github.com/hiyouga/LLaMA-Factory.git#egg=llamafactory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import textwrap\n",
    "from IPython.display import Code, Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by downloading our data from cloud storage to local shared storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://viggo-ds/train.jsonl to ../../../mnt/cluster_storage/viggo/train.jsonl\n",
      "download: s3://viggo-ds/val.jsonl to ../../../mnt/cluster_storage/viggo/val.jsonl\n",
      "download: s3://viggo-ds/test.jsonl to ../../../mnt/cluster_storage/viggo/test.jsonl\n",
      "download: s3://viggo-ds/dataset_info.json to ../../../mnt/cluster_storage/viggo/dataset_info.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf /mnt/cluster_storage/viggo  # clean up\n",
    "aws s3 cp  s3://viggo-ds/train.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/val.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/test.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/dataset_info.json /mnt/cluster_storage/viggo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"instruction\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
      "    \"input\": \"Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.\",\n",
      "    \"output\": \"give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 1 /mnt/cluster_storage/viggo/train.jsonl | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a target sentence construct the underlying meaning representation of the\n",
      "input sentence as a single function with attributes and attribute values. This\n",
      "function should describe the target string accurately and the function must be\n",
      "one of the following ['inform', 'request', 'give_opinion', 'confirm',\n",
      "'verify_attribute', 'suggest', 'request_explanation', 'recommend',\n",
      "'request_attribute']. The attributes must be one of the following: ['name',\n",
      "'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres',\n",
      "'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam',\n",
      "'has_linux_release', 'has_mac_release', 'specifier']\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/cluster_storage/viggo/train.jsonl\", \"r\") as fp:\n",
    "    first_line = fp.readline()\n",
    "    item = json.loads(first_line)\n",
    "system_content = item[\"instruction\"]\n",
    "print(textwrap.fill(system_content, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have an info file that identifies the datasets and format --- alpaca and sharegpt (great for multimodal tasks) formats are supported --- to use for post training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"p\">{</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;viggo-train&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;file_name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;/mnt/cluster_storage/viggo/train.jsonl&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;formatting&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;alpaca&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;columns&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;prompt&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;instruction&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;query&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;response&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;output&quot;</span>\n",
       "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">},</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;viggo-val&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;file_name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;/mnt/cluster_storage/viggo/val.jsonl&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;formatting&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;alpaca&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;columns&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;prompt&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;instruction&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;query&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;response&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;output&quot;</span>\n",
       "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}viggo\\PYZhy{}train\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}file\\PYZus{}name\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}/mnt/cluster\\PYZus{}storage/viggo/train.jsonl\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}formatting\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}alpaca\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}columns\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}prompt\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}instruction\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}query\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}input\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}response\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}output\\PYZdq{}}\n",
       "\\PY{+w}{        }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{},}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}viggo\\PYZhy{}val\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}file\\PYZus{}name\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}/mnt/cluster\\PYZus{}storage/viggo/val.jsonl\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}formatting\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}alpaca\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}columns\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}prompt\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}instruction\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}query\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}input\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}response\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}output\\PYZdq{}}\n",
       "\\PY{+w}{        }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{p}{\\PYZcb{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "{\n",
       "    \"viggo-train\": {\n",
       "        \"file_name\": \"/mnt/cluster_storage/viggo/train.jsonl\",\n",
       "        \"formatting\": \"alpaca\",\n",
       "        \"columns\": {\n",
       "            \"prompt\": \"instruction\",\n",
       "            \"query\": \"input\",\n",
       "            \"response\": \"output\"\n",
       "        }\n",
       "    },\n",
       "    \"viggo-val\": {\n",
       "        \"file_name\": \"/mnt/cluster_storage/viggo/val.jsonl\",\n",
       "        \"formatting\": \"alpaca\",\n",
       "        \"columns\": {\n",
       "            \"prompt\": \"instruction\",\n",
       "            \"query\": \"input\",\n",
       "            \"response\": \"output\"\n",
       "        }\n",
       "    }\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"/mnt/cluster_storage/viggo/dataset_info.json\", language=\"json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [Ray Train](https://docs.ray.io/en/latest/train/train.html) + [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) to peform multinode training. The parameters for our training workload -- post-training method, dataset location, train/val details, etc. --- can be found in the `llama3_lora_sft_ray.yaml` config file. Check out recipes for even more post-training methods (sft, pretraining, ppo, dpo, kto, etc.) [here](https://github.com/hiyouga/LLaMA-Factory/tree/main/examples).\n",
    "\n",
    "**Note**: We also support using other tools like [axolotl](https://axolotl-ai-cloud.github.io/axolotl/docs/ray-integration.html) or even [Ray Train + HF Accelreate + FSDP/Deepspeed](https://docs.ray.io/en/latest/train/huggingface-accelerate.html) directly for complete control of your post-training workloads.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/distributed_training.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\">### model</span>\n",
       "<span class=\"nt\">model_name_or_path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">Qwen/Qwen2.5-7B-Instruct</span>\n",
       "<span class=\"nt\">trust_remote_code</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "\n",
       "<span class=\"c1\">### method</span>\n",
       "<span class=\"nt\">stage</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">sft</span>\n",
       "<span class=\"nt\">do_train</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">finetuning_type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">lora</span>\n",
       "<span class=\"nt\">lora_rank</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">8</span>\n",
       "<span class=\"nt\">lora_target</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">all</span>\n",
       "\n",
       "<span class=\"c1\">### dataset</span>\n",
       "<span class=\"nt\">dataset</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">viggo-train</span>\n",
       "<span class=\"nt\">dataset_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo</span><span class=\"w\">  </span><span class=\"c1\"># shared storage workers have access to</span>\n",
       "<span class=\"nt\">template</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">qwen</span>\n",
       "<span class=\"nt\">cutoff_len</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">2048</span>\n",
       "<span class=\"nt\">max_samples</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1000</span>\n",
       "<span class=\"nt\">overwrite_cache</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">preprocessing_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">16</span>\n",
       "<span class=\"nt\">dataloader_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">4</span>\n",
       "\n",
       "<span class=\"c1\">### output</span>\n",
       "<span class=\"nt\">output_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo/outputs</span><span class=\"w\">  </span><span class=\"c1\"># should be somewhere workers have access to (ex. s3, nfs)</span>\n",
       "<span class=\"nt\">logging_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">10</span>\n",
       "<span class=\"nt\">save_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">500</span>\n",
       "<span class=\"nt\">plot_loss</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">overwrite_output_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">save_only_model</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">false</span>\n",
       "\n",
       "<span class=\"c1\">### ray</span>\n",
       "<span class=\"nt\">ray_run_name</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">lora_sft_ray</span>\n",
       "<span class=\"nt\">ray_storage_path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo/saves</span><span class=\"w\">  </span><span class=\"c1\"># should be somewhere workers have access to (ex. s3, nfs)</span>\n",
       "<span class=\"nt\">ray_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">4</span>\n",
       "<span class=\"nt\">resources_per_worker</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">GPU</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">anyscale/accelerator_shape:4xA10G</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span><span class=\"w\">  </span><span class=\"c1\"># Use this to specify a specific node shape,</span>\n",
       "<span class=\"w\">  </span><span class=\"c1\"># accelerator_type:A10G: 1           # Or use this to simply specify a GPU type.</span>\n",
       "<span class=\"w\">  </span><span class=\"c1\"># see https://docs.ray.io/en/master/ray-core/accelerator-types.html#accelerator-types for a full list of accelerator types</span>\n",
       "<span class=\"nt\">placement_strategy</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">PACK</span>\n",
       "\n",
       "<span class=\"c1\">### train</span>\n",
       "<span class=\"nt\">per_device_train_batch_size</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"nt\">gradient_accumulation_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">8</span>\n",
       "<span class=\"nt\">learning_rate</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1.0e-4</span>\n",
       "<span class=\"nt\">num_train_epochs</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">5.0</span>\n",
       "<span class=\"nt\">lr_scheduler_type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">cosine</span>\n",
       "<span class=\"nt\">warmup_ratio</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">0.1</span>\n",
       "<span class=\"nt\">bf16</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">ddp_timeout</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">180000000</span>\n",
       "<span class=\"nt\">resume_from_checkpoint</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">null</span>\n",
       "\n",
       "<span class=\"c1\">### eval</span>\n",
       "<span class=\"nt\">eval_dataset</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">viggo-val</span><span class=\"w\">  </span><span class=\"c1\"># uses same dataset_dir as training data</span>\n",
       "<span class=\"c1\"># val_size: 0.1  # only if using part of training data for validation</span>\n",
       "<span class=\"nt\">per_device_eval_batch_size</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"nt\">eval_strategy</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">steps</span>\n",
       "<span class=\"nt\">eval_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">500</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} model}\n",
       "\\PY{n+nt}{model\\PYZus{}name\\PYZus{}or\\PYZus{}path}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{Qwen/Qwen2.5\\PYZhy{}7B\\PYZhy{}Instruct}\n",
       "\\PY{n+nt}{trust\\PYZus{}remote\\PYZus{}code}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} method}\n",
       "\\PY{n+nt}{stage}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{sft}\n",
       "\\PY{n+nt}{do\\PYZus{}train}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{finetuning\\PYZus{}type}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{lora}\n",
       "\\PY{n+nt}{lora\\PYZus{}rank}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{8}\n",
       "\\PY{n+nt}{lora\\PYZus{}target}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{all}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} dataset}\n",
       "\\PY{n+nt}{dataset}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{viggo\\PYZhy{}train}\n",
       "\\PY{n+nt}{dataset\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} shared storage workers have access to}\n",
       "\\PY{n+nt}{template}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{qwen}\n",
       "\\PY{n+nt}{cutoff\\PYZus{}len}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{2048}\n",
       "\\PY{n+nt}{max\\PYZus{}samples}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1000}\n",
       "\\PY{n+nt}{overwrite\\PYZus{}cache}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{preprocessing\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{16}\n",
       "\\PY{n+nt}{dataloader\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{4}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} output}\n",
       "\\PY{n+nt}{output\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo/outputs}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} should be somewhere workers have access to (ex. s3, nfs)}\n",
       "\\PY{n+nt}{logging\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{10}\n",
       "\\PY{n+nt}{save\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{500}\n",
       "\\PY{n+nt}{plot\\PYZus{}loss}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{overwrite\\PYZus{}output\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{save\\PYZus{}only\\PYZus{}model}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{false}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} ray}\n",
       "\\PY{n+nt}{ray\\PYZus{}run\\PYZus{}name}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{lora\\PYZus{}sft\\PYZus{}ray}\n",
       "\\PY{n+nt}{ray\\PYZus{}storage\\PYZus{}path}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo/saves}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} should be somewhere workers have access to (ex. s3, nfs)}\n",
       "\\PY{n+nt}{ray\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{4}\n",
       "\\PY{n+nt}{resources\\PYZus{}per\\PYZus{}worker}\\PY{p}{:}\n",
       "\\PY{+w}{  }\\PY{n+nt}{GPU}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{+w}{  }\\PY{n+nt}{anyscale/accelerator\\PYZus{}shape:4xA10G}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} Use this to specify a specific node shape,}\n",
       "\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} accelerator\\PYZus{}type:A10G: 1           \\PYZsh{} Or use this to simply specify a GPU type.}\n",
       "\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} see https://docs.ray.io/en/master/ray\\PYZhy{}core/accelerator\\PYZhy{}types.html\\PYZsh{}accelerator\\PYZhy{}types for a full list of accelerator types}\n",
       "\\PY{n+nt}{placement\\PYZus{}strategy}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{PACK}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} train}\n",
       "\\PY{n+nt}{per\\PYZus{}device\\PYZus{}train\\PYZus{}batch\\PYZus{}size}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{n+nt}{gradient\\PYZus{}accumulation\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{8}\n",
       "\\PY{n+nt}{learning\\PYZus{}rate}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1.0e\\PYZhy{}4}\n",
       "\\PY{n+nt}{num\\PYZus{}train\\PYZus{}epochs}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{5.0}\n",
       "\\PY{n+nt}{lr\\PYZus{}scheduler\\PYZus{}type}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{cosine}\n",
       "\\PY{n+nt}{warmup\\PYZus{}ratio}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{0.1}\n",
       "\\PY{n+nt}{bf16}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{ddp\\PYZus{}timeout}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{180000000}\n",
       "\\PY{n+nt}{resume\\PYZus{}from\\PYZus{}checkpoint}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{null}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} eval}\n",
       "\\PY{n+nt}{eval\\PYZus{}dataset}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{viggo\\PYZhy{}val}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} uses same dataset\\PYZus{}dir as training data}\n",
       "\\PY{c+c1}{\\PYZsh{} val\\PYZus{}size: 0.1  \\PYZsh{} only if using part of training data for validation}\n",
       "\\PY{n+nt}{per\\PYZus{}device\\PYZus{}eval\\PYZus{}batch\\PYZus{}size}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{n+nt}{eval\\PYZus{}strategy}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{steps}\n",
       "\\PY{n+nt}{eval\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{500}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "### model\n",
       "model_name_or_path: Qwen/Qwen2.5-7B-Instruct\n",
       "trust_remote_code: true\n",
       "\n",
       "### method\n",
       "stage: sft\n",
       "do_train: true\n",
       "finetuning_type: lora\n",
       "lora_rank: 8\n",
       "lora_target: all\n",
       "\n",
       "### dataset\n",
       "dataset: viggo-train\n",
       "dataset_dir: /mnt/cluster_storage/viggo  # shared storage workers have access to\n",
       "template: qwen\n",
       "cutoff_len: 2048\n",
       "max_samples: 1000\n",
       "overwrite_cache: true\n",
       "preprocessing_num_workers: 16\n",
       "dataloader_num_workers: 4\n",
       "\n",
       "### output\n",
       "output_dir: /mnt/cluster_storage/viggo/outputs  # should be somewhere workers have access to (ex. s3, nfs)\n",
       "logging_steps: 10\n",
       "save_steps: 500\n",
       "plot_loss: true\n",
       "overwrite_output_dir: true\n",
       "save_only_model: false\n",
       "\n",
       "### ray\n",
       "ray_run_name: lora_sft_ray\n",
       "ray_storage_path: /mnt/cluster_storage/viggo/saves  # should be somewhere workers have access to (ex. s3, nfs)\n",
       "ray_num_workers: 4\n",
       "resources_per_worker:\n",
       "  GPU: 1\n",
       "  anyscale/accelerator_shape:4xA10G: 1  # Use this to specify a specific node shape,\n",
       "  # accelerator_type:A10G: 1           # Or use this to simply specify a GPU type.\n",
       "  # see https://docs.ray.io/en/master/ray-core/accelerator-types.html#accelerator-types for a full list of accelerator types\n",
       "placement_strategy: PACK\n",
       "\n",
       "### train\n",
       "per_device_train_batch_size: 1\n",
       "gradient_accumulation_steps: 8\n",
       "learning_rate: 1.0e-4\n",
       "num_train_epochs: 5.0\n",
       "lr_scheduler_type: cosine\n",
       "warmup_ratio: 0.1\n",
       "bf16: true\n",
       "ddp_timeout: 180000000\n",
       "resume_from_checkpoint: null\n",
       "\n",
       "### eval\n",
       "eval_dataset: viggo-val  # uses same dataset_dir as training data\n",
       "# val_size: 0.1  # only if using part of training data for validation\n",
       "per_device_eval_batch_size: 1\n",
       "eval_strategy: steps\n",
       "eval_steps: 500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"lora_sft_ray.yaml\", language=\"yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen2.5-7B-Instruct\n"
     ]
    }
   ],
   "source": [
    "model_id = \"ft-model\"  # call it whatever you want\n",
    "model_source = yaml.safe_load(open(\"lora_sft_ray.yaml\"))[\"model_name_or_path\"]  # HF model ID, S3 mirror config, or GCS mirror config\n",
    "print (model_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-11 14:47:34 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 14:47:37,753\tINFO worker.py:1660 -- Connecting to existing Ray cluster at address: 10.0.51.51:6379...\n",
      "2025-04-11 14:47:37,763\tINFO worker.py:1843 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-zt5t77xa58pyp3uy28glg2g24d.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-04-11 14:47:37,768\tINFO packaging.py:367 -- Pushing file package 'gcs://_ray_pkg_92dfd12481e31b6eb199fb90a473373c5d0517d0.zip' (2.07MiB) to Ray cluster...\n",
      "2025-04-11 14:47:37,776\tINFO packaging.py:380 -- Successfully pushed file package 'gcs://_ray_pkg_92dfd12481e31b6eb199fb90a473373c5d0517d0.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "View detailed results here: /mnt/cluster_storage/viggo/saves/lora_sft_ray\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-04-11_12-25-06_198434_2329/artifacts/2025-04-11_14-47-37/lora_sft_ray/driver_artifacts`\n",
      "\u001b[36m(autoscaler +5s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[36m(autoscaler +5s)\u001b[0m [autoscaler] [4xA10G:48CPU-192GB] Attempting to add 4 node(s) to the cluster (increasing from 0 to 4).\n",
      "\u001b[36m(autoscaler +10s)\u001b[0m [autoscaler] [4xA10G:48CPU-192GB] Launched 4 instances.\n",
      "\u001b[36m(autoscaler +1m10s)\u001b[0m [autoscaler] Cluster upscaled to {144 CPU, 12 GPU}.\n",
      "\u001b[36m(autoscaler +1m20s)\u001b[0m [autoscaler] Cluster upscaled to {192 CPU, 16 GPU}.\n",
      "\n",
      "Training started with configuration:\n",
      "╭──────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training config                                                                                      │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ train_loop_config/args/bf16                                                                     True │\n",
      "│ train_loop_config/args/cutoff_len                                                               2048 │\n",
      "│ train_loop_config/args/dataloader_num_workers                                                      4 │\n",
      "│ train_loop_config/args/dataset                                                           viggo-train │\n",
      "│ train_loop_config/args/dataset_dir                                              ...ter_storage/viggo │\n",
      "│ train_loop_config/args/ddp_timeout                                                         180000000 │\n",
      "│ train_loop_config/args/do_train                                                                 True │\n",
      "│ train_loop_config/args/eval_dataset                                                        viggo-val │\n",
      "│ train_loop_config/args/eval_steps                                                                500 │\n",
      "│ train_loop_config/args/eval_strategy                                                           steps │\n",
      "│ train_loop_config/args/finetuning_type                                                          lora │\n",
      "│ train_loop_config/args/gradient_accumulation_steps                                                 8 │\n",
      "│ train_loop_config/args/learning_rate                                                          0.0001 │\n",
      "│ train_loop_config/args/logging_steps                                                              10 │\n",
      "│ train_loop_config/args/lora_rank                                                                   8 │\n",
      "│ train_loop_config/args/lora_target                                                               all │\n",
      "│ train_loop_config/args/lr_scheduler_type                                                      cosine │\n",
      "│ train_loop_config/args/max_samples                                                              1000 │\n",
      "│ train_loop_config/args/model_name_or_path                                       ...en2.5-7B-Instruct │\n",
      "│ train_loop_config/args/num_train_epochs                                                          5.0 │\n",
      "│ train_loop_config/args/output_dir                                               ...age/viggo/outputs │\n",
      "│ train_loop_config/args/overwrite_cache                                                          True │\n",
      "│ train_loop_config/args/overwrite_output_dir                                                     True │\n",
      "│ train_loop_config/args/per_device_eval_batch_size                                                  1 │\n",
      "│ train_loop_config/args/per_device_train_batch_size                                                 1 │\n",
      "│ train_loop_config/args/placement_strategy                                                       PACK │\n",
      "│ train_loop_config/args/plot_loss                                                                True │\n",
      "│ train_loop_config/args/preprocessing_num_workers                                                  16 │\n",
      "│ train_loop_config/args/ray_num_workers                                                             4 │\n",
      "│ train_loop_config/args/ray_run_name                                                     lora_sft_ray │\n",
      "│ train_loop_config/args/ray_storage_path                                         ...orage/viggo/saves │\n",
      "│ train_loop_config/args/resources_per_worker/GPU                                                    1 │\n",
      "│ train_loop_config/args/resources_per_worker/anyscale/accelerator_shape:4xA10G                      1 │\n",
      "│ train_loop_config/args/resume_from_checkpoint                                                        │\n",
      "│ train_loop_config/args/save_only_model                                                         False │\n",
      "│ train_loop_config/args/save_steps                                                                500 │\n",
      "│ train_loop_config/args/stage                                                                     sft │\n",
      "│ train_loop_config/args/template                                                                 qwen │\n",
      "│ train_loop_config/args/trust_remote_code                                                        True │\n",
      "│ train_loop_config/args/warmup_ratio                                                              0.1 │\n",
      "│ train_loop_config/callbacks                                                     ... 0x7e1262910e10>] │\n",
      "╰──────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(TorchTrainer pid=3542, ip=10.0.43.196)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=3542, ip=10.0.43.196)\u001b[0m - (node_id=61ef86792074e157decb04aa5faedf0599ddccc3b80160cc438180f6, ip=10.0.43.196, pid=3642) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=3542, ip=10.0.43.196)\u001b[0m - (node_id=37efbd8717d58d4f73129d108f3d5a40e5601eed33b5a23fab9af900, ip=10.0.46.205, pid=3407) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(TorchTrainer pid=3542, ip=10.0.43.196)\u001b[0m - (node_id=6441c36a31e630f348b2ebf6ced9a0430984987bab9dc55260429f50, ip=10.0.33.71, pid=3422) world_rank=2, local_rank=0, node_rank=2\n",
      "\u001b[36m(TorchTrainer pid=3542, ip=10.0.43.196)\u001b[0m - (node_id=9e38d9685dceeb0086537b38003bf61b40836a10d8b0272e32e63656, ip=10.0.16.199, pid=3399) world_rank=3, local_rank=0, node_rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [WARNING|2025-04-11 14:49:30] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|2025-04-11 14:49:30] llamafactory.hparams.parser:379 >> Process rank: 1, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:31,890 >> loading file vocab.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/vocab.json\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:31,890 >> loading file merges.txt from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/merges.txt\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:31,890 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:31,890 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:31,891 >> loading file special_tokens_map.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:31,891 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:31,891 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|tokenization_utils_base.py:2313] 2025-04-11 14:49:32,201 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|configuration_utils.py:699] 2025-04-11 14:49:32,639 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|configuration_utils.py:771] 2025-04-11 14:49:32,640 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"_name_or_path\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"transformers_version\": \"4.49.0\",\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|2025-04-11 14:49:33] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|2025-04-11 14:49:33] llamafactory.data.loader:143 >> Loading dataset /mnt/cluster_storage/viggo/train.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 10206 examples [00:00, 100953.16 examples/s]\n",
      "Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Converting format of dataset (num_proc=16):  25%|██▌       | 252/1000 [00:00<00:00, 2342.02 examples/s]\n",
      "Converting format of dataset (num_proc=16):  69%|██████▉   | 690/1000 [00:00<00:00, 1781.47 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4694.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|2025-04-11 14:49:34] llamafactory.data.loader:143 >> Loading dataset /mnt/cluster_storage/viggo/val.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 714 examples [00:00, 92888.74 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 1372.46 examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [rank2]:[W411 14:49:35.921940742 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:33,287 >> loading file vocab.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/vocab.json\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:33,287 >> loading file merges.txt from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/merges.txt\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:33,287 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer.json\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:33,287 >> loading file added_tokens.json from cache at None\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:33,287 >> loading file special_tokens_map.json from cache at None\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:33,287 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer_config.json\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2050] 2025-04-11 14:49:33,287 >> loading file chat_template.jinja from cache at None\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:09, 100.48 examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2313] 2025-04-11 14:49:33,574 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|configuration_utils.py:699] 2025-04-11 14:49:33,188 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|configuration_utils.py:771] 2025-04-11 14:49:33,190 >> Model config Qwen2Config {\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"_name_or_path\": \"Qwen/Qwen2.5-7B-Instruct\",\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"architectures\": [\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   ],\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"attention_dropout\": 0.0,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"bos_token_id\": 151643,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"eos_token_id\": 151645,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"hidden_act\": \"silu\",\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"hidden_size\": 3584,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"initializer_range\": 0.02,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"intermediate_size\": 18944,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"max_position_embeddings\": 32768,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"max_window_layers\": 28,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"model_type\": \"qwen2\",\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"num_attention_heads\": 28,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"num_hidden_layers\": 28,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"num_key_value_heads\": 4,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"rms_norm_eps\": 1e-06,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"rope_scaling\": null,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"rope_theta\": 1000000.0,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"sliding_window\": 131072,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"tie_word_embeddings\": false,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"torch_dtype\": \"bfloat16\",\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"transformers_version\": \"4.49.0\",\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"use_cache\": true,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"use_sliding_window\": false,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"vocab_size\": 152064\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m }\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Generating train split: 10206 examples [00:00, 100706.87 examples/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 938/1000 [00:01<00:00, 705.55 examples/s]\n",
      "Converting format of dataset (num_proc=16):   0%|          | 0/714 [00:00<?, ? examples/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m training example:\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 22043, 264, 2169, 11652, 9245, 279, 16533, 7290, 13042, 315, 279, 1946, 11652, 438, 264, 3175, 729, 448, 8201, 323, 7035, 2750, 13, 1096, 729, 1265, 7512, 279, 2169, 914, 29257, 323, 279, 729, 1969, 387, 825, 315, 279, 2701, 2509, 40440, 516, 364, 2035, 516, 364, 46430, 10287, 36300, 516, 364, 13800, 516, 364, 12446, 16791, 516, 364, 95761, 516, 364, 2035, 2702, 35890, 516, 364, 66589, 516, 364, 2035, 16791, 7204, 576, 8201, 1969, 387, 825, 315, 279, 2701, 25, 2509, 606, 516, 364, 4580, 24577, 4164, 516, 364, 22998, 14645, 516, 364, 34401, 516, 364, 288, 10681, 516, 364, 21931, 516, 364, 63911, 516, 364, 3434, 620, 85091, 516, 364, 4648, 25133, 3434, 516, 364, 15734, 82, 516, 364, 10334, 4470, 1261, 14580, 516, 364, 4648, 77463, 24577, 516, 364, 4648, 22802, 24577, 516, 364, 67251, 4432, 4923, 38148, 4787, 374, 10008, 458, 16910, 15754, 11, 714, 807, 5880, 73237, 7946, 369, 279, 7401, 323, 773, 429, 40702, 279, 1809, 504, 16910, 311, 1661, 304, 847, 1651, 13, 151645, 198, 151644, 77091, 198, 46430, 10287, 36300, 3153, 58, 21685, 31460, 7946, 1125, 15754, 58, 4923, 38148, 4787, 1125, 10728, 58, 18536, 1125, 702, 22802, 24577, 58, 9693, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m inputs:\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m <|im_start|>system\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 46430, 10287, 36300, 3153, 58, 21685, 31460, 7946, 1125, 15754, 58, 4923, 38148, 4787, 1125, 10728, 58, 18536, 1125, 702, 22802, 24577, 58, 9693, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m labels:\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [WARNING|2025-04-11 14:49:30] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|2025-04-11 14:49:30] llamafactory.hparams.parser:379 >> Process rank: 2, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|2025-04-11 14:49:33] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|2025-04-11 14:49:33] llamafactory.data.loader:143 >> Loading dataset /mnt/cluster_storage/viggo/train.jsonl...\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 22043, 264, 2169, 11652, 9245, 279, 16533, 7290, 13042, 315, 279, 1946, 11652, 438, 264, 3175, 729, 448, 8201, 323, 7035, 2750, 13, 1096, 729, 1265, 7512, 279, 2169, 914, 29257, 323, 279, 729, 1969, 387, 825, 315, 279, 2701, 2509, 40440, 516, 364, 2035, 516, 364, 46430, 10287, 36300, 516, 364, 13800, 516, 364, 12446, 16791, 516, 364, 95761, 516, 364, 2035, 2702, 35890, 516, 364, 66589, 516, 364, 2035, 16791, 7204, 576, 8201, 1969, 387, 825, 315, 279, 2701, 25, 2509, 606, 516, 364, 4580, 24577, 4164, 516, 364, 22998, 14645, 516, 364, 34401, 516, 364, 288, 10681, 516, 364, 21931, 516, 364, 63911, 516, 364, 3434, 620, 85091, 516, 364, 4648, 25133, 3434, 516, 364, 15734, 82, 516, 364, 10334, 4470, 1261, 14580, 516, 364, 4648, 77463, 24577, 516, 364, 4648, 22802, 24577, 516, 364, 67251, 4432, 4923, 38148, 4787, 374, 10008, 458, 16910, 15754, 11, 714, 807, 5880, 73237, 7946, 369, 279, 7401, 323, 773, 429, 40702, 279, 1809, 504, 16910, 311, 1661, 304, 847, 1651, 13, 151645, 198, 151644, 77091, 198, 46430, 10287, 36300, 3153, 58, 21685, 31460, 7946, 1125, 15754, 58, 4923, 38148, 4787, 1125, 10728, 58, 18536, 1125, 702, 22802, 24577, 58, 9693, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 46430, 10287, 36300, 3153, 58, 21685, 31460, 7946, 1125, 15754, 58, 4923, 38148, 4787, 1125, 10728, 58, 18536, 1125, 702, 22802, 24577, 58, 9693, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 22043, 264, 2169, 11652, 9245, 279, 16533, 7290, 13042, 315, 279, 1946, 11652, 438, 264, 3175, 729, 448, 8201, 323, 7035, 2750, 13, 1096, 729, 1265, 7512, 279, 2169, 914, 29257, 323, 279, 729, 1969, 387, 825, 315, 279, 2701, 2509, 40440, 516, 364, 2035, 516, 364, 46430, 10287, 36300, 516, 364, 13800, 516, 364, 12446, 16791, 516, 364, 95761, 516, 364, 2035, 2702, 35890, 516, 364, 66589, 516, 364, 2035, 16791, 7204, 576, 8201, 1969, 387, 825, 315, 279, 2701, 25, 2509, 606, 516, 364, 4580, 24577, 4164, 516, 364, 22998, 14645, 516, 364, 34401, 516, 364, 288, 10681, 516, 364, 21931, 516, 364, 63911, 516, 364, 3434, 620, 85091, 516, 364, 4648, 25133, 3434, 516, 364, 15734, 82, 516, 364, 10334, 4470, 1261, 14580, 516, 364, 4648, 77463, 24577, 516, 364, 4648, 22802, 24577, 516, 364, 67251, 4432, 4923, 38148, 4787, 374, 10008, 458, 16910, 15754, 11, 714, 807, 5880, 73237, 7946, 369, 279, 7401, 323, 773, 429, 40702, 279, 1809, 504, 16910, 311, 1661, 304, 847, 1651, 13, 151645, 198, 151644, 77091, 198, 46430, 10287, 36300, 3153, 58, 21685, 31460, 7946, 1125, 15754, 58, 4923, 38148, 4787, 1125, 10728, 58, 18536, 1125, 702, 22802, 24577, 58, 9693, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 46430, 10287, 36300, 3153, 58, 21685, 31460, 7946, 1125, 15754, 58, 4923, 38148, 4787, 1125, 10728, 58, 18536, 1125, 702, 22802, 24577, 58, 9693, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 22043, 264, 2169, 11652, 9245, 279, 16533, 7290, 13042, 315, 279, 1946, 11652, 438, 264, 3175, 729, 448, 8201, 323, 7035, 2750, 13, 1096, 729, 1265, 7512, 279, 2169, 914, 29257, 323, 279, 729, 1969, 387, 825, 315, 279, 2701, 2509, 40440, 516, 364, 2035, 516, 364, 46430, 10287, 36300, 516, 364, 13800, 516, 364, 12446, 16791, 516, 364, 95761, 516, 364, 2035, 2702, 35890, 516, 364, 66589, 516, 364, 2035, 16791, 7204, 576, 8201, 1969, 387, 825, 315, 279, 2701, 25, 2509, 606, 516, 364, 4580, 24577, 4164, 516, 364, 22998, 14645, 516, 364, 34401, 516, 364, 288, 10681, 516, 364, 21931, 516, 364, 63911, 516, 364, 3434, 620, 85091, 516, 364, 4648, 25133, 3434, 516, 364, 15734, 82, 516, 364, 10334, 4470, 1261, 14580, 516, 364, 4648, 77463, 24577, 516, 364, 4648, 22802, 24577, 516, 364, 67251, 4432, 4923, 38148, 4787, 374, 10008, 458, 16910, 15754, 11, 714, 807, 5880, 73237, 7946, 369, 279, 7401, 323, 773, 429, 40702, 279, 1809, 504, 16910, 311, 1661, 304, 847, 1651, 13, 151645, 198, 151644, 77091, 198, 46430, 10287, 36300, 3153, 58, 21685, 31460, 7946, 1125, 15754, 58, 4923, 38148, 4787, 1125, 10728, 58, 18536, 1125, 702, 22802, 24577, 58, 9693, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 46430, 10287, 36300, 3153, 58, 21685, 31460, 7946, 1125, 15754, 58, 4923, 38148, 4787, 1125, 10728, 58, 18536, 1125, 702, 22802, 24577, 58, 9693, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset (num_proc=16):  32%|███▏      | 225/714 [00:00<00:00, 1757.86 examples/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 714/714 [00:00<00:00, 2957.84 examples/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Generating train split: 714 examples [00:00, 96885.57 examples/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [rank1]:[W411 14:49:35.520539128 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m eval example:\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|2025-04-11 14:49:35] llamafactory.data.loader:143 >> Loading dataset /mnt/cluster_storage/viggo/val.jsonl...\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 22043, 264, 2169, 11652, 9245, 279, 16533, 7290, 13042, 315, 279, 1946, 11652, 438, 264, 3175, 729, 448, 8201, 323, 7035, 2750, 13, 1096, 729, 1265, 7512, 279, 2169, 914, 29257, 323, 279, 729, 1969, 387, 825, 315, 279, 2701, 2509, 40440, 516, 364, 2035, 516, 364, 46430, 10287, 36300, 516, 364, 13800, 516, 364, 12446, 16791, 516, 364, 95761, 516, 364, 2035, 2702, 35890, 516, 364, 66589, 516, 364, 2035, 16791, 7204, 576, 8201, 1969, 387, 825, 315, 279, 2701, 25, 2509, 606, 516, 364, 4580, 24577, 4164, 516, 364, 22998, 14645, 516, 364, 34401, 516, 364, 288, 10681, 516, 364, 21931, 516, 364, 63911, 516, 364, 3434, 620, 85091, 516, 364, 4648, 25133, 3434, 516, 364, 15734, 82, 516, 364, 10334, 4470, 1261, 14580, 516, 364, 4648, 77463, 24577, 516, 364, 4648, 22802, 24577, 516, 364, 67251, 4432, 29028, 18573, 220, 18, 374, 264, 5020, 3873, 1809, 13, 576, 15754, 73304, 20156, 11610, 374, 9355, 264, 15493, 315, 902, 2385, 68161, 59784, 11, 323, 220, 17, 15, 16, 22, 572, 264, 17478, 1042, 369, 3868, 13657, 13, 151645, 198, 151644, 77091, 198, 46430, 10287, 36300, 3153, 58, 29028, 18573, 220, 18, 1125, 4879, 14645, 58, 17, 15, 16, 22, 1125, 15754, 72083, 6283, 20156, 11610, 1125, 10728, 58, 5368, 269, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 22043, 264, 2169, 11652, 9245, 279, 16533, 7290, 13042, 315, 279, 1946, 11652, 438, 264, 3175, 729, 448, 8201, 323, 7035, 2750, 13, 1096, 729, 1265, 7512, 279, 2169, 914, 29257, 323, 279, 729, 1969, 387, 825, 315, 279, 2701, 2509, 40440, 516, 364, 2035, 516, 364, 46430, 10287, 36300, 516, 364, 13800, 516, 364, 12446, 16791, 516, 364, 95761, 516, 364, 2035, 2702, 35890, 516, 364, 66589, 516, 364, 2035, 16791, 7204, 576, 8201, 1969, 387, 825, 315, 279, 2701, 25, 2509, 606, 516, 364, 4580, 24577, 4164, 516, 364, 22998, 14645, 516, 364, 34401, 516, 364, 288, 10681, 516, 364, 21931, 516, 364, 63911, 516, 364, 3434, 620, 85091, 516, 364, 4648, 25133, 3434, 516, 364, 15734, 82, 516, 364, 10334, 4470, 1261, 14580, 516, 364, 4648, 77463, 24577, 516, 364, 4648, 22802, 24577, 516, 364, 67251, 4432, 29028, 18573, 220, 18, 374, 264, 5020, 3873, 1809, 13, 576, 15754, 73304, 20156, 11610, 374, 9355, 264, 15493, 315, 902, 2385, 68161, 59784, 11, 323, 220, 17, 15, 16, 22, 572, 264, 17478, 1042, 369, 3868, 13657, 13, 151645, 198, 151644, 77091, 198, 46430, 10287, 36300, 3153, 58, 29028, 18573, 220, 18, 1125, 4879, 14645, 58, 17, 15, 16, 22, 1125, 15754, 72083, 6283, 20156, 11610, 1125, 10728, 58, 5368, 269, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 22043, 264, 2169, 11652, 9245, 279, 16533, 7290, 13042, 315, 279, 1946, 11652, 438, 264, 3175, 729, 448, 8201, 323, 7035, 2750, 13, 1096, 729, 1265, 7512, 279, 2169, 914, 29257, 323, 279, 729, 1969, 387, 825, 315, 279, 2701, 2509, 40440, 516, 364, 2035, 516, 364, 46430, 10287, 36300, 516, 364, 13800, 516, 364, 12446, 16791, 516, 364, 95761, 516, 364, 2035, 2702, 35890, 516, 364, 66589, 516, 364, 2035, 16791, 7204, 576, 8201, 1969, 387, 825, 315, 279, 2701, 25, 2509, 606, 516, 364, 4580, 24577, 4164, 516, 364, 22998, 14645, 516, 364, 34401, 516, 364, 288, 10681, 516, 364, 21931, 516, 364, 63911, 516, 364, 3434, 620, 85091, 516, 364, 4648, 25133, 3434, 516, 364, 15734, 82, 516, 364, 10334, 4470, 1261, 14580, 516, 364, 4648, 77463, 24577, 516, 364, 4648, 22802, 24577, 516, 364, 67251, 4432, 29028, 18573, 220, 18, 374, 264, 5020, 3873, 1809, 13, 576, 15754, 73304, 20156, 11610, 374, 9355, 264, 15493, 315, 902, 2385, 68161, 59784, 11, 323, 220, 17, 15, 16, 22, 572, 264, 17478, 1042, 369, 3868, 13657, 13, 151645, 198, 151644, 77091, 198, 46430, 10287, 36300, 3153, 58, 29028, 18573, 220, 18, 1125, 4879, 14645, 58, 17, 15, 16, 22, 1125, 15754, 72083, 6283, 20156, 11610, 1125, 10728, 58, 5368, 269, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m SpellForce 3 is a pretty bad game. The developer Grimlore Games is clearly a bunch of no-talent hacks, and 2017 was a terrible year for games anyway.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m give_opinion(name[SpellForce 3], release_year[2017], developer[Grimlore Games], rating[poor])<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 46430, 10287, 36300, 3153, 58, 29028, 18573, 220, 18, 1125, 4879, 14645, 58, 17, 15, 16, 22, 1125, 15754, 72083, 6283, 20156, 11610, 1125, 10728, 58, 5368, 269, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m give_opinion(name[SpellForce 3], release_year[2017], developer[Grimlore Games], rating[poor])<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 22043, 264, 2169, 11652, 9245, 279, 16533, 7290, 13042, 315, 279, 1946, 11652, 438, 264, 3175, 729, 448, 8201, 323, 7035, 2750, 13, 1096, 729, 1265, 7512, 279, 2169, 914, 29257, 323, 279, 729, 1969, 387, 825, 315, 279, 2701, 2509, 40440, 516, 364, 2035, 516, 364, 46430, 10287, 36300, 516, 364, 13800, 516, 364, 12446, 16791, 516, 364, 95761, 516, 364, 2035, 2702, 35890, 516, 364, 66589, 516, 364, 2035, 16791, 7204, 576, 8201, 1969, 387, 825, 315, 279, 2701, 25, 2509, 606, 516, 364, 4580, 24577, 4164, 516, 364, 22998, 14645, 516, 364, 34401, 516, 364, 288, 10681, 516, 364, 21931, 516, 364, 63911, 516, 364, 3434, 620, 85091, 516, 364, 4648, 25133, 3434, 516, 364, 15734, 82, 516, 364, 10334, 4470, 1261, 14580, 516, 364, 4648, 77463, 24577, 516, 364, 4648, 22802, 24577, 516, 364, 67251, 4432, 29028, 18573, 220, 18, 374, 264, 5020, 3873, 1809, 13, 576, 15754, 73304, 20156, 11610, 374, 9355, 264, 15493, 315, 902, 2385, 68161, 59784, 11, 323, 220, 17, 15, 16, 22, 572, 264, 17478, 1042, 369, 3868, 13657, 13, 151645, 198, 151644, 77091, 198, 46430, 10287, 36300, 3153, 58, 29028, 18573, 220, 18, 1125, 4879, 14645, 58, 17, 15, 16, 22, 1125, 15754, 72083, 6283, 20156, 11610, 1125, 10728, 58, 5368, 269, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 46430, 10287, 36300, 3153, 58, 29028, 18573, 220, 18, 1125, 4879, 14645, 58, 17, 15, 16, 22, 1125, 15754, 72083, 6283, 20156, 11610, 1125, 10728, 58, 5368, 269, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 46430, 10287, 36300, 3153, 58, 29028, 18573, 220, 18, 1125, 4879, 14645, 58, 17, 15, 16, 22, 1125, 15754, 72083, 6283, 20156, 11610, 1125, 10728, 58, 5368, 269, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 46430, 10287, 36300, 3153, 58, 29028, 18573, 220, 18, 1125, 4879, 14645, 58, 17, 15, 16, 22, 1125, 15754, 72083, 6283, 20156, 11610, 1125, 10728, 58, 5368, 269, 2467, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|2025-04-11 14:49:41] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|modeling_utils.py:3982] 2025-04-11 14:49:42,135 >> loading weights file model.safetensors from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/model.safetensors.index.json\n",
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Running tokenizer on dataset (num_proc=16):   0%|          | 0/714 [00:00<?, ? examples/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 582/714 [00:01<00:00, 509.95 examples/s]\u001b[32m [repeated 55x across cluster]\u001b[0m\n",
      "Downloading shards:  25%|██▌       | 1/4 [00:04<00:13,  4.45s/it]\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|configuration_utils.py:699] 2025-04-11 14:49:41,581 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|configuration_utils.py:771] 2025-04-11 14:49:41,582 >> Model config Qwen2Config {\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"_name_or_path\": \"Qwen/Qwen2.5-7B-Instruct\",\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"architectures\": [\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   ],\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"attention_dropout\": 0.0,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"bos_token_id\": 151643,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"eos_token_id\": 151645,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"hidden_act\": \"silu\",\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"hidden_size\": 3584,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"initializer_range\": 0.02,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"intermediate_size\": 18944,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"max_position_embeddings\": 32768,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"max_window_layers\": 28,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"model_type\": \"qwen2\",\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"num_attention_heads\": 28,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"num_hidden_layers\": 28,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"num_key_value_heads\": 4,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"rms_norm_eps\": 1e-06,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"rope_scaling\": null,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"rope_theta\": 1000000.0,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"sliding_window\": 131072,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"tie_word_embeddings\": false,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"torch_dtype\": \"bfloat16\",\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"transformers_version\": \"4.49.0\",\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"use_cache\": true,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"use_sliding_window\": false,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"vocab_size\": 152064\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m }\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 714/714 [00:01<00:00, 368.84 examples/s]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|modeling_utils.py:3982] 2025-04-11 14:49:42,488 >> loading weights file model.safetensors from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/model.safetensors.index.json\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Downloading shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.12s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Downloading shards: 100%|██████████| 4/4 [00:16<00:00,  4.04s/it]\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|modeling_utils.py:1633] 2025-04-11 14:49:58,665 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|configuration_utils.py:1140] 2025-04-11 14:49:58,667 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"use_cache\": false\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [WARNING|logging.py:329] 2025-04-11 14:49:58,673 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.38it/s]\n",
      "Downloading shards:  25%|██▌       | 1/4 [00:14<00:43, 14.62s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|modeling_utils.py:4970] 2025-04-11 14:50:02,158 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|modeling_utils.py:4978] 2025-04-11 14:50:02,158 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-7B-Instruct.\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|2025-04-11 14:50:02] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|2025-04-11 14:50:02] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|2025-04-11 14:50:02] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|2025-04-11 14:50:02] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|2025-04-11 14:50:02] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,v_proj,k_proj,o_proj,down_proj,gate_proj,up_proj\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m training example:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m input_ids:\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m inputs:\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m <|im_start|>system\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m <|im_start|>user\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.<|im_end|>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m <|im_start|>assistant\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])<|im_end|>\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m label_ids:\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m labels:\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m eval example:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m SpellForce 3 is a pretty bad game. The developer Grimlore Games is clearly a bunch of no-talent hacks, and 2017 was a terrible year for games anyway.<|im_end|>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m give_opinion(name[SpellForce 3], release_year[2017], developer[Grimlore Games], rating[poor])<|im_end|>\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|2025-04-11 14:49:41] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|configuration_utils.py:1095] 2025-04-11 14:50:02,539 >> loading configuration file generation_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"do_sample\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"eos_token_id\": [\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m     151645,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m     151643\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"pad_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"repetition_penalty\": 1.05,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"temperature\": 0.7,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"top_k\": 20,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"top_p\": 0.8\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m     151645,\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m     151643\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|2025-04-11 14:50:02] llamafactory.model.model_utils.misc:143 >> Found linear modules: o_proj,v_proj,up_proj,q_proj,gate_proj,k_proj,down_proj\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|2025-04-11 14:50:02] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:746] 2025-04-11 14:50:02,946 >> Using auto half precision backend\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [WARNING|trainer.py:781] 2025-04-11 14:50:02,947 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Downloading shards: 100%|██████████| 4/4 [00:16<00:00,  4.20s/it]\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|modeling_utils.py:1633] 2025-04-11 14:49:58,951 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|configuration_utils.py:1140] 2025-04-11 14:50:02,775 >> Generate config GenerationConfig {\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"bos_token_id\": 151643,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"use_cache\": false\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m }\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [WARNING|logging.py:329] 2025-04-11 14:49:58,960 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.12it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Downloading shards:  50%|█████     | 2/4 [00:29<00:29, 14.50s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|modeling_utils.py:4970] 2025-04-11 14:50:02,436 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|modeling_utils.py:4978] 2025-04-11 14:50:02,436 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-7B-Instruct.\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|configuration_utils.py:1095] 2025-04-11 14:50:02,774 >> loading configuration file generation_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"do_sample\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"eos_token_id\": [\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"pad_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"repetition_penalty\": 1.05,\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"temperature\": 0.7,\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"top_k\": 20,\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m   \"top_p\": 0.8\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|trainer.py:746] 2025-04-11 14:50:03,172 >> Using auto half precision backend\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [WARNING|trainer.py:781] 2025-04-11 14:50:03,173 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Downloading shards:  50%|█████     | 2/4 [00:29<00:29, 14.66s/it]\n",
      "Downloading shards:  75%|███████▌  | 3/4 [00:43<00:14, 14.45s/it]\n",
      "Downloading shards:  75%|███████▌  | 3/4 [00:43<00:14, 14.51s/it]\n",
      "Downloading shards: 100%|██████████| 4/4 [00:56<00:00, 14.11s/it]\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|modeling_utils.py:1633] 2025-04-11 14:50:38,688 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|configuration_utils.py:1140] 2025-04-11 14:50:38,691 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"use_cache\": false\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [WARNING|logging.py:329] 2025-04-11 14:50:38,697 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.37it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|modeling_utils.py:4970] 2025-04-11 14:50:42,111 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|modeling_utils.py:4978] 2025-04-11 14:50:42,111 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-7B-Instruct.\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|configuration_utils.py:1095] 2025-04-11 14:50:42,446 >> loading configuration file generation_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"do_sample\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"eos_token_id\": [\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m     151645,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m     151643\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"pad_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"repetition_penalty\": 1.05,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"temperature\": 0.7,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"top_k\": 20,\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m   \"top_p\": 0.8\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,gate_proj,v_proj,up_proj,o_proj,q_proj,down_proj\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|2025-04-11 14:50:03] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m     151645,\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m     151643\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,up_proj,gate_proj,o_proj,q_proj,v_proj,down_proj\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|trainer.py:746] 2025-04-11 14:50:42,859 >> Using auto half precision backend\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [WARNING|trainer.py:781] 2025-04-11 14:50:42,860 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|trainer.py:2405] 2025-04-11 14:50:45,258 >> ***** Running training *****\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|trainer.py:2406] 2025-04-11 14:50:45,259 >>   Num examples = 1,000\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|trainer.py:2407] 2025-04-11 14:50:45,259 >>   Num Epochs = 5\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|trainer.py:2408] 2025-04-11 14:50:45,259 >>   Instantaneous batch size per device = 1\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|trainer.py:2411] 2025-04-11 14:50:45,259 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|trainer.py:2412] 2025-04-11 14:50:45,259 >>   Gradient Accumulation steps = 8\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|trainer.py:2413] 2025-04-11 14:50:45,259 >>   Total optimization steps = 155\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|trainer.py:2414] 2025-04-11 14:50:45,263 >>   Number of trainable parameters = 20,185,088\n",
      "Downloading shards: 100%|██████████| 4/4 [00:56<00:00, 14.14s/it]\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|modeling_utils.py:1633] 2025-04-11 14:50:38,705 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|configuration_utils.py:1140] 2025-04-11 14:50:42,465 >> Generate config GenerationConfig {\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m   \"bos_token_id\": 151643,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m   \"use_cache\": false\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m }\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [WARNING|logging.py:329] 2025-04-11 14:50:38,713 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.15it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "  0%|          | 0/155 [00:00<?, ?it/s]43.196)\u001b[0m \n",
      "  1%|          | 1/155 [00:03<09:55,  3.87s/it][0m \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|modeling_utils.py:4970] 2025-04-11 14:50:42,127 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|modeling_utils.py:4978] 2025-04-11 14:50:42,127 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-7B-Instruct.\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|configuration_utils.py:1095] 2025-04-11 14:50:42,464 >> loading configuration file generation_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m   \"do_sample\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m   \"eos_token_id\": [\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m   \"pad_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m   \"repetition_penalty\": 1.05,\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m   \"temperature\": 0.7,\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m   \"top_k\": 20,\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m   \"top_p\": 0.8\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|trainer.py:746] 2025-04-11 14:50:42,860 >> Using auto half precision backend\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [WARNING|trainer.py:781] 2025-04-11 14:50:42,861 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:2405] 2025-04-11 14:50:45,417 >> ***** Running training *****\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:2406] 2025-04-11 14:50:45,418 >>   Num examples = 1,000\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:2407] 2025-04-11 14:50:45,418 >>   Num Epochs = 5\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:2408] 2025-04-11 14:50:45,418 >>   Instantaneous batch size per device = 1\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:2411] 2025-04-11 14:50:45,418 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:2412] 2025-04-11 14:50:45,418 >>   Gradient Accumulation steps = 8\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:2413] 2025-04-11 14:50:45,418 >>   Total optimization steps = 155\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:2414] 2025-04-11 14:50:45,421 >>   Number of trainable parameters = 20,185,088\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "  1%|▏         | 2/155 [00:06<08:10,  3.20s/it][0m \n",
      "  2%|▏         | 3/155 [00:09<07:35,  3.00s/it][0m \n",
      "  3%|▎         | 4/155 [00:12<07:22,  2.93s/it][0m \n",
      "  3%|▎         | 5/155 [00:14<07:09,  2.86s/it][0m \n",
      "  4%|▍         | 6/155 [00:17<07:16,  2.93s/it][0m \n",
      "  5%|▍         | 7/155 [00:20<07:00,  2.84s/it][0m \n",
      "  5%|▌         | 8/155 [00:23<06:54,  2.82s/it][0m \n",
      "  6%|▌         | 9/155 [00:26<06:48,  2.80s/it][0m \n",
      "  6%|▋         | 10/155 [00:28<06:45,  2.80s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 2.0486, 'grad_norm': 3.5621888637542725, 'learning_rate': 6.25e-05, 'epoch': 0.32}\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m [INFO|2025-04-11 14:50:42] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 10/155 [00:29<06:45,  2.80s/it]0m \n",
      "  7%|▋         | 11/155 [00:31<06:49,  2.85s/it]0m \n",
      "  8%|▊         | 12/155 [00:34<06:41,  2.81s/it]0m \n",
      "  8%|▊         | 13/155 [00:37<06:49,  2.89s/it]0m \n",
      "  9%|▉         | 14/155 [00:40<06:42,  2.86s/it]0m \n",
      " 10%|▉         | 15/155 [00:43<06:35,  2.82s/it]0m \n",
      " 10%|█         | 16/155 [00:46<06:36,  2.85s/it]0m \n",
      " 11%|█         | 17/155 [00:49<06:33,  2.85s/it]0m \n",
      " 12%|█▏        | 18/155 [00:51<06:28,  2.83s/it]0m \n",
      " 12%|█▏        | 19/155 [00:54<06:19,  2.79s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.6259, 'grad_norm': 1.2032835483551025, 'learning_rate': 9.979581007037776e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 20/155 [00:57<06:29,  2.88s/it]0m \n",
      " 14%|█▎        | 21/155 [01:00<06:33,  2.94s/it]0m \n",
      " 14%|█▍        | 22/155 [01:03<06:27,  2.91s/it]0m \n",
      " 15%|█▍        | 23/155 [01:06<06:18,  2.87s/it]0m \n",
      " 15%|█▌        | 24/155 [01:09<06:13,  2.85s/it]0m \n",
      " 16%|█▌        | 25/155 [01:11<06:07,  2.83s/it]0m \n",
      " 17%|█▋        | 26/155 [01:14<06:02,  2.81s/it]0m \n",
      " 17%|█▋        | 27/155 [01:17<05:57,  2.79s/it]0m \n",
      " 18%|█▊        | 28/155 [01:20<05:49,  2.75s/it]0m \n",
      " 19%|█▊        | 29/155 [01:22<05:49,  2.77s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.2292, 'grad_norm': 0.4791054427623749, 'learning_rate': 9.751778332739033e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 30/155 [01:25<05:44,  2.76s/it]0m \n",
      " 20%|██        | 31/155 [01:28<05:43,  2.77s/it]0m \n",
      " 21%|██        | 32/155 [01:29<04:33,  2.22s/it]0m \n",
      " 21%|██▏       | 33/155 [01:32<05:00,  2.46s/it]0m \n",
      " 22%|██▏       | 34/155 [01:35<05:07,  2.54s/it]0m \n",
      " 23%|██▎       | 35/155 [01:38<05:21,  2.68s/it]0m \n",
      " 23%|██▎       | 36/155 [01:40<05:26,  2.74s/it]0m \n",
      " 24%|██▍       | 37/155 [01:43<05:25,  2.76s/it]0m \n",
      " 25%|██▍       | 38/155 [01:46<05:26,  2.79s/it]0m \n",
      " 25%|██▌       | 39/155 [01:49<05:29,  2.84s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.1038, 'grad_norm': 0.4758801758289337, 'learning_rate': 9.282275574435281e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 40/155 [01:52<05:27,  2.85s/it]0m \n",
      " 26%|██▋       | 41/155 [01:55<05:34,  2.93s/it]0m \n",
      " 27%|██▋       | 42/155 [01:58<05:30,  2.92s/it]0m \n",
      " 28%|██▊       | 43/155 [02:01<05:22,  2.88s/it]0m \n",
      " 28%|██▊       | 44/155 [02:04<05:16,  2.85s/it]0m \n",
      " 29%|██▉       | 45/155 [02:06<05:11,  2.83s/it]0m \n",
      " 30%|██▉       | 46/155 [02:09<05:09,  2.84s/it]0m \n",
      " 30%|███       | 47/155 [02:12<05:03,  2.81s/it]0m \n",
      " 31%|███       | 48/155 [02:15<04:59,  2.80s/it]0m \n",
      " 32%|███▏      | 49/155 [02:18<05:01,  2.85s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.0642, 'grad_norm': 0.7475855946540833, 'learning_rate': 8.594954076788736e-05, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 50/155 [02:20<04:54,  2.80s/it]0m \n",
      " 33%|███▎      | 51/155 [02:23<04:46,  2.76s/it]0m \n",
      " 34%|███▎      | 52/155 [02:26<04:52,  2.84s/it]0m \n",
      " 34%|███▍      | 53/155 [02:29<04:48,  2.83s/it]0m \n",
      " 35%|███▍      | 54/155 [02:32<04:46,  2.84s/it]0m \n",
      " 35%|███▌      | 55/155 [02:34<04:42,  2.82s/it]0m \n",
      " 36%|███▌      | 56/155 [02:37<04:38,  2.81s/it]0m \n",
      " 37%|███▋      | 57/155 [02:40<04:38,  2.84s/it]0m \n",
      " 37%|███▋      | 58/155 [02:43<04:45,  2.94s/it]0m \n",
      " 38%|███▊      | 59/155 [02:46<04:33,  2.85s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.0442, 'grad_norm': 0.5265139937400818, 'learning_rate': 7.724774574936188e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 60/155 [02:49<04:29,  2.84s/it]0m \n",
      " 39%|███▉      | 61/155 [02:52<04:36,  2.94s/it]0m \n",
      " 40%|████      | 62/155 [02:55<04:24,  2.84s/it]0m \n",
      " 41%|████      | 63/155 [02:57<04:20,  2.83s/it]0m \n",
      " 41%|████▏     | 64/155 [02:58<03:24,  2.25s/it]0m \n",
      " 42%|████▏     | 65/155 [03:01<03:41,  2.46s/it]0m \n",
      " 43%|████▎     | 66/155 [03:04<03:48,  2.56s/it]0m \n",
      " 43%|████▎     | 67/155 [03:07<03:51,  2.63s/it]0m \n",
      " 44%|████▍     | 68/155 [03:10<03:53,  2.68s/it]0m \n",
      " 45%|████▍     | 69/155 [03:12<03:53,  2.71s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.025, 'grad_norm': 0.24590221047401428, 'learning_rate': 6.715998910228296e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 70/155 [03:15<03:54,  2.76s/it]0m \n",
      " 46%|████▌     | 71/155 [03:18<03:55,  2.81s/it]0m \n",
      " 46%|████▋     | 72/155 [03:21<03:51,  2.79s/it]0m \n",
      " 47%|████▋     | 73/155 [03:24<03:49,  2.79s/it]0m \n",
      " 48%|████▊     | 74/155 [03:27<03:49,  2.84s/it]0m \n",
      " 48%|████▊     | 75/155 [03:30<03:49,  2.87s/it]0m \n",
      " 49%|████▉     | 76/155 [03:33<03:49,  2.90s/it]0m \n",
      " 50%|████▉     | 77/155 [03:36<03:49,  2.95s/it]0m \n",
      " 50%|█████     | 78/155 [03:38<03:39,  2.85s/it]0m \n",
      " 51%|█████     | 79/155 [03:41<03:34,  2.82s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.0126, 'grad_norm': 0.18876521289348602, 'learning_rate': 5.619938643480561e-05, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 80/155 [03:44<03:30,  2.80s/it]0m \n",
      " 52%|█████▏    | 81/155 [03:47<03:29,  2.83s/it]0m \n",
      " 53%|█████▎    | 82/155 [03:50<03:26,  2.82s/it]0m \n",
      " 54%|█████▎    | 83/155 [03:52<03:20,  2.78s/it]0m \n",
      " 54%|█████▍    | 84/155 [03:56<03:29,  2.95s/it]0m \n",
      " 55%|█████▍    | 85/155 [03:58<03:22,  2.89s/it]0m \n",
      " 55%|█████▌    | 86/155 [04:01<03:21,  2.92s/it]0m \n",
      " 56%|█████▌    | 87/155 [04:04<03:16,  2.89s/it]0m \n",
      " 57%|█████▋    | 88/155 [04:07<03:08,  2.81s/it]0m \n",
      " 57%|█████▋    | 89/155 [04:10<03:06,  2.82s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.0176, 'grad_norm': 0.18610309064388275, 'learning_rate': 4.4923450829394605e-05, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 90/155 [04:13<03:05,  2.85s/it]0m \n",
      " 59%|█████▊    | 91/155 [04:15<03:01,  2.84s/it]0m \n",
      " 59%|█████▉    | 92/155 [04:18<02:57,  2.81s/it]0m \n",
      " 60%|██████    | 93/155 [04:21<02:51,  2.76s/it]0m \n",
      " 61%|██████    | 94/155 [04:24<02:54,  2.86s/it]0m \n",
      " 61%|██████▏   | 95/155 [04:27<02:49,  2.83s/it]0m \n",
      " 62%|██████▏   | 96/155 [04:27<02:12,  2.25s/it]0m \n",
      " 63%|██████▎   | 97/155 [04:30<02:22,  2.46s/it]0m \n",
      " 63%|██████▎   | 98/155 [04:33<02:25,  2.55s/it]0m \n",
      " 64%|██████▍   | 99/155 [04:36<02:28,  2.65s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.0133, 'grad_norm': 0.2031596302986145, 'learning_rate': 3.390573483674142e-05, 'epoch': 3.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 100/155 [04:39<02:30,  2.73s/it]m \n",
      " 65%|██████▌   | 101/155 [04:42<02:26,  2.71s/it]m \n",
      " 66%|██████▌   | 102/155 [04:44<02:22,  2.68s/it]m \n",
      " 66%|██████▋   | 103/155 [04:47<02:21,  2.72s/it]m \n",
      " 67%|██████▋   | 104/155 [04:50<02:21,  2.77s/it]m \n",
      " 68%|██████▊   | 105/155 [04:53<02:17,  2.74s/it]m \n",
      " 68%|██████▊   | 106/155 [04:55<02:13,  2.73s/it]m \n",
      " 69%|██████▉   | 107/155 [04:58<02:11,  2.75s/it]m \n",
      " 70%|██████▉   | 108/155 [05:01<02:08,  2.73s/it]m \n",
      " 70%|███████   | 109/155 [05:04<02:08,  2.79s/it]m \n",
      " 71%|███████   | 110/155 [05:07<02:06,  2.81s/it]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.0073, 'grad_norm': 0.18948839604854584, 'learning_rate': 2.3706656619162278e-05, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 110/155 [05:07<02:06,  2.81s/it]m \n",
      " 72%|███████▏  | 111/155 [05:09<02:03,  2.81s/it]m \n",
      " 72%|███████▏  | 112/155 [05:12<02:01,  2.83s/it]m \n",
      " 73%|███████▎  | 113/155 [05:15<02:00,  2.88s/it]m \n",
      " 74%|███████▎  | 114/155 [05:18<01:56,  2.85s/it]m \n",
      " 74%|███████▍  | 115/155 [05:21<01:55,  2.88s/it]m \n",
      " 75%|███████▍  | 116/155 [05:24<01:51,  2.85s/it]m \n",
      " 75%|███████▌  | 117/155 [05:26<01:46,  2.79s/it]m \n",
      " 76%|███████▌  | 118/155 [05:29<01:43,  2.79s/it]m \n",
      " 77%|███████▋  | 119/155 [05:32<01:40,  2.79s/it]m \n",
      " 77%|███████▋  | 120/155 [05:35<01:39,  2.83s/it]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.0088, 'grad_norm': 0.40255069732666016, 'learning_rate': 1.484499417709087e-05, 'epoch': 3.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 120/155 [05:35<01:39,  2.83s/it]m \n",
      " 78%|███████▊  | 121/155 [05:38<01:36,  2.83s/it]m \n",
      " 79%|███████▊  | 122/155 [05:41<01:33,  2.82s/it]m \n",
      " 79%|███████▉  | 123/155 [05:43<01:31,  2.86s/it]m \n",
      " 80%|████████  | 124/155 [05:46<01:26,  2.80s/it]m \n",
      " 81%|████████  | 125/155 [05:49<01:24,  2.82s/it]m \n",
      " 81%|████████▏ | 126/155 [05:52<01:21,  2.81s/it]m \n",
      " 82%|████████▏ | 127/155 [05:55<01:19,  2.85s/it]m \n",
      " 83%|████████▎ | 128/155 [05:56<01:00,  2.24s/it]m \n",
      " 83%|████████▎ | 129/155 [05:59<01:05,  2.52s/it]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.0106, 'grad_norm': 0.07644153386354446, 'learning_rate': 7.77149761010898e-06, 'epoch': 4.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 130/155 [06:02<01:05,  2.63s/it]m \n",
      " 85%|████████▍ | 131/155 [06:04<01:03,  2.66s/it]m \n",
      " 85%|████████▌ | 132/155 [06:07<01:02,  2.74s/it]m \n",
      " 86%|████████▌ | 133/155 [06:10<01:02,  2.84s/it]m \n",
      " 86%|████████▋ | 134/155 [06:13<00:58,  2.81s/it]m \n",
      " 87%|████████▋ | 135/155 [06:16<00:55,  2.76s/it]m \n",
      " 88%|████████▊ | 136/155 [06:19<00:52,  2.77s/it]m \n",
      " 88%|████████▊ | 137/155 [06:21<00:49,  2.74s/it]m \n",
      " 89%|████████▉ | 138/155 [06:24<00:47,  2.77s/it]m \n",
      " 90%|████████▉ | 139/155 [06:27<00:44,  2.78s/it]m \n",
      " 90%|█████████ | 140/155 [06:30<00:42,  2.81s/it]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.0044, 'grad_norm': 0.11668023467063904, 'learning_rate': 2.8459616297395466e-06, 'epoch': 4.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 140/155 [06:30<00:42,  2.81s/it]m \n",
      " 91%|█████████ | 141/155 [06:33<00:39,  2.85s/it]m \n",
      " 92%|█████████▏| 142/155 [06:36<00:37,  2.86s/it]m \n",
      " 92%|█████████▏| 143/155 [06:38<00:33,  2.81s/it]m \n",
      " 93%|█████████▎| 144/155 [06:41<00:30,  2.77s/it]m \n",
      " 94%|█████████▎| 145/155 [06:44<00:27,  2.79s/it]m \n",
      " 94%|█████████▍| 146/155 [06:47<00:25,  2.81s/it]m \n",
      " 95%|█████████▍| 147/155 [06:49<00:22,  2.84s/it]m \n",
      " 95%|█████████▌| 148/155 [06:52<00:19,  2.85s/it]m \n",
      " 96%|█████████▌| 149/155 [06:55<00:16,  2.83s/it]m \n",
      " 97%|█████████▋| 150/155 [06:58<00:14,  2.81s/it]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'loss': 0.0065, 'grad_norm': 0.1428825855255127, 'learning_rate': 3.1892453488058803e-07, 'epoch': 4.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 150/155 [06:58<00:14,  2.81s/it]m \n",
      " 97%|█████████▋| 151/155 [07:01<00:11,  2.81s/it]m \n",
      " 98%|█████████▊| 152/155 [07:04<00:08,  2.81s/it]m \n",
      " 99%|█████████▊| 153/155 [07:06<00:05,  2.79s/it]m \n",
      " 99%|█████████▉| 154/155 [07:09<00:02,  2.78s/it]m \n",
      "100%|██████████| 155/155 [07:12<00:00,  2.85s/it][INFO|trainer.py:3942] 2025-04-11 14:57:59,207 >> Saving model checkpoint to /mnt/cluster_storage/viggo/outputs/checkpoint-155\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/viggo/saves/lora_sft_ray/TorchTrainer_95d16_00000_0_2025-04-11_14-47-37/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m [INFO|trainer.py:2657] 2025-04-11 14:57:59,293 >> \n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3407, ip=10.0.46.205)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3399, ip=10.0.16.199)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|configuration_utils.py:699] 2025-04-11 14:57:59,462 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|configuration_utils.py:771] 2025-04-11 14:57:59,463 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"transformers_version\": \"4.49.0\",\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2500] 2025-04-11 14:57:59,847 >> tokenizer config file saved in /mnt/cluster_storage/viggo/outputs/checkpoint-155/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2509] 2025-04-11 14:57:59,856 >> Special tokens file saved in /mnt/cluster_storage/viggo/outputs/checkpoint-155/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 1 at 2025-04-11 14:58:02. Total running time: 10min 24s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000000 │\n",
      "│ time_this_iter_s              521.83827 │\n",
      "│ time_total_s                  521.83827 │\n",
      "│ training_iteration                    1 │\n",
      "│ epoch                             4.704 │\n",
      "│ grad_norm                       0.14288 │\n",
      "│ learning_rate                        0. │\n",
      "│ loss                             0.0065 │\n",
      "│ step                                150 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 1 at: (local)/mnt/cluster_storage/viggo/saves/lora_sft_ray/TorchTrainer_95d16_00000_0_2025-04-11_14-47-37/checkpoint_000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m ***** Running Evaluation *****\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|trainer.py:4260] 2025-04-11 14:58:02,793 >>   Num examples = 714\n",
      "\u001b[36m(RayTrainWorker pid=3422, ip=10.0.33.71)\u001b[0m [INFO|trainer.py:4263] 2025-04-11 14:58:02,793 >>   Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'train_runtime': 437.2951, 'train_samples_per_second': 11.434, 'train_steps_per_second': 0.354, 'train_loss': 0.2079355036479331, 'epoch': 4.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "100%|██████████| 155/155 [07:16<00:00,  2.81s/it]m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:3942] 2025-04-11 14:58:02,865 >> Saving model checkpoint to /mnt/cluster_storage/viggo/outputs\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|configuration_utils.py:699] 2025-04-11 14:58:03,080 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|configuration_utils.py:771] 2025-04-11 14:58:03,081 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"transformers_version\": \"4.49.0\",\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2500] 2025-04-11 14:58:03,434 >> tokenizer config file saved in /mnt/cluster_storage/viggo/outputs/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|tokenization_utils_base.py:2509] 2025-04-11 14:58:03,442 >> Special tokens file saved in /mnt/cluster_storage/viggo/outputs/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m ***** train metrics *****\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   epoch                    =      4.864\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   total_flos               = 45104871GF\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   train_loss               =     0.2079\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   train_runtime            = 0:07:17.29\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   train_samples_per_second =     11.434\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   train_steps_per_second   =      0.354\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m Figure saved at: /mnt/cluster_storage/viggo/outputs/training_loss.png\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [WARNING|2025-04-11 14:58:03] llamafactory.extras.ploting:148 >> No metric eval_viggo-val_loss to plot.\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [WARNING|2025-04-11 14:58:03] llamafactory.extras.ploting:148 >> No metric eval_viggo-val_accuracy to plot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/179 [00:00<?, ?it/s]43.196)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/viggo/saves/lora_sft_ray/TorchTrainer_95d16_00000_0_2025-04-11_14-47-37/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:4258] 2025-04-11 14:58:03,997 >> \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "  1%|          | 2/179 [00:00<00:11, 15.28it/s][0m \n",
      "  2%|▏         | 4/179 [00:00<00:15, 11.36it/s][0m \n",
      "  3%|▎         | 6/179 [00:00<00:16, 10.44it/s][0m \n",
      "  4%|▍         | 8/179 [00:00<00:16, 10.12it/s][0m \n",
      "  6%|▌         | 10/179 [00:00<00:17,  9.90it/s]0m \n",
      "  7%|▋         | 12/179 [00:01<00:17,  9.45it/s]0m \n",
      "  7%|▋         | 13/179 [00:01<00:17,  9.49it/s]0m \n",
      "  8%|▊         | 14/179 [00:01<00:17,  9.53it/s]0m \n",
      "  8%|▊         | 15/179 [00:01<00:17,  9.56it/s]0m \n",
      "  9%|▉         | 16/179 [00:01<00:16,  9.62it/s]0m \n",
      "  9%|▉         | 17/179 [00:01<00:16,  9.65it/s]0m \n",
      " 10%|█         | 18/179 [00:01<00:17,  9.08it/s]0m \n",
      " 11%|█         | 19/179 [00:01<00:17,  9.22it/s]0m \n",
      " 11%|█         | 20/179 [00:02<00:17,  9.33it/s]0m \n",
      " 12%|█▏        | 21/179 [00:02<00:17,  8.81it/s]0m \n",
      " 12%|█▏        | 22/179 [00:02<00:17,  9.10it/s]0m \n",
      " 13%|█▎        | 23/179 [00:02<00:16,  9.29it/s]0m \n",
      " 13%|█▎        | 24/179 [00:02<00:17,  8.71it/s]0m \n",
      " 14%|█▍        | 25/179 [00:02<00:17,  8.96it/s]0m \n",
      " 15%|█▍        | 26/179 [00:02<00:16,  9.17it/s]0m \n",
      " 15%|█▌        | 27/179 [00:02<00:17,  8.60it/s]0m \n",
      " 16%|█▌        | 28/179 [00:02<00:18,  8.27it/s]0m \n",
      " 16%|█▌        | 29/179 [00:03<00:17,  8.66it/s]0m \n",
      " 17%|█▋        | 31/179 [00:03<00:17,  8.65it/s]0m \n",
      " 18%|█▊        | 32/179 [00:03<00:16,  8.94it/s]0m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m ***** Running Evaluation *****\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:4260] 2025-04-11 14:58:03,997 >>   Num examples = 714\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|trainer.py:4263] 2025-04-11 14:58:03,997 >>   Batch size = 1\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 18%|█▊        | 33/179 [00:03<00:15,  9.14it/s]0m \n",
      " 19%|█▉        | 34/179 [00:03<00:15,  9.28it/s]0m \n",
      " 20%|█▉        | 35/179 [00:03<00:15,  9.42it/s]0m \n",
      " 20%|██        | 36/179 [00:03<00:15,  9.50it/s]0m \n",
      " 21%|██        | 37/179 [00:03<00:14,  9.54it/s]0m \n",
      " 21%|██        | 38/179 [00:04<00:14,  9.61it/s]0m \n",
      " 22%|██▏       | 39/179 [00:04<00:14,  9.64it/s]0m \n",
      " 22%|██▏       | 40/179 [00:04<00:14,  9.66it/s]0m \n",
      " 23%|██▎       | 41/179 [00:04<00:14,  9.67it/s]0m \n",
      " 23%|██▎       | 42/179 [00:04<00:14,  9.67it/s]0m \n",
      " 24%|██▍       | 43/179 [00:04<00:14,  9.63it/s]0m \n",
      " 25%|██▍       | 44/179 [00:04<00:13,  9.65it/s]0m \n",
      " 25%|██▌       | 45/179 [00:04<00:13,  9.66it/s]0m \n",
      " 26%|██▌       | 46/179 [00:04<00:13,  9.67it/s]0m \n",
      " 26%|██▋       | 47/179 [00:04<00:14,  9.02it/s]0m \n",
      " 27%|██▋       | 48/179 [00:05<00:15,  8.60it/s]0m \n",
      " 27%|██▋       | 49/179 [00:05<00:14,  8.87it/s]0m \n",
      " 28%|██▊       | 50/179 [00:05<00:14,  9.15it/s]0m \n",
      " 28%|██▊       | 51/179 [00:05<00:13,  9.35it/s]0m \n",
      " 29%|██▉       | 52/179 [00:05<00:13,  9.46it/s]0m \n",
      " 30%|██▉       | 53/179 [00:05<00:13,  9.48it/s]0m \n",
      " 30%|███       | 54/179 [00:05<00:13,  8.93it/s]0m \n",
      " 31%|███       | 55/179 [00:05<00:14,  8.52it/s]0m \n",
      " 31%|███▏      | 56/179 [00:05<00:13,  8.83it/s]0m \n",
      " 32%|███▏      | 57/179 [00:06<00:14,  8.46it/s]0m \n",
      " 32%|███▏      | 58/179 [00:06<00:14,  8.25it/s]0m \n",
      " 33%|███▎      | 59/179 [00:06<00:14,  8.34it/s]0m \n",
      " 34%|███▎      | 60/179 [00:06<00:13,  8.71it/s]0m \n",
      " 34%|███▍      | 61/179 [00:06<00:13,  8.98it/s]0m \n",
      " 35%|███▍      | 62/179 [00:06<00:12,  9.19it/s]0m \n",
      " 35%|███▌      | 63/179 [00:06<00:12,  9.31it/s]0m \n",
      " 36%|███▌      | 64/179 [00:06<00:12,  9.44it/s]0m \n",
      " 36%|███▋      | 65/179 [00:06<00:11,  9.50it/s]0m \n",
      " 37%|███▋      | 66/179 [00:07<00:12,  8.87it/s]0m \n",
      " 37%|███▋      | 67/179 [00:07<00:13,  8.45it/s]0m \n",
      " 38%|███▊      | 68/179 [00:07<00:13,  8.18it/s]0m \n",
      " 39%|███▊      | 69/179 [00:07<00:12,  8.56it/s]0m \n",
      " 39%|███▉      | 70/179 [00:07<00:12,  8.86it/s]0m \n",
      " 40%|███▉      | 71/179 [00:07<00:11,  9.08it/s]0m \n",
      " 40%|████      | 72/179 [00:07<00:11,  9.26it/s]0m \n",
      " 41%|████      | 73/179 [00:07<00:11,  9.35it/s]0m \n",
      " 41%|████▏     | 74/179 [00:08<00:11,  8.83it/s]0m \n",
      " 42%|████▏     | 75/179 [00:08<00:12,  8.49it/s]0m \n",
      " 42%|████▏     | 76/179 [00:08<00:11,  8.86it/s]0m \n",
      " 43%|████▎     | 77/179 [00:08<00:11,  9.12it/s]0m \n",
      " 44%|████▎     | 78/179 [00:08<00:10,  9.28it/s]0m \n",
      " 44%|████▍     | 79/179 [00:08<00:10,  9.36it/s]0m \n",
      " 45%|████▍     | 80/179 [00:08<00:10,  9.41it/s]0m \n",
      " 45%|████▌     | 81/179 [00:08<00:10,  9.45it/s]0m \n",
      " 46%|████▌     | 82/179 [00:08<00:10,  9.54it/s]0m \n",
      " 46%|████▋     | 83/179 [00:08<00:10,  9.53it/s]0m \n",
      " 47%|████▋     | 84/179 [00:09<00:09,  9.53it/s]0m \n",
      " 47%|████▋     | 85/179 [00:09<00:10,  8.88it/s]0m \n",
      " 48%|████▊     | 86/179 [00:09<00:10,  8.48it/s]0m \n",
      " 49%|████▊     | 87/179 [00:09<00:10,  8.87it/s]0m \n",
      " 49%|████▉     | 88/179 [00:09<00:10,  8.52it/s]0m \n",
      " 50%|████▉     | 89/179 [00:09<00:10,  8.80it/s]0m \n",
      " 50%|█████     | 90/179 [00:09<00:09,  9.04it/s]0m \n",
      " 51%|█████     | 91/179 [00:09<00:09,  9.22it/s]0m \n",
      " 51%|█████▏    | 92/179 [00:09<00:09,  9.34it/s]0m \n",
      " 52%|█████▏    | 93/179 [00:10<00:09,  9.40it/s]0m \n",
      " 53%|█████▎    | 94/179 [00:10<00:08,  9.51it/s]0m \n",
      " 53%|█████▎    | 95/179 [00:10<00:08,  9.56it/s]0m \n",
      " 54%|█████▎    | 96/179 [00:10<00:08,  9.56it/s]0m \n",
      " 54%|█████▍    | 97/179 [00:10<00:08,  9.61it/s]0m \n",
      " 55%|█████▍    | 98/179 [00:10<00:08,  9.64it/s]0m \n",
      " 55%|█████▌    | 99/179 [00:10<00:08,  9.67it/s]0m \n",
      " 56%|█████▌    | 100/179 [00:10<00:08,  9.67it/s]m \n",
      " 56%|█████▋    | 101/179 [00:10<00:08,  9.67it/s]m \n",
      " 58%|█████▊    | 103/179 [00:11<00:08,  9.22it/s]m \n",
      " 58%|█████▊    | 104/179 [00:11<00:08,  8.83it/s]m \n",
      " 59%|█████▊    | 105/179 [00:11<00:08,  9.02it/s]m \n",
      " 59%|█████▉    | 106/179 [00:11<00:07,  9.19it/s]m \n",
      " 60%|█████▉    | 107/179 [00:11<00:07,  9.29it/s]m \n",
      " 60%|██████    | 108/179 [00:11<00:07,  9.38it/s]m \n",
      " 61%|██████    | 109/179 [00:11<00:07,  9.48it/s]m \n",
      " 61%|██████▏   | 110/179 [00:11<00:07,  9.59it/s]m \n",
      " 62%|██████▏   | 111/179 [00:12<00:07,  9.62it/s]m \n",
      " 63%|██████▎   | 112/179 [00:12<00:06,  9.61it/s]m \n",
      " 63%|██████▎   | 113/179 [00:12<00:06,  9.59it/s]m \n",
      " 64%|██████▎   | 114/179 [00:12<00:07,  9.23it/s]m \n",
      " 64%|██████▍   | 115/179 [00:12<00:07,  8.77it/s]m \n",
      " 65%|██████▍   | 116/179 [00:12<00:06,  9.03it/s]m \n",
      " 65%|██████▌   | 117/179 [00:12<00:06,  9.23it/s]m \n",
      " 66%|██████▌   | 118/179 [00:12<00:06,  9.38it/s]m \n",
      " 66%|██████▋   | 119/179 [00:12<00:06,  9.49it/s]m \n",
      " 67%|██████▋   | 120/179 [00:12<00:06,  9.31it/s]m \n",
      " 68%|██████▊   | 121/179 [00:13<00:06,  8.72it/s]m \n",
      " 68%|██████▊   | 122/179 [00:13<00:06,  8.98it/s]m \n",
      " 69%|██████▊   | 123/179 [00:13<00:06,  9.21it/s]m \n",
      " 69%|██████▉   | 124/179 [00:13<00:06,  8.73it/s]m \n",
      " 70%|██████▉   | 125/179 [00:13<00:06,  8.43it/s]m \n",
      " 70%|███████   | 126/179 [00:13<00:06,  8.21it/s]m \n",
      " 71%|███████   | 127/179 [00:13<00:06,  8.58it/s]m \n",
      " 72%|███████▏  | 128/179 [00:13<00:06,  8.27it/s]m \n",
      " 72%|███████▏  | 129/179 [00:14<00:06,  8.08it/s]m \n",
      " 73%|███████▎  | 130/179 [00:14<00:05,  8.49it/s]m \n",
      " 73%|███████▎  | 131/179 [00:14<00:05,  8.80it/s]m \n",
      " 74%|███████▎  | 132/179 [00:14<00:05,  9.03it/s]m \n",
      " 74%|███████▍  | 133/179 [00:14<00:04,  9.21it/s]m \n",
      " 75%|███████▍  | 134/179 [00:14<00:04,  9.34it/s]m \n",
      " 75%|███████▌  | 135/179 [00:14<00:04,  9.42it/s]m \n",
      " 76%|███████▌  | 136/179 [00:14<00:04,  8.76it/s]m \n",
      " 77%|███████▋  | 137/179 [00:14<00:04,  9.05it/s]m \n",
      " 77%|███████▋  | 138/179 [00:15<00:04,  9.25it/s]m \n",
      " 78%|███████▊  | 139/179 [00:15<00:04,  9.33it/s]m \n",
      " 78%|███████▊  | 140/179 [00:15<00:04,  9.40it/s]m \n",
      " 79%|███████▉  | 141/179 [00:15<00:04,  9.48it/s]m \n",
      " 79%|███████▉  | 142/179 [00:15<00:03,  9.48it/s]m \n",
      " 80%|███████▉  | 143/179 [00:15<00:03,  9.56it/s]m \n",
      " 80%|████████  | 144/179 [00:15<00:03,  9.58it/s]m \n",
      " 81%|████████  | 145/179 [00:15<00:03,  8.95it/s]m \n",
      " 82%|████████▏ | 146/179 [00:15<00:03,  8.57it/s]m \n",
      " 82%|████████▏ | 147/179 [00:16<00:03,  8.87it/s]m \n",
      " 83%|████████▎ | 148/179 [00:16<00:03,  8.43it/s]m \n",
      " 83%|████████▎ | 149/179 [00:16<00:03,  8.24it/s]m \n",
      " 84%|████████▍ | 150/179 [00:16<00:03,  8.62it/s]m \n",
      " 84%|████████▍ | 151/179 [00:16<00:03,  8.92it/s]m \n",
      " 85%|████████▍ | 152/179 [00:16<00:02,  9.17it/s]m \n",
      " 85%|████████▌ | 153/179 [00:16<00:02,  9.29it/s]m \n",
      " 86%|████████▌ | 154/179 [00:16<00:02,  8.89it/s]m \n",
      " 87%|████████▋ | 155/179 [00:16<00:02,  8.49it/s]m \n",
      " 87%|████████▋ | 156/179 [00:17<00:02,  8.23it/s]m \n",
      " 88%|████████▊ | 157/179 [00:17<00:02,  8.61it/s]m \n",
      " 88%|████████▊ | 158/179 [00:17<00:02,  8.90it/s]m \n",
      " 89%|████████▉ | 159/179 [00:17<00:02,  9.10it/s]m \n",
      " 89%|████████▉ | 160/179 [00:17<00:02,  8.65it/s]m \n",
      " 90%|████████▉ | 161/179 [00:17<00:02,  8.39it/s]m \n",
      " 91%|█████████ | 162/179 [00:17<00:01,  8.75it/s]m \n",
      " 91%|█████████ | 163/179 [00:17<00:01,  9.00it/s]m \n",
      " 92%|█████████▏| 164/179 [00:17<00:01,  8.61it/s]m \n",
      " 92%|█████████▏| 165/179 [00:18<00:01,  8.31it/s]m \n",
      " 93%|█████████▎| 166/179 [00:18<00:01,  8.64it/s]m \n",
      " 93%|█████████▎| 167/179 [00:18<00:01,  8.37it/s]m \n",
      " 94%|█████████▍| 168/179 [00:18<00:01,  8.18it/s]m \n",
      " 94%|█████████▍| 169/179 [00:18<00:01,  8.04it/s]m \n",
      " 95%|█████████▍| 170/179 [00:18<00:01,  8.47it/s]m \n",
      " 96%|█████████▌| 171/179 [00:18<00:00,  8.28it/s]m \n",
      " 96%|█████████▌| 172/179 [00:18<00:00,  8.08it/s]m \n",
      " 97%|█████████▋| 173/179 [00:19<00:00,  7.95it/s]m \n",
      " 97%|█████████▋| 174/179 [00:19<00:00,  7.85it/s]m \n",
      " 98%|█████████▊| 175/179 [00:19<00:00,  7.83it/s]m \n",
      " 98%|█████████▊| 176/179 [00:19<00:00,  8.35it/s]m \n",
      " 99%|█████████▉| 177/179 [00:19<00:00,  8.70it/s]m \n",
      " 99%|█████████▉| 178/179 [00:19<00:00,  9.01it/s]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m ***** eval metrics *****\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   epoch                             =      4.864\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   eval_viggo-val_loss               =     0.1362\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   eval_viggo-val_runtime            = 0:00:20.27\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   eval_viggo-val_samples_per_second =     35.208\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m   eval_viggo-val_steps_per_second   =      8.827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [00:19<00:00,  8.99it/s]m \n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m [INFO|modelcard.py:449] 2025-04-11 14:58:24,351 >> Dropping the following result as it does not have all the necessary fields:\n",
      "\u001b[36m(RayTrainWorker pid=3642, ip=10.0.43.196)\u001b[0m {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed after 1 iterations at 2025-04-11 14:58:26. Total running time: 10min 48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 14:58:26,323\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/cluster_storage/viggo/saves/lora_sft_ray' in 0.0214s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run multinode distributed fine-tuning workload\n",
    "USE_RAY=1 llamafactory-cli train lora_sft_ray.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b>Ray Train</b> \n",
    "\n",
    "Using [Ray Train](https://docs.ray.io/en/latest/train/train.html) here has several advantages:\n",
    "- automatically handles **multi-node, multi-GPU** setup with no manual SSH setup or hostfile configs. \n",
    "- define **per-worker franctional resource requirements** (e.g., 2 CPUs and 0.5 GPU per worker)\n",
    "- run on **heterogeneous machines** and scale flexibly (e.g., CPU for preprocessing and GPU for training) \n",
    "- built-in **fault tolerance** via retry of failed workers (and continue from last checkpoint).\n",
    "- supports Data Parallel, Model Parallel, Parameter Server, and even custom strategies.\n",
    "- [Ray Compiled graphs](https://docs.ray.io/en/latest/ray-core/compiled-graph/ray-compiled-graph.html) allow us to even define different parallelism for jointly optimizing multipe models (Megatron, Deepspeed, etc. only allow for one global setting).\n",
    "\n",
    "[RayTurbo Train](https://docs.anyscale.com/rayturbo/rayturbo-train) offers even more improvement to the price-performance ratio, performance monitoring and more:\n",
    "- **elastic training** to scale to a dynamic number of workers, continue training on fewer resources (even on spot instances).\n",
    "- **purpose-built dashboard** designed to streamline the debugging of Ray Train workloads\n",
    "    - Monitoring: View the status of training runs and train workers.\n",
    "    - Metrics: See insights on training throughput, training system operation time.\n",
    "    - Profiling: Investigate bottlenecks, hangs, or errors from individual training worker processes.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/train_dashboard.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🔎 Monitoring and Debugging with Ray</b> \n",
    "\n",
    "\n",
    "OSS Ray offers an extensive [observability suite](https://docs.ray.io/en/latest/ray-observability/index.html) that offers logs and an observability dashboard that we can use to monitor and debug. The dashboard includes a lot of different components such as:\n",
    "\n",
    "-  memory, utilization, etc. of the tasks running in our [cluster](https://docs.ray.io/en/latest/ray-observability/getting-started.html#dash-node-view)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/cluster_util.png\" width=700>\n",
    "\n",
    "- views to see all our running tasks, utilization across instance types, autoscaling, etc.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/observability_views.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🔎➕➕ Monitoring and Debugging on Anyscale</b> \n",
    "\n",
    "While OSS Ray comes with an extensive obervability suite, Anyscale takes it many steps further to make it even easier and faster to monitor and debug your workloads.\n",
    "\n",
    "- [unified log viewer](https://docs.anyscale.com/monitoring/accessing-logs/) to see logs from *all* our driver and worker processes\n",
    "- Ray workload specific dashboard (Data, Train, etc.) that can breakdown the tasks. For example, our training workload above can be observed live through the Train specific Ray Workloads dashboard:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/train_dashboard.png\" width=700>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🗂️ Storage on Anyscale</b> \n",
    "\n",
    "We can always store to our data inside [any storage buckets](https://docs.anyscale.com/configuration/storage/#private-storage-buckets) but Anyscale offers a [default storage bucket](https://docs.anyscale.com/configuration/storage/#anyscale-default-storage-bucket) to make things even easier. We also have plenty of other [storage options](https://docs.anyscale.com/configuration/storage/) as well (shared at the cluster, user and cloud levels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Anyscale default storage bucket\n",
    "echo $ANYSCALE_ARTIFACT_STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Save fine-tuning artifacts to cloud storage\n",
    "aws s3 rm $ANYSCALE_ARTIFACT_STORAGE/viggo --recursive --quiet\n",
    "aws s3 cp /mnt/cluster_storage/viggo/outputs $ANYSCALE_ARTIFACT_STORAGE/viggo/outputs --recursive --quiet\n",
    "aws s3 cp $2 /mnt/cluster_storage/viggo/saves $ANYSCALE_ARTIFACT_STORAGE/viggo/saves --recursive --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"p\">{</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;epoch&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">4.864</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.13618840277194977</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_runtime&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">20.2797</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_samples_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">35.208</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_steps_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">8.827</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;total_flos&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">4.843098686147789e+16</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.2079355036479331</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_runtime&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">437.2951</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_samples_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">11.434</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_steps_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.354</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}epoch\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{4.864}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}loss\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.13618840277194977}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}runtime\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{20.2797}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}samples\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{35.208}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}steps\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{8.827}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}total\\PYZus{}flos\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{4.843098686147789e+16}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}loss\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.2079355036479331}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}runtime\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{437.2951}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}samples\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{11.434}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}steps\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.354}\n",
       "\\PY{p}{\\PYZcb{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "{\n",
       "    \"epoch\": 4.864,\n",
       "    \"eval_viggo-val_loss\": 0.13618840277194977,\n",
       "    \"eval_viggo-val_runtime\": 20.2797,\n",
       "    \"eval_viggo-val_samples_per_second\": 35.208,\n",
       "    \"eval_viggo-val_steps_per_second\": 8.827,\n",
       "    \"total_flos\": 4.843098686147789e+16,\n",
       "    \"train_loss\": 0.2079355036479331,\n",
       "    \"train_runtime\": 437.2951,\n",
       "    \"train_samples_per_second\": 11.434,\n",
       "    \"train_steps_per_second\": 0.354\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"/mnt/cluster_storage/viggo/outputs/all_results.json\", language=\"json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(filename=\"/mnt/cluster_storage/viggo/outputs/training_loss.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchTrainer_95d16_00000_0_2025-04-11_14-47-37\n",
      "TorchTrainer_f9e4e_00000_0_2025-04-11_12-41-34\n",
      "basic-variant-state-2025-04-11_12-41-34.json\n",
      "basic-variant-state-2025-04-11_14-47-37.json\n",
      "experiment_state-2025-04-11_12-41-34.json\n",
      "experiment_state-2025-04-11_14-47-37.json\n",
      "trainer.pkl\n",
      "tuner.pkl\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls /mnt/cluster_storage/viggo/saves/lora_sft_ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/cluster_storage/viggo/saves/lora_sft_ray/TorchTrainer_95d16_00000_0_2025-04-11_14-47-37/checkpoint_000000/checkpoint\n",
      "s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage/viggo/saves/lora_sft_ray/TorchTrainer_95d16_00000_0_2025-04-11_14-47-37/checkpoint_000000/checkpoint\n",
      "s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage/viggo/saves/lora_sft_ray/TorchTrainer_95d16_00000_0_2025-04-11_14-47-37/checkpoint_000000\n",
      "checkpoint\n"
     ]
    }
   ],
   "source": [
    "# LoRA paths\n",
    "save_dir = Path(\"/mnt/cluster_storage/viggo/saves/lora_sft_ray\")\n",
    "trainer_dirs = [d for d in save_dir.iterdir() if d.name.startswith(\"TorchTrainer_\") and d.is_dir()]\n",
    "latest_trainer = max(trainer_dirs, key=lambda d: d.stat().st_mtime, default=None)\n",
    "lora_path = f\"{latest_trainer}/checkpoint_000000/checkpoint\"\n",
    "s3_lora_path = os.path.join(os.getenv(\"ANYSCALE_ARTIFACT_STORAGE\"), lora_path.split(\"/mnt/cluster_storage/\")[-1])\n",
    "dynamic_lora_path, lora_id = s3_lora_path.rsplit(\"/\", 1)\n",
    "print (lora_path)\n",
    "print (s3_lora_path)\n",
    "print (dynamic_lora_path)\n",
    "print (lora_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "adapter_config.json\n",
      "adapter_model.safetensors\n",
      "added_tokens.json\n",
      "merges.txt\n",
      "optimizer.pt\n",
      "rng_state_0.pth\n",
      "rng_state_1.pth\n",
      "rng_state_2.pth\n",
      "rng_state_3.pth\n",
      "scheduler.pt\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "trainer_state.json\n",
      "training_args.bin\n",
      "vocab.json\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$lora_path\"\n",
    "ls $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference \n",
    "[`Overview`](https://docs.ray.io/en/latest/data/working-with-llms.html) |  [`API reference`](https://docs.ray.io/en/latest/data/api/llm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ray.data.llm` module integrates with key large language model (LLM) inference engines and deployed models to enable LLM batch inference. These llm modules use [Ray Data](https://docs.ray.io/en/latest/data/data.html) under the hood, which makes it extremely easy to distribute our workloads but also ensures that they happen:\n",
    "- **efficiently**: minimize CPU/GPU idletime with hetergenous resource scheduling.\n",
    "- **at scale**: streaming execution to petabyte-scale datasets (especially when [working with LLMs](https://docs.ray.io/en/latest/data/working-with-llms.html))\n",
    "- **reliably** by checkpointing processes, especially when running workloads on spot instanes (with on-demand fallback).\n",
    "- **flexiblibly**: connect to data from any source, apply your transformations and save to any format/location for your next workload.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/ray_data_solution.png\" width=800>\n",
    "\n",
    "[RayTurbo Data](https://docs.anyscale.com/rayturbo/rayturbo-data) has even more functionality on top of Ray Data:\n",
    "- **accelerated metadata fetching** to improve reading first time from large datasets \n",
    "- **optimized autoscaling** where Jobs can kick off before waiting for the entire cluster to start\n",
    "- **high reliabilty** where entire fails jobs (head node, cluster, uncaptured exceptions, etc.) can resume from checkpoints (OSS Ray can only recover from worker node failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the [vLLM engine processor config](https://docs.ray.io/en/latest/data/api/doc/ray.data.llm.vLLMEngineProcessorConfig.html#ray.data.llm.vLLMEngineProcessorConfig) where we can select the model we want to use and the [engine behavior](https://docs.vllm.ai/en/stable/serving/engine_args.html). The model can come from [HuggingFace (HF) Hub](https://huggingface.co/models) or a local model path `/path/to/your/model` (GPTQ, GGUF, or LoRA model formats supported).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/data_llm.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-11 14:58:40 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = vLLMEngineProcessorConfig(\n",
    "    model_source=model_source,\n",
    "    # runtime_env={\"env_vars\": {\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}},\n",
    "    engine_kwargs={\n",
    "        \"enable_lora\": True,\n",
    "        \"max_lora_rank\": 8,\n",
    "        \"max_loras\": 1,\n",
    "        \"pipeline_parallel_size\": 1, \n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"max_num_batched_tokens\": 4096,\n",
    "        \"max_model_len\": 4096,  # or increase KV cache size \n",
    "        # complete list: https://docs.vllm.ai/en/stable/serving/engine_args.html\n",
    "    },\n",
    "    concurrency=1,\n",
    "    batch_size=16,\n",
    "    accelerator_type=\"A10G\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll pass our config to an [llm processor](https://docs.ray.io/en/master/data/api/doc/ray.data.llm.build_llm_processor.html#ray.data.llm.build_llm_processor) where we can define the preprocessing and postprocessing steps around inference. With our base model defined in the processor config, we can define the lora adapter layers as part of the preprocessing step of the llm processor itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 14:58:40,942\tINFO worker.py:1660 -- Connecting to existing Ray cluster at address: 10.0.51.51:6379...\n",
      "2025-04-11 14:58:40,953\tINFO worker.py:1843 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-zt5t77xa58pyp3uy28glg2g24d.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-04-11 14:58:40,960\tINFO packaging.py:367 -- Pushing file package 'gcs://_ray_pkg_e71d58b4dc01d065456a9fc0325ee2682e13de88.zip' (2.16MiB) to Ray cluster...\n",
      "2025-04-11 14:58:40,969\tINFO packaging.py:380 -- Successfully pushed file package 'gcs://_ray_pkg_e71d58b4dc01d065456a9fc0325ee2682e13de88.zip'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9171027a5a249ff801e77f763506f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=51260)\u001b[0m INFO 04-11 14:58:47 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    }
   ],
   "source": [
    "processor = build_llm_processor(\n",
    "    config,\n",
    "    preprocess=lambda row: dict(\n",
    "        model=lora_path,  # REMOVE this line if doing inference with just the base model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": row[\"input\"]}\n",
    "        ],\n",
    "        sampling_params={\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 250,\n",
    "            # complete list: https://docs.vllm.ai/en/stable/api/inference_params.html\n",
    "        },\n",
    "    ),\n",
    "    postprocess=lambda row: {\n",
    "        **row,  # all contents\n",
    "        \"generated_output\": row[\"generated_text\"],\n",
    "        # add additional outputs\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 14:59:39,912\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-11_12-25-06_198434_2329/logs/ray-data\n",
      "2025-04-11 14:59:39,913\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> ActorPoolMapOperator[Map(_preprocess)->MapBatches(ChatTemplateUDF)] -> ActorPoolMapOperator[MapBatches(TokenizeUDF)] -> ActorPoolMapOperator[MapBatches(vLLMEngineStageUDF)] -> ActorPoolMapOperator[MapBatches(DetokenizeUDF)] -> TaskPoolMapOperator[Map(_postprocess)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b9412b2e7e43be8cad74a1e14d161c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=6151, ip=10.0.43.196)\u001b[0m INFO 04-11 14:59:45 __init__.py:190] Automatically detected platform cuda.\n",
      "\u001b[36m(_MapWorker pid=5808, ip=10.0.46.205)\u001b[0m INFO 04-11 14:59:52 __init__.py:190] Automatically detected platform cuda.\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 14:59:58 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m Max pending requests is set to 141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:06 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m WARNING 04-11 15:00:06 config.py:2162] LoRA with chunked prefill is still experimental and may be unstable.\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:06 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:08 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:09 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.82it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.59it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.58it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.54it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.58it/s]\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:12 model_runner.py:1115] Loading model weights took 14.2487 GB\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:12 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:18 worker.py:267] Memory profiling takes 6.01 seconds\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:18 worker.py:267] the current vLLM instance can use total_gpu_memory (21.99GiB) x gpu_memory_utilization (0.90) = 19.79GiB\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:18 worker.py:267] model weights take 14.25GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 4.07GiB.\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:19 executor_base.py:110] # CUDA blocks: 4760, # CPU blocks: 4681\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:19 executor_base.py:115] Maximum concurrency for 4096 tokens per request: 18.59x\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:24 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:01<00:41,  1.22s/it]\n",
      "Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:29,  1.14it/s]\n",
      "Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:24,  1.30it/s]\n",
      "Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:03<00:22,  1.39it/s]\n",
      "Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.45it/s]\n",
      "Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.49it/s]\n",
      "Capturing CUDA graph shapes:  20%|██        | 7/35 [00:05<00:18,  1.51it/s]\n",
      "Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]\n",
      "Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:16,  1.58it/s]\n",
      "Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.66it/s]\n",
      "Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:13,  1.72it/s]\n",
      "Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:13,  1.77it/s]\n",
      "Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:12,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:11,  1.83it/s]\n",
      "Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:10,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +1m59s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:10,  1.85it/s]\n",
      "Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:09,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +2m0s)\u001b[0m [autoscaler] Downscaling node i-044ab31828ac73c69 (node IP: 10.0.16.199) due to node idle termination.\n",
      "\u001b[36m(autoscaler +2m0s)\u001b[0m [autoscaler] Cluster resized to {144 CPU, 12 GPU}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:08,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:08,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:07,  1.93it/s]\n",
      "Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:12<00:07,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:06,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:13<00:06,  1.95it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:05,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:14<00:05,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:15<00:04,  1.97it/s]\n",
      "Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:15<00:04,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:16<00:03,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:16<00:03,  1.93it/s]\n",
      "Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:17<00:02,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:17<00:02,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:18<00:01,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:18<00:01,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:19<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:44 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.35 GiB\n",
      "\u001b[36m(_MapWorker pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:00:44 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 31.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:19<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=6319, ip=10.0.43.196)\u001b[0m INFO 04-11 15:00:49 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e543df097fe34adab614aab8a01d3171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ListFiles 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70562b92b82e4c25bea8d34f93de603e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadFiles 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4379078f77c84cdca664809182e105a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(_preprocess)->MapBatches(ChatTemplateUDF) 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e6bb7fa63546da935ca89926faaecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(TokenizeUDF) 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bf3f0f901648e28f211719bebda859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(vLLMEngineStageUDF) 5: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77578479edc946fb8f7e5fe2b53e458b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(DetokenizeUDF) 6: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c037e16c46584a4fa45018268950f9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(_postprocess) 7: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:01 metrics.py:455] Avg prompt throughput: 231.8 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 130 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:01 metrics.py:471] Prefix cache hit rate: GPU: 82.41%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 953256301fbf42aab5c4920045f26d03 with size 16: 9.468520947000002\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch ff4aa0da26e74b5799879974cd740b82 with size 16: 9.468874662000076\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch ad2cd2487c8b4ab59867ee2e92195404 with size 16: 9.469709004000038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:06 metrics.py:455] Avg prompt throughput: 8206.5 tokens/s, Avg generation throughput: 891.3 tokens/s, Running: 139 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.7%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:06 metrics.py:471] Prefix cache hit rate: GPU: 82.71%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch ce55bf8f79c84cc7ab9168e23cda45d5 with size 16: 11.01234923200002\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 07bd862787d14ddeb150683f747e7534 with size 16: 11.156081823000022\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch c54007dc0abb44a09f456f58701847ed with size 16: 12.605863096999997\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 1dc806ae738045439bd9c5800c045338 with size 16: 12.606078980000007\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch d7a6b5341cbf4986bb7506ff277cc9cf with size 16: 12.739279831999966\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 939bafbeba384ce8bb55036bec33bfae with size 16: 12.902142667000021\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 23691a1d63b842a695b9bf0949248a32 with size 16: 13.186404474999904\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch d198e43da6db4e70885bb077870d20b2 with size 16: 13.593806072000007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:11 metrics.py:455] Avg prompt throughput: 5789.1 tokens/s, Avg generation throughput: 1054.8 tokens/s, Running: 139 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.5%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:11 metrics.py:471] Prefix cache hit rate: GPU: 82.74%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch ac6e432683694b28a0435a4dbc6742ce with size 16: 15.817252292000035\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch e508dd1cb7254579b3b0216458f15c96 with size 16: 15.950514872999975\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 8dd0e17b54d744539eaa1574b58ea60b with size 16: 16.464633338999988\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 13a3f50706704d669c216a42b92b1b44 with size 16: 16.464724479999973\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch b0df9f4018f24773b1dfd5c61bb68e43 with size 16: 16.768144784000015\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch c6fdde39cd1b4a039688fb0d4a132384 with size 16: 17.720332422999945\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 649270828f9349f6ac21ed04959fe4d2 with size 16: 17.72085921000007\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 39b3e16251cd4e7db2d2169628379917 with size 16: 17.998654923999993\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 51292b81041a4cfd911886c980238c11 with size 16: 18.253415128999904\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 95180a68328d414f9ba92ae8796bed91 with size 16: 19.25512809199995\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch a9a248d1035e4ab2accbe3911918fafd with size 16: 19.25534504299992\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch aa2b62cf9a804601873073a8559ac6b6 with size 16: 19.38725856399992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:17 metrics.py:455] Avg prompt throughput: 6273.8 tokens/s, Avg generation throughput: 1034.9 tokens/s, Running: 139 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.3%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:17 metrics.py:471] Prefix cache hit rate: GPU: 83.12%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 1e1ad36732c94561ad24a77f5658b26a with size 16: 19.856246397000064\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 951d4d5d946a45cf9bca7b7bab03f4de with size 16: 20.23295990600002\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch cd8f8c14ab7e4aada972a709e51d0c45 with size 16: 21.99705536299996\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 64b2748965874b8db23018db13d9bc47 with size 16: 22.646705173999976\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch b0f6af19489b487aa8ea62ca1ab058fe with size 16: 22.780421508000018\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch fd837a98e5dd4f4e8577a82a422573d1 with size 16: 22.780099164000035\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 8fc87d66a96f4cf8b4307b34444dd7a4 with size 16: 23.054458213999965\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch a12535a26160470f967024a04b75a032 with size 16: 23.481234467000036\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch d8514ce099ab445899816d3e440eea65 with size 16: 23.596987243000058\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch c5784fa6a199410690d7c6e206abcd5d with size 16: 24.1956411970001\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 2088bf8d44cd4737bfa6c83e627ef22c with size 16: 24.35931167900003\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 89b8a0e2ad42456a8918374e6ef62394 with size 16: 24.723402340000007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:22 metrics.py:455] Avg prompt throughput: 6773.7 tokens/s, Avg generation throughput: 1004.7 tokens/s, Running: 139 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.1%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:22 metrics.py:471] Prefix cache hit rate: GPU: 83.43%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch e0c78ee2a42542a99e5c87034ee1e564 with size 16: 25.094723236999926\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 37e80976a29f46bdb97ef21f2599a3e9 with size 16: 25.504361700999993\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch c9f9ed1c966d4b86b1a5a8ce886a4362 with size 16: 26.32410293999999\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch da923f2bd9ab440790018c184ecda97f with size 16: 26.636943299999984\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 55e0a09eeb5e4db9aa76477bfc738677 with size 16: 26.63729204499998\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch a71a11586d93404c88684ed123ee2d22 with size 16: 27.104517794999992\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 977993b516b7466fb9556337d850ee6d with size 16: 27.245912871000087\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 316ff60b04ea4e1181ed1a2cb8d25da2 with size 16: 28.125030405000075\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch a3140b4f5d4b4f32b480466133ffb4e4 with size 16: 28.59486228000003\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 9ed8bd24984b4a64a1883d2d9c4288c5 with size 16: 29.13451922099989\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch e4a745990dd94240a786c5f51837d430 with size 16: 29.409228686000006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:27 metrics.py:455] Avg prompt throughput: 6771.5 tokens/s, Avg generation throughput: 984.1 tokens/s, Running: 131 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.1%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:27 metrics.py:471] Prefix cache hit rate: GPU: 83.54%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 8dda9832b6a54a6a9ebcc786cc3b1e45 with size 16: 29.9559936710001\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch e3a93ba1fcb940d0af1962af747fbdab with size 16: 30.719505132999984\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 91177a8f348147fea5584ff467e0fe77 with size 16: 30.799654365000038\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 337cf1f95e4145de9511d39785c35e74 with size 16: 31.081981982000002\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch a366709a7feb4d42939a8648f8e8a92f with size 16: 31.223188635999918\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 8e7cc687d0574478b4afa525ff47a8ab with size 16: 31.605468459000008\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch a71691414be9456e8295c10ad3aa8dc4 with size 16: 33.41769935499997\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch bb82d90832954e64b725184a2239bcb9 with size 16: 33.41799498800003\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 2228c19f4cc04d97a2ee79091053fb93 with size 16: 33.553426786000045\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 96839a943f124199b9c5ac1af73290f1 with size 16: 33.746320795999964\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 028adde0266c40f2b1fc63fd3b895bc0 with size 16: 33.74645076699994\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 879f1c167a5d40ae90ccf60cd49edc13 with size 16: 33.808900386000005\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch a1cd522377184c08af5c3524daf9716f with size 16: 33.869914465999955\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 5c94157ff38f4b68b3bb987a98493d95 with size 16: 33.86987858500004\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 0135b3ce3cb84b0c85c4d6ee44aefbce with size 16: 34.45721783900001\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 9aeba7459f1a42a9ad5670349cc0514c with size 11: 34.59110982700008\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 289a6e334e624480973f247ce96aa684 with size 16: 34.83549549199995\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 70299fd7c2cf4540a8797cd9011003b8 with size 16: 34.83594015799997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:32 metrics.py:455] Avg prompt throughput: 3843.5 tokens/s, Avg generation throughput: 1044.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:32 metrics.py:471] Prefix cache hit rate: GPU: 83.43%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 4ccff6afe90e4dc591cbef29f208edf4 with size 16: 35.07814248400007\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch a7a9a6c6e015467e83dc64c39c773658 with size 16: 35.07880637300002\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 1474d8dc69c04ed0b7668de4cbb1b5f8 with size 16: 35.22839372999999\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=5883, ip=10.0.33.71)\u001b[0m [vLLM] Elapsed time for batch 0d02bceee4cc40638202a68f55b512aa with size 16: 35.264812354000014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_uuid': 'd7a6b5341cbf4986bb7506ff277cc9cf',\n",
       " 'embeddings': None,\n",
       " 'generated_text': 'request(esrb)',\n",
       " 'generated_tokens': [2035, 50236, 10681, 8, 151645],\n",
       " 'input': 'Do you have a favorite ESRB content rating?',\n",
       " 'instruction': \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
       " 'messages': [{'content': \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
       "   'role': 'system'},\n",
       "  {'content': 'Do you have a favorite ESRB content rating?', 'role': 'user'}],\n",
       " 'metrics': {'arrival_time': 1744408857.148983,\n",
       "  'finished_time': 1744408863.09091,\n",
       "  'first_scheduled_time': 1744408859.130259,\n",
       "  'first_token_time': 1744408862.7087252,\n",
       "  'last_token_time': 1744408863.089174,\n",
       "  'model_execute_time': None,\n",
       "  'model_forward_time': None,\n",
       "  'scheduler_time': 0.04162892400017881,\n",
       "  'time_in_queue': 1.981276035308838},\n",
       " 'model': '/mnt/cluster_storage/viggo/saves/lora_sft_ray/TorchTrainer_95d16_00000_0_2025-04-11_14-47-37/checkpoint_000000/checkpoint',\n",
       " 'num_generated_tokens': 5,\n",
       " 'num_input_tokens': 164,\n",
       " 'output': 'request_attribute(esrb[])',\n",
       " 'params': 'SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.3, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=250, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None)',\n",
       " 'prompt': \"<|im_start|>system\\nGiven a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']<|im_end|>\\n<|im_start|>user\\nDo you have a favorite ESRB content rating?<|im_end|>\\n<|im_start|>assistant\\n\",\n",
       " 'prompt_token_ids': [151644,\n",
       "  8948,\n",
       "  198,\n",
       "  22043,\n",
       "  264,\n",
       "  2169,\n",
       "  11652,\n",
       "  9245,\n",
       "  279,\n",
       "  16533,\n",
       "  7290,\n",
       "  13042,\n",
       "  315,\n",
       "  279,\n",
       "  1946,\n",
       "  11652,\n",
       "  438,\n",
       "  264,\n",
       "  3175,\n",
       "  729,\n",
       "  448,\n",
       "  8201,\n",
       "  323,\n",
       "  7035,\n",
       "  2750,\n",
       "  13,\n",
       "  1096,\n",
       "  729,\n",
       "  1265,\n",
       "  7512,\n",
       "  279,\n",
       "  2169,\n",
       "  914,\n",
       "  29257,\n",
       "  323,\n",
       "  279,\n",
       "  729,\n",
       "  1969,\n",
       "  387,\n",
       "  825,\n",
       "  315,\n",
       "  279,\n",
       "  2701,\n",
       "  2509,\n",
       "  40440,\n",
       "  516,\n",
       "  364,\n",
       "  2035,\n",
       "  516,\n",
       "  364,\n",
       "  46430,\n",
       "  10287,\n",
       "  36300,\n",
       "  516,\n",
       "  364,\n",
       "  13800,\n",
       "  516,\n",
       "  364,\n",
       "  12446,\n",
       "  16791,\n",
       "  516,\n",
       "  364,\n",
       "  95761,\n",
       "  516,\n",
       "  364,\n",
       "  2035,\n",
       "  2702,\n",
       "  35890,\n",
       "  516,\n",
       "  364,\n",
       "  66589,\n",
       "  516,\n",
       "  364,\n",
       "  2035,\n",
       "  16791,\n",
       "  7204,\n",
       "  576,\n",
       "  8201,\n",
       "  1969,\n",
       "  387,\n",
       "  825,\n",
       "  315,\n",
       "  279,\n",
       "  2701,\n",
       "  25,\n",
       "  2509,\n",
       "  606,\n",
       "  516,\n",
       "  364,\n",
       "  4580,\n",
       "  24577,\n",
       "  4164,\n",
       "  516,\n",
       "  364,\n",
       "  22998,\n",
       "  14645,\n",
       "  516,\n",
       "  364,\n",
       "  34401,\n",
       "  516,\n",
       "  364,\n",
       "  288,\n",
       "  10681,\n",
       "  516,\n",
       "  364,\n",
       "  21931,\n",
       "  516,\n",
       "  364,\n",
       "  63911,\n",
       "  516,\n",
       "  364,\n",
       "  3434,\n",
       "  620,\n",
       "  85091,\n",
       "  516,\n",
       "  364,\n",
       "  4648,\n",
       "  25133,\n",
       "  3434,\n",
       "  516,\n",
       "  364,\n",
       "  15734,\n",
       "  82,\n",
       "  516,\n",
       "  364,\n",
       "  10334,\n",
       "  4470,\n",
       "  1261,\n",
       "  14580,\n",
       "  516,\n",
       "  364,\n",
       "  4648,\n",
       "  77463,\n",
       "  24577,\n",
       "  516,\n",
       "  364,\n",
       "  4648,\n",
       "  22802,\n",
       "  24577,\n",
       "  516,\n",
       "  364,\n",
       "  67251,\n",
       "  660,\n",
       "  151645,\n",
       "  198,\n",
       "  151644,\n",
       "  872,\n",
       "  198,\n",
       "  5404,\n",
       "  498,\n",
       "  614,\n",
       "  264,\n",
       "  6930,\n",
       "  468,\n",
       "  14557,\n",
       "  33,\n",
       "  2213,\n",
       "  10728,\n",
       "  30,\n",
       "  151645,\n",
       "  198,\n",
       "  151644,\n",
       "  77091,\n",
       "  198],\n",
       " 'request_id': 94,\n",
       " 'time_taken_llm': 6.028705836999961,\n",
       " 'generated_output': 'request(esrb)'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation on test dataset\n",
    "ds = ray.data.read_json(\"/mnt/cluster_storage/viggo/test.jsonl\")  # complete list: https://docs.ray.io/en/latest/data/api/input_output.html\n",
    "ds = processor(ds)\n",
    "results = ds.take_all()\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6879039704524469"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exact match (strict!)\n",
    "matches = 0\n",
    "for item in results:\n",
    "    if item[\"output\"] == item[\"generated_output\"]:\n",
    "        matches += 1\n",
    "matches / float(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the objective of fine-tuning here is not to create the most performant model (increase `num_train_epochs` if you want to though) but to show it can be leveraged for downstream workloads (batch inference and online serving) at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the individual steps in our our batch inference workload through the Anyscale Ray Data dashboard:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/data_dashboard.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "💡 For more advanced guides on topics like optimized model loading, multi-lora, openai-compatible endpoints, etc. check out [more examples](https://docs.ray.io/en/latest/data/working-with-llms.html) and the [API reference](https://docs.ray.io/en/latest/data/api/llm.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online serving\n",
    "[`Overview`](https://docs.ray.io/en/latest/serve/llm/serving-llms.html) | [`API reference`](https://docs.ray.io/en/latest/serve/api/index.html#llm-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/ray_serve.png\" width=600>\n",
    "\n",
    "`ray.serve.llm` APIs allow users to deploy multiple LLM models together with a familiar Ray Serve API, while providing compatibility with the OpenAI API.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/serve_llm.png\" width=500>\n",
    "\n",
    "Ray Serve LLM is designed with the following features:\n",
    "- Automatic scaling and load balancing\n",
    "- Unified multi-node multi-model deployment\n",
    "- OpenAI compatibility\n",
    "- Multi-LoRA support with shared base models\n",
    "- Deep integration with inference engines (vLLM to start)\n",
    "- Composable multi-model LLM pipelines\n",
    "\n",
    "[RayTurbo Serve](https://docs.anyscale.com/rayturbo/rayturbo-serve) on Anyscale has even more functionality on top of Ray Serve:\n",
    "- **fast autoscaling and model loading** to get our services up and running even faster ([5x improvements](https://www.anyscale.com/blog/autoscale-large-ai-models-faster) even for LLMs)\n",
    "- 54% **higher QPS** and up-to 3x **streaming tokens per second** for high traffic serving use-cases\n",
    "- **replica compaction** into fewer nodes where possible to reduce resource fragmentation and improve hardware utilization\n",
    "- **zero-downtime** [incremental rollouts](https://docs.anyscale.com/platform/services/update-a-service/#resource-constrained-updates) so your service is never interrupted\n",
    "- [**different environments**](https://docs.anyscale.com/platform/services/multi-app/#multiple-applications-in-different-containers) for each service in a multi-serve application\n",
    "- **multi availability-zone** aware scheduling of Ray Serve replicas to provide higher redundancy to availability zone failures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI  # to use openai api format\n",
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, build_openai_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an [LLM config](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) where we can define where our model comes from, it's [autoscaling behavior](https://docs.ray.io/en/latest/serve/autoscaling-guide.html#serve-autoscaling), what hardware to use and [engine arguments](https://docs.vllm.ai/en/stable/serving/engine_args.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config={\n",
    "        \"model_id\": model_id,\n",
    "        \"model_source\": model_source\n",
    "    },\n",
    "    lora_config={  # REMOVE this section if you are only using a base model\n",
    "        \"dynamic_lora_loading_path\": dynamic_lora_path,\n",
    "        \"max_num_adapters_per_replica\": 16,  # we only have 1\n",
    "    },\n",
    "    # runtime_env={\"env_vars\": {\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}},\n",
    "    deployment_config={\n",
    "        \"autoscaling_config\": {\n",
    "            \"min_replicas\": 1, \n",
    "            \"max_replicas\": 2,\n",
    "            # complete list: https://docs.ray.io/en/latest/serve/autoscaling-guide.html#serve-autoscaling\n",
    "        }\n",
    "    },\n",
    "    accelerator_type=\"A10G\",\n",
    "    engine_kwargs={\n",
    "        \"max_model_len\": 4096,  # or increase KV cache size\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"enable_lora\": True,\n",
    "        # complete list: https://docs.vllm.ai/en/stable/serving/engine_args.html\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll deploy our llm config as an application. And since this is all built on top of [Ray Serve](https://docs.ray.io/en/latest/serve/index.html), we can have advanvced service logic around composing models together, deploying multiple applications, model multiplexing, observability, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ProxyActor pid=52148)\u001b[0m INFO 2025-04-11 15:01:45,123 proxy 10.0.51.51 -- Proxy starting on node ae1bdcfe62f6e8fa81c68e97ebd254e84a668d7d2a390c9592aa61c5 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=52148)\u001b[0m INFO 2025-04-11 15:01:45,184 proxy 10.0.51.51 -- Got updated endpoints: {}.\n",
      "INFO 2025-04-11 15:01:45,213 serve 4035 -- Started Serve in namespace \"serve\".\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m INFO 2025-04-11 15:01:45,235 controller 52082 -- Deploying new version of Deployment(name='LLMDeployment:ft-model', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m INFO 2025-04-11 15:01:45,236 controller 52082 -- Deploying new version of Deployment(name='LLMRouter', app='default') (initial target replicas: 2).\n",
      "\u001b[36m(ProxyActor pid=52148)\u001b[0m INFO 2025-04-11 15:01:45,239 proxy 10.0.51.51 -- Got updated endpoints: {Deployment(name='LLMRouter', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=52148)\u001b[0m INFO 2025-04-11 15:01:45,256 proxy 10.0.51.51 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7fcb985a7090>.\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m INFO 2025-04-11 15:01:45,342 controller 52082 -- Adding 1 replica to Deployment(name='LLMDeployment:ft-model', app='default').\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m INFO 2025-04-11 15:01:45,344 controller 52082 -- Adding 2 replicas to Deployment(name='LLMRouter', app='default').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMRouter pid=6415, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:51 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:01:51,779 default_LLMDeployment:ft-model jeea28iu -- No cloud storage mirror configured\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:01:51,780 default_LLMDeployment:ft-model jeea28iu -- Downloading the tokenizer for Qwen/Qwen2.5-7B-Instruct\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m You are using a model of type qwen2 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m You are using a model of type qwen2 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ProxyActor pid=6419, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:01:52,590 proxy 10.0.33.71 -- Proxy starting on node 6441c36a31e630f348b2ebf6ced9a0430984987bab9dc55260429f50 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=6419, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:01:52,642 proxy 10.0.33.71 -- Got updated endpoints: {Deployment(name='LLMRouter', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=6419, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:01:52,656 proxy 10.0.33.71 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x77f18d0d7b50>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:00 config.py:542] This model supports multiple tasks: {'classify', 'embed', 'reward', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 04-11 15:01:51 __init__.py:190] Automatically detected platform cuda.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:01,239 default_LLMDeployment:ft-model jeea28iu -- [STATUS] Getting the server ready ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:06 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:06,738 serve 6938 -- Clearing the current platform cache ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:06 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m Connecting to existing Ray cluster at address: 10.0.51.51:6379...\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:07 ray_distributed_executor.py:149] use_ray_spmd_worker: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:11,289 default_LLMDeployment:ft-model jeea28iu -- [STATUS] Waiting for engine process ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=7029, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:12 __init__.py:190] Automatically detected platform cuda.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:13 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:14 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:14 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m WARNING 2025-04-11 15:02:15,378 controller 52082 -- Deployment 'LLMDeployment:ft-model' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m WARNING 2025-04-11 15:02:15,379 controller 52082 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.80it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.57it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.56it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.52it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.56it/s]\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:17 model_runner.py:1115] Loading model weights took 14.2487 GB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:17 punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:22,340 default_LLMDeployment:ft-model jeea28iu -- [STATUS] Waiting for engine process ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:22 worker.py:267] Memory profiling takes 4.28 seconds\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:22 worker.py:267] the current vLLM instance can use total_gpu_memory (21.99GiB) x gpu_memory_utilization (0.90) = 19.79GiB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:22 worker.py:267] model weights take 14.25GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 4.07GiB.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:22 executor_base.py:110] # CUDA blocks: 4760, # CPU blocks: 4681\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:22 executor_base.py:115] Maximum concurrency for 4096 tokens per request: 18.59x\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:27 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:31,  1.07it/s]\n",
      "Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:25,  1.29it/s]\n",
      "Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:23,  1.38it/s]\n",
      "Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:21,  1.43it/s]\n",
      "Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.46it/s]\n",
      "Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.48it/s]\n",
      "Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.49it/s]\n",
      "Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.50it/s]\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:33,391 default_LLMDeployment:ft-model jeea28iu -- [STATUS] Waiting for engine process ...\n",
      "Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:16,  1.53it/s]\n",
      "Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]\n",
      "Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:14,  1.64it/s]\n",
      "Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:13,  1.69it/s]\n",
      "Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:12,  1.72it/s]\n",
      "Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:11,  1.75it/s]\n",
      "Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:11,  1.76it/s]\n",
      "Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:10,  1.75it/s]\n",
      "Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:10,  1.77it/s]\n",
      "Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:09,  1.79it/s]\n",
      "Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:08,  1.81it/s]\n",
      "Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:08,  1.83it/s]\n",
      "Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:12<00:07,  1.83it/s]\n",
      "Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:07,  1.84it/s]\n",
      "Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:13<00:06,  1.84it/s]\n",
      "Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:05,  1.85it/s]\n",
      "Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:14<00:05,  1.86it/s]\n",
      "Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:15<00:04,  1.86it/s]\n",
      "Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.86it/s]\n",
      "Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:16<00:03,  1.87it/s]\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:44,441 default_LLMDeployment:ft-model jeea28iu -- [STATUS] Waiting for engine process ...\n",
      "Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:17<00:03,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +4m10s)\u001b[0m [autoscaler] Downscaling node i-0715a4e5fd9d16900 (node IP: 10.0.43.196) due to node idle termination.\n",
      "\u001b[36m(autoscaler +4m10s)\u001b[0m [autoscaler] Downscaling node i-06261b849e689cc65 (node IP: 10.0.46.205) due to node idle termination.\n",
      "\u001b[36m(autoscaler +4m10s)\u001b[0m [autoscaler] Cluster resized to {48 CPU, 4 GPU}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:17<00:02,  1.87it/s]\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m WARNING 2025-04-11 15:02:45,398 controller 52082 -- Deployment 'LLMDeployment:ft-model' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m WARNING 2025-04-11 15:02:45,399 controller 52082 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=52082)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:18<00:02,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:18<00:01,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:19<00:01,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:19<00:00,  1.87it/s]\n",
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:20<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:47 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.35 GiB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:02:47 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 30.06 seconds\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 4 PYTHON worker processes have been started on node: ae1bdcfe62f6e8fa81c68e97ebd254e84a668d7d2a390c9592aa61c5 with address: 10.0.51.51. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:48,093 default_LLMDeployment:ft-model jeea28iu -- [STATUS] Server is ready.\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:48,093 default_LLMDeployment:ft-model jeea28iu -- Started vLLM engine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=52529)\u001b[0m INFO 04-11 15:02:54 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:55,458 default_LLMDeployment:ft-model jeea28iu d154be60-f7e1-4ad8-bc78-ae7f30be2ea9 -- CALL llm_config OK 188.0ms\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:55,461 default_LLMDeployment:ft-model jeea28iu 01535536-63f6-4dc5-b215-279e90671346 -- CALL llm_config OK 190.2ms\n",
      "INFO 2025-04-11 15:02:56,485 serve 4035 -- Application 'default' is ready at http://127.0.0.1:8000/.\n",
      "INFO 2025-04-11 15:02:56,493 serve 4035 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7a77f6cf18d0>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeploymentHandle(deployment='LLMRouter')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})\n",
    "serve.run(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:56,699 default_LLMDeployment:ft-model jeea28iu 9a3a3010-f7e3-4829-8043-9627ecdb40ff -- Received streaming request 9a3a3010-f7e3-4829-8043-9627ecdb40ff\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:56,700 default_LLMDeployment:ft-model jeea28iu 9a3a3010-f7e3-4829-8043-9627ecdb40ff -- Loading model 'ft-model:checkpoint'.\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:02:56,781 default_LLMDeployment:ft-model jeea28iu -- Downloading s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage/viggo/saves/lora_sft_ray/TorchTrainer_95d16_00000_0_2025-04-11_14-47-37/checkpoint_000000/checkpoint to /tmp/ray/llm/lora/cache/checkpoint\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:03:06,580 default_LLMDeployment:ft-model jeea28iu 9a3a3010-f7e3-4829-8043-9627ecdb40ff -- Successfully loaded model 'ft-model:checkpoint' in 9880.0ms.\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:03:06,591 default_LLMDeployment:ft-model jeea28iu 9a3a3010-f7e3-4829-8043-9627ecdb40ff -- Request 9a3a3010-f7e3-4829-8043-9627ecdb40ff started. Prompt: <|im_start|>system\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']<|im_end|>\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.<|im_end|>\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m \n",
      "{\"asctime\": \"2025-04-11 15:03:07,200\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"job_id\": \"05000000\", \"worker_id\": \"05000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"ae1bdcfe62f6e8fa81c68e97ebd254e84a668d7d2a390c9592aa61c5\", \"timestamp_ns\": 1744408987200210903}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:03:07 engine.py:275] Added request 9a3a3010-f7e3-4829-8043-9627ecdb40ff.\n",
      "give\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:03:07 metrics.py:455] Avg prompt throughput: 20.3 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])"
     ]
    }
   ],
   "source": [
    "# Initialize client\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"fake-key\")\n",
    "response = client.chat.completions.create(\n",
    "    model=f\"{model_id}:{lora_id}\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\"},\n",
    "        {\"role\": \"user\", \"content\": \"Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.\"},\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can observe our running service (deployments and metrics like QPS, latency, etc.) through the [Ray Dashboard](https://docs.ray.io/en/latest/ray-observability/getting-started.html)'s [Serve view](https://docs.ray.io/en/latest/ray-observability/getting-started.html#dash-serve-view):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/serve_dashboard.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "💡 For more advanced guides on topics like structured outputs (ex. json), vision LMs, multi-lora on shared base models, using other inference engines (ex. sglang), etc. fast model loading, etc. check out [more examples](https://docs.ray.io/en/latest/serve/llm/overview.html) and the [API reference](https://docs.ray.io/en/latest/serve/llm/api.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production\n",
    "\n",
    "Seamlessly integrate with your existing CI/CD pipelines by leveraging the Anyscale [CLI](https://docs.anyscale.com/reference/quickstart-cli) or [SDK](https://docs.anyscale.com/reference/quickstart-sdk) to run [reliable batch jobs](https://docs.anyscale.com/platform/jobs) and deploy [highly available services](https://docs.anyscale.com/platform/services). Given we've been developing in an environment that's almost identical to production (multinode cluster), this should drastically speed up our dev → prod velocity.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/cicd.png\" width=600>\n",
    "\n",
    "[Anyscale Jobs](https://docs.anyscale.com/platform/jobs/) ([API ref](https://docs.anyscale.com/reference/job-api/)) allows us to execute discrete workloads in production such as batch inference, embeddings generation, or model fine-tuning.\n",
    "- [define and manage](https://docs.anyscale.com/platform/jobs/manage-jobs) our Jobs in many different ways (CLI, Python SDK)\n",
    "- set up [queues](https://docs.anyscale.com/platform/jobs/job-queues) and [schedules](https://docs.anyscale.com/platform/jobs/schedules)\n",
    "- set up all the [observability, alerting, etc.](https://docs.anyscale.com/platform/jobs/monitoring-and-debugging) around our Jobs\n",
    "\n",
    "[Anyscale Services](https://docs.anyscale.com/platform/services/) ([API ref](https://docs.anyscale.com/reference/service-api/)) offers an extremely fault tolerant, scalable and optimized way to serve our Ray Serve applications.\n",
    "- we can [rollout and update](https://docs.anyscale.com/platform/services/update-a-service) our services with canary deployment (zero-downtime upgrades)\n",
    "- [monitor](https://docs.anyscale.com/platform/services/monitoring) our Services through a dedicated Service page, unified log viewer, tracing, set up alerts, etc.\n",
    "- scale a service (`num_replicas=auto`) and utilize replica compaction to consolidate nodes that are fractionally utilized\n",
    "- [head node fault tolerance](https://docs.anyscale.com/platform/services/production-best-practices#head-node-ft) (OSS Ray recovers from failed workers and replicas but not head node crashes)\n",
    "- serving [muliple applications](https://docs.anyscale.com/platform/services/multi-app) in a single Service\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/canary.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:03:08 engine.py:293] Aborted request 9a3a3010-f7e3-4829-8043-9627ecdb40ff.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:03:08,061 default_LLMDeployment:ft-model jeea28iu 9a3a3010-f7e3-4829-8043-9627ecdb40ff -- Request 9a3a3010-f7e3-4829-8043-9627ecdb40ff finished (stop). Total time: 1.4686842349999552s, Queue time: 0.48091769218444824s, Generation+async time: 0.9877665428155069s, Input tokens: 184, Generated tokens: 26, tokens/s: 212.6008433140698, generated tokens/s: 26.322009172218166.\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:ft-model pid=6418, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:03:08,062 default_LLMDeployment:ft-model jeea28iu 9a3a3010-f7e3-4829-8043-9627ecdb40ff -- CALL /v1/chat/completions OK 11363.5ms\n",
      "\u001b[36m(ServeReplica:default:LLMRouter pid=6415, ip=10.0.33.71)\u001b[0m INFO 2025-04-11 15:03:08,064 default_LLMRouter d4653yy0 9a3a3010-f7e3-4829-8043-9627ecdb40ff -- POST /v1/chat/completions 200 11372.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:03:18 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=6938, ip=10.0.33.71)\u001b[0m INFO 04-11 15:03:28 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# clean up\n",
    "rm -rf /mnt/cluster_storage/viggo  # clean up\n",
    "aws s3 rm $ANYSCALE_ARTIFACT_STORAGE/viggo --recursive --quiet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
