{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Recognition with LLMs\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/🚀 Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/e2e-llm-workflows\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune an LLM to perform batch inference and online serving for entity recognition. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/e2e_llm.png\" width=800>\n",
    "\n",
    "**Note**: the intent of this tutorial is to show how Ray can be use to implement end-to-end LLM workflows that can extend to any use case. Also the objective of fine-tuning here is not to create the most performant model (increase `num_train_epochs` if you want to though) but to show it can be leveraged for downstream workloads (batch inference and online serving) at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "If you're on [Anyscale](https://console.anyscale.com/), you can run this entire tutorial for free (all dependencies are setup and the necessary compute will autoscale). Otherwise be sure to install the dependencies from the [`containerfile`](containerfile) and provision the appropriate GPU resources (4xA10s).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/compute.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"<INSERT_HF_TOKEN>\"  # or use .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Be sure to add your [HuggingFace token](https://huggingface.co/settings/tokens) (`HF_TOKEN=<HF_TOKEN>`) (with access to the model you want to use) and `HF_HUB_ENABLE_HF_TRANSFER=1` (enbales faster uploads and downloads from HF hub) to the *Dependencies* tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import textwrap\n",
    "from IPython.display import Code, Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by downloading our data from cloud storage to local shared storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://viggo-ds/train.jsonl to ../../../mnt/cluster_storage/viggo/train.jsonl\n",
      "download: s3://viggo-ds/val.jsonl to ../../../mnt/cluster_storage/viggo/val.jsonl\n",
      "download: s3://viggo-ds/test.jsonl to ../../../mnt/cluster_storage/viggo/test.jsonl\n",
      "download: s3://viggo-ds/dataset_info.json to ../../../mnt/cluster_storage/viggo/dataset_info.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf /mnt/cluster_storage/viggo  # clean up\n",
    "aws s3 cp  s3://viggo-ds/train.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/val.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/test.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/dataset_info.json /mnt/cluster_storage/viggo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"instruction\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
      "    \"input\": \"Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.\",\n",
      "    \"output\": \"give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 1 /mnt/cluster_storage/viggo/train.jsonl | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a target sentence construct the underlying meaning representation of the\n",
      "input sentence as a single function with attributes and attribute values. This\n",
      "function should describe the target string accurately and the function must be\n",
      "one of the following ['inform', 'request', 'give_opinion', 'confirm',\n",
      "'verify_attribute', 'suggest', 'request_explanation', 'recommend',\n",
      "'request_attribute']. The attributes must be one of the following: ['name',\n",
      "'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres',\n",
      "'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam',\n",
      "'has_linux_release', 'has_mac_release', 'specifier']\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/cluster_storage/viggo/train.jsonl\", \"r\") as fp:\n",
    "    first_line = fp.readline()\n",
    "    item = json.loads(first_line)\n",
    "system_content = item[\"instruction\"]\n",
    "print(textwrap.fill(system_content, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have an info file that identifies the datasets and format --- alpaca and sharegpt (great for multimodal tasks) formats are supported --- to use for post training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"p\">{</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;viggo-train&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;file_name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;/mnt/cluster_storage/viggo/train.jsonl&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;formatting&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;alpaca&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;columns&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;prompt&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;instruction&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;query&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;response&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;output&quot;</span>\n",
       "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">},</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;viggo-val&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;file_name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;/mnt/cluster_storage/viggo/val.jsonl&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;formatting&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;alpaca&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;columns&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;prompt&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;instruction&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;query&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;response&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;output&quot;</span>\n",
       "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}viggo\\PYZhy{}train\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}file\\PYZus{}name\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}/mnt/cluster\\PYZus{}storage/viggo/train.jsonl\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}formatting\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}alpaca\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}columns\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}prompt\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}instruction\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}query\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}input\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}response\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}output\\PYZdq{}}\n",
       "\\PY{+w}{        }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{},}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}viggo\\PYZhy{}val\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}file\\PYZus{}name\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}/mnt/cluster\\PYZus{}storage/viggo/val.jsonl\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}formatting\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}alpaca\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}columns\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}prompt\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}instruction\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}query\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}input\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}response\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}output\\PYZdq{}}\n",
       "\\PY{+w}{        }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{p}{\\PYZcb{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "{\n",
       "    \"viggo-train\": {\n",
       "        \"file_name\": \"/mnt/cluster_storage/viggo/train.jsonl\",\n",
       "        \"formatting\": \"alpaca\",\n",
       "        \"columns\": {\n",
       "            \"prompt\": \"instruction\",\n",
       "            \"query\": \"input\",\n",
       "            \"response\": \"output\"\n",
       "        }\n",
       "    },\n",
       "    \"viggo-val\": {\n",
       "        \"file_name\": \"/mnt/cluster_storage/viggo/val.jsonl\",\n",
       "        \"formatting\": \"alpaca\",\n",
       "        \"columns\": {\n",
       "            \"prompt\": \"instruction\",\n",
       "            \"query\": \"input\",\n",
       "            \"response\": \"output\"\n",
       "        }\n",
       "    }\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"/mnt/cluster_storage/viggo/dataset_info.json\", language=\"json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [Ray Train](https://docs.ray.io/en/latest/train/train.html) + [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) to peform multinode training. The parameters for our training workload -- post-training method, dataset location, train/val details, etc. --- can be found in the `llama3_lora_sft_ray.yaml` config file. Check out recipes for even more post-training methods (sft, pretraining, ppo, dpo, kto, etc.) [here](https://github.com/hiyouga/LLaMA-Factory/tree/main/examples).\n",
    "\n",
    "**Note**: We also support using other tools like [axolotl](https://axolotl-ai-cloud.github.io/axolotl/docs/ray-integration.html) or even [Ray Train + HF Accelreate + FSDP/Deepspeed](https://docs.ray.io/en/latest/train/huggingface-accelerate.html) directly for complete control of your post-training workloads.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/distributed_training.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\">### model</span>\n",
       "<span class=\"nt\">model_name_or_path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">meta-llama/Meta-Llama-3-8B-Instruct</span>\n",
       "<span class=\"nt\">trust_remote_code</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "\n",
       "<span class=\"c1\">### method</span>\n",
       "<span class=\"nt\">stage</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">sft</span>\n",
       "<span class=\"nt\">do_train</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">finetuning_type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">lora</span>\n",
       "<span class=\"nt\">lora_rank</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">8</span>\n",
       "<span class=\"nt\">lora_target</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">all</span>\n",
       "\n",
       "<span class=\"c1\">### dataset</span>\n",
       "<span class=\"nt\">dataset</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">viggo-train</span>\n",
       "<span class=\"nt\">dataset_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo</span><span class=\"w\">  </span><span class=\"c1\"># shared storage workers have access to</span>\n",
       "<span class=\"nt\">template</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">llama3</span>\n",
       "<span class=\"nt\">cutoff_len</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">2048</span>\n",
       "<span class=\"nt\">max_samples</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1000</span>\n",
       "<span class=\"nt\">overwrite_cache</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">preprocessing_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">16</span>\n",
       "<span class=\"nt\">dataloader_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">4</span>\n",
       "\n",
       "<span class=\"c1\">### output</span>\n",
       "<span class=\"nt\">output_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo/outputs</span><span class=\"w\">  </span><span class=\"c1\"># should be somewhere workers have access to (ex. s3, nfs)</span>\n",
       "<span class=\"nt\">logging_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">10</span>\n",
       "<span class=\"nt\">save_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">500</span>\n",
       "<span class=\"nt\">plot_loss</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">overwrite_output_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">save_only_model</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">false</span>\n",
       "\n",
       "<span class=\"c1\">### ray</span>\n",
       "<span class=\"nt\">ray_run_name</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">llama3_8b_sft_lora</span>\n",
       "<span class=\"nt\">ray_storage_path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo/saves</span><span class=\"w\">  </span><span class=\"c1\"># should be somewhere workers have access to (ex. s3, nfs)</span>\n",
       "<span class=\"nt\">ray_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">4</span><span class=\"w\">  </span><span class=\"c1\"># number of GPUs to use</span>\n",
       "<span class=\"nt\">resources_per_worker</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">GPU</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"nt\">placement_strategy</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">PACK</span>\n",
       "\n",
       "<span class=\"c1\">### train</span>\n",
       "<span class=\"nt\">per_device_train_batch_size</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"nt\">gradient_accumulation_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">8</span>\n",
       "<span class=\"nt\">learning_rate</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1.0e-4</span>\n",
       "<span class=\"nt\">num_train_epochs</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">3.0</span>\n",
       "<span class=\"nt\">lr_scheduler_type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">cosine</span>\n",
       "<span class=\"nt\">warmup_ratio</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">0.1</span>\n",
       "<span class=\"nt\">bf16</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">ddp_timeout</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">180000000</span>\n",
       "<span class=\"nt\">resume_from_checkpoint</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">null</span>\n",
       "\n",
       "<span class=\"c1\">### eval</span>\n",
       "<span class=\"nt\">eval_dataset</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">viggo-val</span><span class=\"w\">  </span><span class=\"c1\"># uses same dataset_dir as training data</span>\n",
       "<span class=\"c1\"># val_size: 0.1  # only if using part of training data for validation</span>\n",
       "<span class=\"nt\">per_device_eval_batch_size</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"nt\">eval_strategy</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">steps</span>\n",
       "<span class=\"nt\">eval_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">500</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} model}\n",
       "\\PY{n+nt}{model\\PYZus{}name\\PYZus{}or\\PYZus{}path}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{meta\\PYZhy{}llama/Meta\\PYZhy{}Llama\\PYZhy{}3\\PYZhy{}8B\\PYZhy{}Instruct}\n",
       "\\PY{n+nt}{trust\\PYZus{}remote\\PYZus{}code}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} method}\n",
       "\\PY{n+nt}{stage}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{sft}\n",
       "\\PY{n+nt}{do\\PYZus{}train}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{finetuning\\PYZus{}type}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{lora}\n",
       "\\PY{n+nt}{lora\\PYZus{}rank}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{8}\n",
       "\\PY{n+nt}{lora\\PYZus{}target}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{all}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} dataset}\n",
       "\\PY{n+nt}{dataset}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{viggo\\PYZhy{}train}\n",
       "\\PY{n+nt}{dataset\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} shared storage workers have access to}\n",
       "\\PY{n+nt}{template}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{llama3}\n",
       "\\PY{n+nt}{cutoff\\PYZus{}len}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{2048}\n",
       "\\PY{n+nt}{max\\PYZus{}samples}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1000}\n",
       "\\PY{n+nt}{overwrite\\PYZus{}cache}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{preprocessing\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{16}\n",
       "\\PY{n+nt}{dataloader\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{4}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} output}\n",
       "\\PY{n+nt}{output\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo/outputs}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} should be somewhere workers have access to (ex. s3, nfs)}\n",
       "\\PY{n+nt}{logging\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{10}\n",
       "\\PY{n+nt}{save\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{500}\n",
       "\\PY{n+nt}{plot\\PYZus{}loss}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{overwrite\\PYZus{}output\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{save\\PYZus{}only\\PYZus{}model}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{false}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} ray}\n",
       "\\PY{n+nt}{ray\\PYZus{}run\\PYZus{}name}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{llama3\\PYZus{}8b\\PYZus{}sft\\PYZus{}lora}\n",
       "\\PY{n+nt}{ray\\PYZus{}storage\\PYZus{}path}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo/saves}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} should be somewhere workers have access to (ex. s3, nfs)}\n",
       "\\PY{n+nt}{ray\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{4}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} number of GPUs to use}\n",
       "\\PY{n+nt}{resources\\PYZus{}per\\PYZus{}worker}\\PY{p}{:}\n",
       "\\PY{+w}{  }\\PY{n+nt}{GPU}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{n+nt}{placement\\PYZus{}strategy}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{PACK}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} train}\n",
       "\\PY{n+nt}{per\\PYZus{}device\\PYZus{}train\\PYZus{}batch\\PYZus{}size}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{n+nt}{gradient\\PYZus{}accumulation\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{8}\n",
       "\\PY{n+nt}{learning\\PYZus{}rate}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1.0e\\PYZhy{}4}\n",
       "\\PY{n+nt}{num\\PYZus{}train\\PYZus{}epochs}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{3.0}\n",
       "\\PY{n+nt}{lr\\PYZus{}scheduler\\PYZus{}type}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{cosine}\n",
       "\\PY{n+nt}{warmup\\PYZus{}ratio}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{0.1}\n",
       "\\PY{n+nt}{bf16}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{ddp\\PYZus{}timeout}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{180000000}\n",
       "\\PY{n+nt}{resume\\PYZus{}from\\PYZus{}checkpoint}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{null}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} eval}\n",
       "\\PY{n+nt}{eval\\PYZus{}dataset}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{viggo\\PYZhy{}val}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} uses same dataset\\PYZus{}dir as training data}\n",
       "\\PY{c+c1}{\\PYZsh{} val\\PYZus{}size: 0.1  \\PYZsh{} only if using part of training data for validation}\n",
       "\\PY{n+nt}{per\\PYZus{}device\\PYZus{}eval\\PYZus{}batch\\PYZus{}size}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{n+nt}{eval\\PYZus{}strategy}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{steps}\n",
       "\\PY{n+nt}{eval\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{500}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "### model\n",
       "model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct\n",
       "trust_remote_code: true\n",
       "\n",
       "### method\n",
       "stage: sft\n",
       "do_train: true\n",
       "finetuning_type: lora\n",
       "lora_rank: 8\n",
       "lora_target: all\n",
       "\n",
       "### dataset\n",
       "dataset: viggo-train\n",
       "dataset_dir: /mnt/cluster_storage/viggo  # shared storage workers have access to\n",
       "template: llama3\n",
       "cutoff_len: 2048\n",
       "max_samples: 1000\n",
       "overwrite_cache: true\n",
       "preprocessing_num_workers: 16\n",
       "dataloader_num_workers: 4\n",
       "\n",
       "### output\n",
       "output_dir: /mnt/cluster_storage/viggo/outputs  # should be somewhere workers have access to (ex. s3, nfs)\n",
       "logging_steps: 10\n",
       "save_steps: 500\n",
       "plot_loss: true\n",
       "overwrite_output_dir: true\n",
       "save_only_model: false\n",
       "\n",
       "### ray\n",
       "ray_run_name: llama3_8b_sft_lora\n",
       "ray_storage_path: /mnt/cluster_storage/viggo/saves  # should be somewhere workers have access to (ex. s3, nfs)\n",
       "ray_num_workers: 4  # number of GPUs to use\n",
       "resources_per_worker:\n",
       "  GPU: 1\n",
       "placement_strategy: PACK\n",
       "\n",
       "### train\n",
       "per_device_train_batch_size: 1\n",
       "gradient_accumulation_steps: 8\n",
       "learning_rate: 1.0e-4\n",
       "num_train_epochs: 3.0\n",
       "lr_scheduler_type: cosine\n",
       "warmup_ratio: 0.1\n",
       "bf16: true\n",
       "ddp_timeout: 180000000\n",
       "resume_from_checkpoint: null\n",
       "\n",
       "### eval\n",
       "eval_dataset: viggo-val  # uses same dataset_dir as training data\n",
       "# val_size: 0.1  # only if using part of training data for validation\n",
       "per_device_eval_batch_size: 1\n",
       "eval_strategy: steps\n",
       "eval_steps: 500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"llama3_lora_sft_ray.yaml\", language=\"yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 22:55:58 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 22:56:01,130\tINFO worker.py:1660 -- Connecting to existing Ray cluster at address: 10.0.45.206:6379...\n",
      "2025-04-09 22:56:01,139\tINFO worker.py:1843 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-zt5t77xa58pyp3uy28glg2g24d.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-04-09 22:56:01,144\tINFO packaging.py:367 -- Pushing file package 'gcs://_ray_pkg_ed6c6bcd217058548322bdd912a302ca171a421e.zip' (1.96MiB) to Ray cluster...\n",
      "2025-04-09 22:56:01,152\tINFO packaging.py:380 -- Successfully pushed file package 'gcs://_ray_pkg_ed6c6bcd217058548322bdd912a302ca171a421e.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "View detailed results here: /mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-04-09_22-54-49_487486_292703/artifacts/2025-04-09_22-56-02/llama3_8b_sft_lora/driver_artifacts`\n",
      "\u001b[36m(autoscaler +7s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[36m(autoscaler +7s)\u001b[0m [autoscaler] Downscaling node i-02b3c66b7ce7a86db (node IP: 10.0.111.111) due to node idle termination.\n",
      "\n",
      "Training started with configuration:\n",
      "╭───────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training config                                                           │\n",
      "├───────────────────────────────────────────────────────────────────────────┤\n",
      "│ train_loop_config/args/bf16                                          True │\n",
      "│ train_loop_config/args/cutoff_len                                    2048 │\n",
      "│ train_loop_config/args/dataloader_num_workers                           4 │\n",
      "│ train_loop_config/args/dataset                                viggo-train │\n",
      "│ train_loop_config/args/dataset_dir                   ...ter_storage/viggo │\n",
      "│ train_loop_config/args/ddp_timeout                              180000000 │\n",
      "│ train_loop_config/args/do_train                                      True │\n",
      "│ train_loop_config/args/eval_dataset                             viggo-val │\n",
      "│ train_loop_config/args/eval_steps                                     500 │\n",
      "│ train_loop_config/args/eval_strategy                                steps │\n",
      "│ train_loop_config/args/finetuning_type                               lora │\n",
      "│ train_loop_config/args/gradient_accumulation_steps                      8 │\n",
      "│ train_loop_config/args/learning_rate                               0.0001 │\n",
      "│ train_loop_config/args/logging_steps                                   10 │\n",
      "│ train_loop_config/args/lora_rank                                        8 │\n",
      "│ train_loop_config/args/lora_target                                    all │\n",
      "│ train_loop_config/args/lr_scheduler_type                           cosine │\n",
      "│ train_loop_config/args/max_samples                                   1000 │\n",
      "│ train_loop_config/args/model_name_or_path            ...ama-3-8B-Instruct │\n",
      "│ train_loop_config/args/num_train_epochs                               3.0 │\n",
      "│ train_loop_config/args/output_dir                    ...age/viggo/outputs │\n",
      "│ train_loop_config/args/overwrite_cache                               True │\n",
      "│ train_loop_config/args/overwrite_output_dir                          True │\n",
      "│ train_loop_config/args/per_device_eval_batch_size                       1 │\n",
      "│ train_loop_config/args/per_device_train_batch_size                      1 │\n",
      "│ train_loop_config/args/placement_strategy                            PACK │\n",
      "│ train_loop_config/args/plot_loss                                     True │\n",
      "│ train_loop_config/args/preprocessing_num_workers                       16 │\n",
      "│ train_loop_config/args/ray_num_workers                                  4 │\n",
      "│ train_loop_config/args/ray_run_name                    llama3_8b_sft_lora │\n",
      "│ train_loop_config/args/ray_storage_path              ...orage/viggo/saves │\n",
      "│ train_loop_config/args/resources_per_worker/GPU                         1 │\n",
      "│ train_loop_config/args/resume_from_checkpoint                             │\n",
      "│ train_loop_config/args/save_only_model                              False │\n",
      "│ train_loop_config/args/save_steps                                     500 │\n",
      "│ train_loop_config/args/stage                                          sft │\n",
      "│ train_loop_config/args/template                                    llama3 │\n",
      "│ train_loop_config/args/trust_remote_code                             True │\n",
      "│ train_loop_config/args/warmup_ratio                                   0.1 │\n",
      "│ train_loop_config/callbacks                          ... 0x7cbfc900a9c0>] │\n",
      "╰───────────────────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(TorchTrainer pid=50157, ip=10.0.111.111)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=50157, ip=10.0.111.111)\u001b[0m - (node_id=60601da1390eefd14bfef564c865ae4654f6cb7d86cf019cf75ef9d8, ip=10.0.111.111, pid=50269) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=50157, ip=10.0.111.111)\u001b[0m - (node_id=60601da1390eefd14bfef564c865ae4654f6cb7d86cf019cf75ef9d8, ip=10.0.111.111, pid=50270) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=50157, ip=10.0.111.111)\u001b[0m - (node_id=60601da1390eefd14bfef564c865ae4654f6cb7d86cf019cf75ef9d8, ip=10.0.111.111, pid=50268) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=50157, ip=10.0.111.111)\u001b[0m - (node_id=60601da1390eefd14bfef564c865ae4654f6cb7d86cf019cf75ef9d8, ip=10.0.111.111, pid=50271) world_rank=3, local_rank=3, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [WARNING|2025-04-09 22:56:21] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:21] llamafactory.hparams.parser:379 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:22,722 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:22,722 >> loading file tokenizer.model from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:22,722 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:22,722 >> loading file special_tokens_map.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:22,722 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:22,722 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2323] 2025-04-09 22:56:23,122 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|configuration_utils.py:693] 2025-04-09 22:56:23,551 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|configuration_utils.py:765] 2025-04-09 22:56:23,551 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"transformers_version\": \"4.51.0\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:23,639 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:23,639 >> loading file tokenizer.model from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:23,639 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:23,640 >> loading file special_tokens_map.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:23,640 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-09 22:56:23,640 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=50268, ip=10.0.111.111)\u001b[0m [rank2]:[W409 22:56:24.186055210 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:24] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:24] llamafactory.data.template:143 >> Add <|eot_id|>,<|eom_id|> to stop words.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [WARNING|2025-04-09 22:56:24] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:24] llamafactory.data.loader:143 >> Loading dataset /mnt/cluster_storage/viggo/train.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2323] 2025-04-09 22:56:24,008 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 10206 examples [00:00, 101262.18 examples/s]\n",
      "Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Converting format of dataset (num_proc=16):  81%|████████▏ | 814/1000 [00:00<00:00, 7773.45 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 5292.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:24] llamafactory.data.loader:143 >> Loading dataset /mnt/cluster_storage/viggo/val.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 714 examples [00:00, 101457.91 examples/s]\n",
      "Converting format of dataset (num_proc=16):   0%|          | 0/714 [00:00<?, ? examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 714/714 [00:00<00:00, 4072.12 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:13, 69.28 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  13%|█▎        | 126/1000 [00:01<00:06, 137.71 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  19%|█▉        | 189/1000 [00:01<00:04, 198.28 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  25%|██▌       | 252/1000 [00:01<00:03, 249.13 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  32%|███▏      | 315/1000 [00:01<00:02, 291.71 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  38%|███▊      | 378/1000 [00:01<00:01, 323.96 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  44%|████▍     | 441/1000 [00:01<00:01, 352.69 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  50%|█████     | 504/1000 [00:01<00:01, 371.91 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 566/1000 [00:02<00:01, 389.96 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 628/1000 [00:02<00:00, 395.66 examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [rank0]:[W409 22:56:25.918582420 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 690/1000 [00:02<00:00, 399.43 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 814/1000 [00:02<00:00, 462.47 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 876/1000 [00:02<00:00, 447.57 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 938/1000 [00:02<00:00, 445.58 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:03<00:00, 430.46 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:03<00:00, 316.96 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m training example:\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [128000, 128006, 882, 128007, 271, 22818, 264, 2218, 11914, 9429, 279, 16940, 7438, 13340, 315, 279, 1988, 11914, 439, 264, 3254, 734, 449, 8365, 323, 7180, 2819, 13, 1115, 734, 1288, 7664, 279, 2218, 925, 30357, 323, 279, 734, 2011, 387, 832, 315, 279, 2768, 2570, 41540, 518, 364, 2079, 518, 364, 47530, 10499, 37400, 518, 364, 14119, 518, 364, 12728, 17209, 518, 364, 96861, 518, 364, 2079, 2769, 36990, 518, 364, 67689, 518, 364, 2079, 17209, 7352, 578, 8365, 2011, 387, 832, 315, 279, 2768, 25, 2570, 609, 518, 364, 4683, 25596, 4257, 518, 364, 23859, 14987, 518, 364, 35501, 518, 364, 288, 10910, 518, 364, 22696, 518, 364, 65011, 518, 364, 3517, 623, 86191, 518, 364, 4752, 26190, 3517, 518, 364, 16111, 82, 518, 364, 10547, 4570, 1284, 14922, 518, 364, 4752, 78563, 25596, 518, 364, 4752, 23647, 25596, 518, 364, 68351, 4532, 5028, 39248, 4892, 374, 10213, 459, 17339, 16131, 11, 719, 814, 6004, 74337, 8105, 369, 279, 7553, 323, 779, 430, 41802, 279, 1847, 505, 17339, 311, 1695, 304, 856, 1684, 13, 128009, 128006, 78191, 128007, 271, 47530, 10499, 37400, 3232, 58, 22427, 32560, 8105, 1145, 16131, 58, 5028, 39248, 4892, 1145, 10959, 58, 19045, 1145, 706, 23647, 25596, 58, 9891, 2526, 128009]\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m inputs:\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform','request', 'give_opinion', 'confirm','verify_attribute','suggest','request_explanation','recommend','request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date','release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release','specifier']\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])<|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 47530, 10499, 37400, 3232, 58, 22427, 32560, 8105, 1145, 16131, 58, 5028, 39248, 4892, 1145, 10959, 58, 19045, 1145, 706, 23647, 25596, 58, 9891, 2526, 128009]\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m labels:\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])<|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=50271, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:21] llamafactory.hparams.parser:379 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset (num_proc=16):   0%|          | 0/714 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   6%|▋         | 45/714 [00:00<00:12, 51.84 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  13%|█▎        | 90/714 [00:01<00:07, 83.77 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  25%|██▌       | 180/714 [00:01<00:03, 175.11 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  32%|███▏      | 225/714 [00:01<00:02, 191.78 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  44%|████▍     | 315/714 [00:01<00:01, 226.91 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  50%|█████     | 360/714 [00:01<00:01, 245.48 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 405/714 [00:02<00:01, 262.57 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 450/714 [00:02<00:00, 272.33 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 494/714 [00:02<00:00, 278.12 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 538/714 [00:02<00:00, 294.54 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 582/714 [00:02<00:00, 325.06 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 626/714 [00:02<00:00, 316.31 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 670/714 [00:02<00:00, 311.83 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 714/714 [00:03<00:00, 314.12 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 714/714 [00:03<00:00, 224.77 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m eval example:\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [128000, 128006, 882, 128007, 271, 22818, 264, 2218, 11914, 9429, 279, 16940, 7438, 13340, 315, 279, 1988, 11914, 439, 264, 3254, 734, 449, 8365, 323, 7180, 2819, 13, 1115, 734, 1288, 7664, 279, 2218, 925, 30357, 323, 279, 734, 2011, 387, 832, 315, 279, 2768, 2570, 41540, 518, 364, 2079, 518, 364, 47530, 10499, 37400, 518, 364, 14119, 518, 364, 12728, 17209, 518, 364, 96861, 518, 364, 2079, 2769, 36990, 518, 364, 67689, 518, 364, 2079, 17209, 7352, 578, 8365, 2011, 387, 832, 315, 279, 2768, 25, 2570, 609, 518, 364, 4683, 25596, 4257, 518, 364, 23859, 14987, 518, 364, 35501, 518, 364, 288, 10910, 518, 364, 22696, 518, 364, 65011, 518, 364, 3517, 623, 86191, 518, 364, 4752, 26190, 3517, 518, 364, 16111, 82, 518, 364, 10547, 4570, 1284, 14922, 518, 364, 4752, 78563, 25596, 518, 364, 4752, 23647, 25596, 518, 364, 68351, 4532, 30128, 19085, 220, 18, 374, 264, 5128, 3958, 1847, 13, 578, 16131, 74404, 20763, 11871, 374, 9539, 264, 15860, 315, 912, 2442, 69261, 60884, 11, 323, 220, 679, 22, 574, 264, 17936, 1060, 369, 3953, 13971, 13, 128009, 128006, 78191, 128007, 271, 47530, 10499, 37400, 3232, 58, 30128, 19085, 220, 18, 1145, 4984, 14987, 58, 679, 22, 1145, 16131, 73183, 6417, 20763, 11871, 1145, 10959, 58, 5481, 269, 2526, 128009]\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m inputs:\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform','request', 'give_opinion', 'confirm','verify_attribute','suggest','request_explanation','recommend','request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date','release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release','specifier']\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m SpellForce 3 is a pretty bad game. The developer Grimlore Games is clearly a bunch of no-talent hacks, and 2017 was a terrible year for games anyway.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m give_opinion(name[SpellForce 3], release_year[2017], developer[Grimlore Games], rating[poor])<|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 47530, 10499, 37400, 3232, 58, 30128, 19085, 220, 18, 1145, 4984, 14987, 58, 679, 22, 1145, 16131, 73183, 6417, 20763, 11871, 1145, 10959, 58, 5481, 269, 2526, 128009]\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m labels:\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m give_opinion(name[SpellForce 3], release_year[2017], developer[Grimlore Games], rating[poor])<|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:34] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|configuration_utils.py:693] 2025-04-09 22:56:34,096 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|configuration_utils.py:765] 2025-04-09 22:56:34,097 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"transformers_version\": \"4.51.0\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|modeling_utils.py:1124] 2025-04-09 22:56:34,681 >> loading weights file model.safetensors from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/model.safetensors.index.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|modeling_utils.py:2167] 2025-04-09 22:56:52,935 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|configuration_utils.py:1142] 2025-04-09 22:56:52,937 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"use_cache\": false\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.00it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|modeling_utils.py:4926] 2025-04-09 22:56:56,277 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|modeling_utils.py:4934] 2025-04-09 22:56:56,277 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:56] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:56] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:56] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:56] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:56] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,gate_proj,up_proj,q_proj,v_proj,o_proj,down_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|configuration_utils.py:1097] 2025-04-09 22:56:56,384 >> loading configuration file generation_config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/generation_config.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|configuration_utils.py:1142] 2025-04-09 22:56:56,384 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"do_sample\": true,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"eos_token_id\": [\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m     128001,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m     128009\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"max_length\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"temperature\": 0.6,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"top_p\": 0.9\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|2025-04-09 22:56:56] llamafactory.model.loader:143 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:748] 2025-04-09 22:56:56,838 >> Using auto half precision backend\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [WARNING|trainer.py:783] 2025-04-09 22:56:56,839 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[36m(RayTrainWorker pid=50271, ip=10.0.111.111)\u001b[0m No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:2414] 2025-04-09 22:57:03,221 >> ***** Running training *****\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:2415] 2025-04-09 22:57:03,221 >>   Num examples = 1,000\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:2416] 2025-04-09 22:57:03,221 >>   Num Epochs = 3\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:2417] 2025-04-09 22:57:03,221 >>   Instantaneous batch size per device = 1\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:2420] 2025-04-09 22:57:03,221 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:2421] 2025-04-09 22:57:03,221 >>   Gradient Accumulation steps = 8\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:2422] 2025-04-09 22:57:03,221 >>   Total optimization steps = 93\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:2423] 2025-04-09 22:57:03,224 >>   Number of trainable parameters = 20,971,520\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.02s/it]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=50270, ip=10.0.111.111)\u001b[0m No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "  0%|          | 0/93 [00:00<?, ?it/s]0.111.111)\u001b[0m \n",
      "  1%|          | 1/93 [00:03<05:57,  3.89s/it]1)\u001b[0m \n",
      "  2%|▏         | 2/93 [00:06<04:49,  3.19s/it]1)\u001b[0m \n",
      "  3%|▎         | 3/93 [00:09<04:29,  2.99s/it]1)\u001b[0m \n",
      "  4%|▍         | 4/93 [00:12<04:19,  2.92s/it]1)\u001b[0m \n",
      "  5%|▌         | 5/93 [00:14<04:07,  2.81s/it]1)\u001b[0m \n",
      "  6%|▋         | 6/93 [00:17<04:06,  2.84s/it]1)\u001b[0m \n",
      "  8%|▊         | 7/93 [00:20<03:57,  2.76s/it]1)\u001b[0m \n",
      "  9%|▊         | 8/93 [00:23<03:58,  2.81s/it]1)\u001b[0m \n",
      " 10%|▉         | 9/93 [00:26<03:56,  2.81s/it]1)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m {'loss': 1.7355, 'grad_norm': 1.8911470174789429, 'learning_rate': 9e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10/93 [00:28<03:55,  2.84s/it])\u001b[0m \n",
      " 12%|█▏        | 11/93 [00:31<03:49,  2.80s/it])\u001b[0m \n",
      " 13%|█▎        | 12/93 [00:34<03:48,  2.82s/it])\u001b[0m \n",
      " 14%|█▍        | 13/93 [00:37<03:47,  2.84s/it])\u001b[0m \n",
      " 15%|█▌        | 14/93 [00:40<03:44,  2.84s/it])\u001b[0m \n",
      " 16%|█▌        | 15/93 [00:42<03:37,  2.79s/it])\u001b[0m \n",
      " 17%|█▋        | 16/93 [00:45<03:37,  2.83s/it])\u001b[0m \n",
      " 18%|█▊        | 17/93 [00:48<03:31,  2.79s/it])\u001b[0m \n",
      " 19%|█▉        | 18/93 [00:51<03:25,  2.75s/it])\u001b[0m \n",
      " 20%|██        | 19/93 [00:53<03:24,  2.76s/it])\u001b[0m \n",
      " 22%|██▏       | 20/93 [00:57<03:28,  2.85s/it])\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m {'loss': 0.3972, 'grad_norm': 1.3567222356796265, 'learning_rate': 9.712680772628364e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 20/93 [00:57<03:28,  2.85s/it])\u001b[0m \n",
      " 23%|██▎       | 21/93 [00:59<03:27,  2.88s/it])\u001b[0m \n",
      " 24%|██▎       | 22/93 [01:02<03:24,  2.88s/it])\u001b[0m \n",
      " 25%|██▍       | 23/93 [01:05<03:19,  2.85s/it])\u001b[0m \n",
      " 26%|██▌       | 24/93 [01:08<03:17,  2.86s/it])\u001b[0m \n",
      " 27%|██▋       | 25/93 [01:11<03:14,  2.86s/it])\u001b[0m \n",
      " 28%|██▊       | 26/93 [01:14<03:11,  2.86s/it])\u001b[0m \n",
      " 29%|██▉       | 27/93 [01:16<03:04,  2.80s/it])\u001b[0m \n",
      " 30%|███       | 28/93 [01:19<02:58,  2.75s/it])\u001b[0m \n",
      " 31%|███       | 29/93 [01:22<02:54,  2.72s/it])\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m {'loss': 0.1269, 'grad_norm': 0.7699804306030273, 'learning_rate': 8.761797511897906e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 30/93 [01:25<02:54,  2.76s/it])\u001b[0m \n",
      " 33%|███▎      | 31/93 [01:27<02:50,  2.74s/it])\u001b[0m \n",
      " 34%|███▍      | 32/93 [01:28<02:14,  2.21s/it])\u001b[0m \n",
      " 35%|███▌      | 33/93 [01:31<02:28,  2.47s/it])\u001b[0m \n",
      " 37%|███▋      | 34/93 [01:34<02:29,  2.53s/it])\u001b[0m \n",
      " 38%|███▊      | 35/93 [01:37<02:32,  2.63s/it])\u001b[0m \n",
      " 39%|███▊      | 36/93 [01:40<02:33,  2.69s/it])\u001b[0m \n",
      " 40%|███▉      | 37/93 [01:42<02:30,  2.68s/it])\u001b[0m \n",
      " 41%|████      | 38/93 [01:45<02:27,  2.68s/it])\u001b[0m \n",
      " 42%|████▏     | 39/93 [01:48<02:26,  2.71s/it])\u001b[0m \n",
      " 43%|████▎     | 40/93 [01:51<02:25,  2.74s/it])\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m {'loss': 0.0588, 'grad_norm': 0.4717925190925598, 'learning_rate': 7.278379692281208e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 40/93 [01:51<02:25,  2.74s/it])\u001b[0m \n",
      " 44%|████▍     | 41/93 [01:53<02:23,  2.76s/it])\u001b[0m \n",
      " 45%|████▌     | 42/93 [01:56<02:22,  2.80s/it])\u001b[0m \n",
      " 46%|████▌     | 43/93 [01:59<02:17,  2.75s/it])\u001b[0m \n",
      " 47%|████▋     | 44/93 [02:02<02:13,  2.72s/it])\u001b[0m \n",
      " 48%|████▊     | 45/93 [02:05<02:15,  2.82s/it])\u001b[0m \n",
      " 49%|████▉     | 46/93 [02:07<02:10,  2.78s/it])\u001b[0m \n",
      " 51%|█████     | 47/93 [02:10<02:08,  2.79s/it])\u001b[0m \n",
      " 52%|█████▏    | 48/93 [02:13<02:05,  2.80s/it])\u001b[0m \n",
      " 53%|█████▎    | 49/93 [02:16<02:03,  2.80s/it])\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m {'loss': 0.0347, 'grad_norm': 0.7581666111946106, 'learning_rate': 5.472425659440157e-05, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 50/93 [02:18<01:58,  2.76s/it])\u001b[0m \n",
      " 55%|█████▍    | 51/93 [02:21<01:54,  2.72s/it])\u001b[0m \n",
      " 56%|█████▌    | 52/93 [02:24<01:53,  2.76s/it])\u001b[0m \n",
      " 57%|█████▋    | 53/93 [02:27<01:51,  2.78s/it])\u001b[0m \n",
      " 58%|█████▊    | 54/93 [02:30<01:49,  2.82s/it])\u001b[0m \n",
      " 59%|█████▉    | 55/93 [02:33<01:49,  2.89s/it])\u001b[0m \n",
      " 60%|██████    | 56/93 [02:35<01:44,  2.83s/it])\u001b[0m \n",
      " 61%|██████▏   | 57/93 [02:38<01:41,  2.82s/it])\u001b[0m \n",
      " 62%|██████▏   | 58/93 [02:41<01:39,  2.84s/it])\u001b[0m \n",
      " 63%|██████▎   | 59/93 [02:44<01:36,  2.83s/it])\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m {'loss': 0.0244, 'grad_norm': 0.6526265144348145, 'learning_rate': 3.599593228875465e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 60/93 [02:46<01:31,  2.78s/it])\u001b[0m \n",
      " 66%|██████▌   | 61/93 [02:49<01:30,  2.82s/it])\u001b[0m \n",
      " 67%|██████▋   | 62/93 [02:52<01:26,  2.80s/it])\u001b[0m \n",
      " 68%|██████▊   | 63/93 [02:55<01:24,  2.80s/it])\u001b[0m \n",
      " 69%|██████▉   | 64/93 [02:56<01:05,  2.26s/it])\u001b[0m \n",
      " 70%|██████▉   | 65/93 [02:59<01:08,  2.43s/it])\u001b[0m \n",
      " 71%|███████   | 66/93 [03:02<01:09,  2.56s/it])\u001b[0m \n",
      " 72%|███████▏  | 67/93 [03:05<01:09,  2.66s/it])\u001b[0m \n",
      " 73%|███████▎  | 68/93 [03:07<01:07,  2.71s/it])\u001b[0m \n",
      " 74%|███████▍  | 69/93 [03:10<01:06,  2.76s/it])\u001b[0m \n",
      " 75%|███████▌  | 70/93 [03:13<01:04,  2.80s/it])\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m {'loss': 0.0128, 'grad_norm': 0.7971675395965576, 'learning_rate': 1.9250077799102322e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 70/93 [03:13<01:04,  2.80s/it])\u001b[0m \n",
      " 76%|███████▋  | 71/93 [03:16<01:01,  2.81s/it])\u001b[0m \n",
      " 77%|███████▋  | 72/93 [03:19<00:59,  2.82s/it])\u001b[0m \n",
      " 78%|███████▊  | 73/93 [03:22<00:56,  2.83s/it])\u001b[0m \n",
      " 80%|███████▉  | 74/93 [03:25<00:54,  2.85s/it])\u001b[0m \n",
      " 81%|████████  | 75/93 [03:27<00:51,  2.84s/it])\u001b[0m \n",
      " 82%|████████▏ | 76/93 [03:30<00:47,  2.79s/it])\u001b[0m \n",
      " 83%|████████▎ | 77/93 [03:33<00:45,  2.83s/it])\u001b[0m \n",
      " 84%|████████▍ | 78/93 [03:36<00:41,  2.77s/it])\u001b[0m \n",
      " 85%|████████▍ | 79/93 [03:38<00:38,  2.74s/it])\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m {'loss': 0.0088, 'grad_norm': 0.112918421626091, 'learning_rate': 6.857300848378856e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 80/93 [03:41<00:35,  2.76s/it])\u001b[0m \n",
      " 87%|████████▋ | 81/93 [03:44<00:32,  2.74s/it])\u001b[0m \n",
      " 88%|████████▊ | 82/93 [03:46<00:29,  2.73s/it])\u001b[0m \n",
      " 89%|████████▉ | 83/93 [03:49<00:27,  2.70s/it])\u001b[0m \n",
      " 90%|█████████ | 84/93 [03:52<00:25,  2.82s/it])\u001b[0m \n",
      " 91%|█████████▏| 85/93 [03:55<00:22,  2.78s/it])\u001b[0m \n",
      " 92%|█████████▏| 86/93 [03:58<00:19,  2.74s/it])\u001b[0m \n",
      " 94%|█████████▎| 87/93 [04:00<00:16,  2.73s/it])\u001b[0m \n",
      " 95%|█████████▍| 88/93 [04:03<00:13,  2.70s/it])\u001b[0m \n",
      " 96%|█████████▌| 89/93 [04:06<00:11,  2.76s/it])\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m {'loss': 0.0131, 'grad_norm': 0.3144412934780121, 'learning_rate': 5.719707204055735e-07, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 90/93 [04:09<00:08,  2.86s/it])\u001b[0m \n",
      " 98%|█████████▊| 91/93 [04:12<00:05,  2.92s/it])\u001b[0m \n",
      " 99%|█████████▉| 92/93 [04:15<00:02,  2.83s/it])\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50268, ip=10.0.111.111)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_7c0d8_00000_0_2025-04-09_22-56-02/checkpoint_000000)\n",
      "100%|██████████| 93/93 [04:17<00:00,  2.83s/it][INFO|trainer.py:3984] 2025-04-09 23:01:21,680 >> Saving model checkpoint to /mnt/cluster_storage/viggo/outputs/checkpoint-93\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|configuration_utils.py:693] 2025-04-09 23:01:21,979 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|configuration_utils.py:765] 2025-04-09 23:01:21,979 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"transformers_version\": \"4.51.0\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2510] 2025-04-09 23:01:22,536 >> tokenizer config file saved in /mnt/cluster_storage/viggo/outputs/checkpoint-93/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2519] 2025-04-09 23:01:22,544 >> Special tokens file saved in /mnt/cluster_storage/viggo/outputs/checkpoint-93/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 1 at 2025-04-09 23:01:26. Total running time: 5min 23s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000000 │\n",
      "│ time_this_iter_s              312.12164 │\n",
      "│ time_total_s                  312.12164 │\n",
      "│ training_iteration                    1 │\n",
      "│ epoch                             2.832 │\n",
      "│ grad_norm                       0.31444 │\n",
      "│ learning_rate                        0. │\n",
      "│ loss                             0.0131 │\n",
      "│ step                                 90 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 1 at: (local)/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_7c0d8_00000_0_2025-04-09_22-56-02/checkpoint_000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:2681] 2025-04-09 23:01:26,580 >> \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m {'train_runtime': 263.3559, 'train_samples_per_second': 11.391, 'train_steps_per_second': 0.353, 'train_loss': 0.2595549144691998, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [04:22<00:00,  2.83s/it])\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:3984] 2025-04-09 23:01:26,692 >> Saving model checkpoint to /mnt/cluster_storage/viggo/outputs\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|configuration_utils.py:693] 2025-04-09 23:01:26,922 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|configuration_utils.py:765] 2025-04-09 23:01:26,922 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"transformers_version\": \"4.51.0\",\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_7c0d8_00000_0_2025-04-09_22-56-02/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2510] 2025-04-09 23:01:27,532 >> tokenizer config file saved in /mnt/cluster_storage/viggo/outputs/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|tokenization_utils_base.py:2519] 2025-04-09 23:01:27,540 >> Special tokens file saved in /mnt/cluster_storage/viggo/outputs/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m ***** train metrics *****\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   epoch                    =      2.928\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   total_flos               = 26130810GF\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   train_loss               =     0.2596\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   train_runtime            = 0:04:23.35\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   train_samples_per_second =     11.391\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   train_steps_per_second   =      0.353\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m Figure saved at: /mnt/cluster_storage/viggo/outputs/training_loss.png\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [WARNING|2025-04-09 23:01:28] llamafactory.extras.ploting:148 >> No metric eval_viggo-val_loss to plot.\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [WARNING|2025-04-09 23:01:28] llamafactory.extras.ploting:148 >> No metric eval_viggo-val_accuracy to plot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:4307] 2025-04-09 23:01:28,009 >> \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m ***** Running Evaluation *****\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:4309] 2025-04-09 23:01:28,009 >>   Num examples = 714\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|trainer.py:4312] 2025-04-09 23:01:28,009 >>   Batch size = 1\n",
      "  0%|          | 0/179 [00:00<?, ?it/s].111.111)\u001b[0m \n",
      "  1%|          | 2/179 [00:00<00:09, 18.24it/s])\u001b[0m \n",
      "  2%|▏         | 4/179 [00:00<00:14, 11.88it/s])\u001b[0m \n",
      "  3%|▎         | 6/179 [00:00<00:16, 10.69it/s])\u001b[0m \n",
      "  4%|▍         | 8/179 [00:00<00:16, 10.23it/s])\u001b[0m \n",
      "  6%|▌         | 10/179 [00:00<00:16,  9.97it/s]\u001b[0m \n",
      "  7%|▋         | 12/179 [00:01<00:16,  9.83it/s]\u001b[0m \n",
      "  8%|▊         | 14/179 [00:01<00:16,  9.74it/s]\u001b[0m \n",
      "  8%|▊         | 15/179 [00:01<00:16,  9.72it/s]\u001b[0m \n",
      "  9%|▉         | 16/179 [00:01<00:16,  9.70it/s]\u001b[0m \n",
      "  9%|▉         | 17/179 [00:01<00:16,  9.70it/s]\u001b[0m \n",
      " 10%|█         | 18/179 [00:01<00:16,  9.64it/s]\u001b[0m \n",
      " 11%|█         | 19/179 [00:01<00:16,  9.63it/s]\u001b[0m \n",
      " 11%|█         | 20/179 [00:01<00:16,  9.59it/s]\u001b[0m \n",
      " 12%|█▏        | 21/179 [00:02<00:16,  9.51it/s]\u001b[0m \n",
      " 13%|█▎        | 23/179 [00:02<00:15,  9.83it/s]\u001b[0m \n",
      " 13%|█▎        | 24/179 [00:02<00:18,  8.53it/s]\u001b[0m \n",
      " 14%|█▍        | 25/179 [00:02<00:17,  8.74it/s]\u001b[0m \n",
      " 15%|█▍        | 26/179 [00:02<00:17,  8.95it/s]\u001b[0m \n",
      " 15%|█▌        | 27/179 [00:02<00:19,  7.88it/s]\u001b[0m \n",
      " 16%|█▌        | 28/179 [00:03<00:20,  7.20it/s]\u001b[0m \n",
      " 16%|█▌        | 29/179 [00:03<00:19,  7.75it/s]\u001b[0m \n",
      " 17%|█▋        | 31/179 [00:03<00:19,  7.76it/s]\u001b[0m \n",
      " 18%|█▊        | 33/179 [00:03<00:16,  8.60it/s]\u001b[0m \n",
      " 19%|█▉        | 34/179 [00:03<00:16,  8.80it/s]\u001b[0m \n",
      " 20%|█▉        | 35/179 [00:03<00:15,  9.02it/s]\u001b[0m \n",
      " 20%|██        | 36/179 [00:03<00:15,  9.16it/s]\u001b[0m \n",
      " 21%|██        | 37/179 [00:03<00:15,  9.16it/s]\u001b[0m \n",
      " 21%|██        | 38/179 [00:04<00:15,  9.27it/s]\u001b[0m \n",
      " 22%|██▏       | 39/179 [00:04<00:14,  9.36it/s]\u001b[0m \n",
      " 22%|██▏       | 40/179 [00:04<00:14,  9.44it/s]\u001b[0m \n",
      " 23%|██▎       | 41/179 [00:04<00:14,  9.45it/s]\u001b[0m \n",
      " 23%|██▎       | 42/179 [00:04<00:14,  9.51it/s]\u001b[0m \n",
      " 24%|██▍       | 43/179 [00:04<00:14,  9.49it/s]\u001b[0m \n",
      " 25%|██▍       | 44/179 [00:04<00:14,  9.55it/s]\u001b[0m \n",
      " 25%|██▌       | 45/179 [00:04<00:13,  9.58it/s]\u001b[0m \n",
      " 26%|██▌       | 46/179 [00:04<00:13,  9.57it/s]\u001b[0m \n",
      " 26%|██▋       | 47/179 [00:05<00:13,  9.54it/s]\u001b[0m \n",
      " 27%|██▋       | 48/179 [00:05<00:13,  9.49it/s]\u001b[0m \n",
      " 27%|██▋       | 49/179 [00:05<00:13,  9.50it/s]\u001b[0m \n",
      " 28%|██▊       | 51/179 [00:05<00:13,  9.81it/s]\u001b[0m \n",
      " 29%|██▉       | 52/179 [00:05<00:12,  9.78it/s]\u001b[0m \n",
      " 30%|██▉       | 53/179 [00:05<00:12,  9.72it/s]\u001b[0m \n",
      " 30%|███       | 54/179 [00:05<00:12,  9.68it/s]\u001b[0m \n",
      " 31%|███       | 55/179 [00:05<00:13,  9.53it/s]\u001b[0m \n",
      " 31%|███▏      | 56/179 [00:05<00:12,  9.55it/s]\u001b[0m \n",
      " 32%|███▏      | 57/179 [00:06<00:12,  9.53it/s]\u001b[0m \n",
      " 32%|███▏      | 58/179 [00:06<00:12,  9.52it/s]\u001b[0m \n",
      " 33%|███▎      | 59/179 [00:06<00:12,  9.54it/s]\u001b[0m \n",
      " 34%|███▎      | 60/179 [00:06<00:12,  9.58it/s]\u001b[0m \n",
      " 34%|███▍      | 61/179 [00:06<00:12,  9.57it/s]\u001b[0m \n",
      " 35%|███▍      | 62/179 [00:06<00:12,  9.59it/s]\u001b[0m \n",
      " 35%|███▌      | 63/179 [00:06<00:12,  9.58it/s]\u001b[0m \n",
      " 36%|███▌      | 64/179 [00:06<00:11,  9.61it/s]\u001b[0m \n",
      " 36%|███▋      | 65/179 [00:06<00:11,  9.57it/s]\u001b[0m \n",
      " 37%|███▋      | 66/179 [00:07<00:11,  9.49it/s]\u001b[0m \n",
      " 37%|███▋      | 67/179 [00:07<00:13,  8.10it/s]\u001b[0m \n",
      " 38%|███▊      | 68/179 [00:07<00:15,  7.33it/s]\u001b[0m \n",
      " 39%|███▊      | 69/179 [00:07<00:13,  7.86it/s]\u001b[0m \n",
      " 39%|███▉      | 70/179 [00:07<00:13,  8.29it/s]\u001b[0m \n",
      " 40%|███▉      | 71/179 [00:07<00:12,  8.66it/s]\u001b[0m \n",
      " 40%|████      | 72/179 [00:07<00:11,  8.92it/s]\u001b[0m \n",
      " 41%|████      | 73/179 [00:07<00:11,  9.11it/s]\u001b[0m \n",
      " 41%|████▏     | 74/179 [00:07<00:11,  9.20it/s]\u001b[0m \n",
      " 42%|████▏     | 75/179 [00:08<00:11,  9.24it/s]\u001b[0m \n",
      " 42%|████▏     | 76/179 [00:08<00:10,  9.38it/s]\u001b[0m \n",
      " 44%|████▎     | 78/179 [00:08<00:10,  9.73it/s]\u001b[0m \n",
      " 44%|████▍     | 79/179 [00:08<00:10,  9.66it/s]\u001b[0m \n",
      " 45%|████▍     | 80/179 [00:08<00:10,  9.64it/s]\u001b[0m \n",
      " 45%|████▌     | 81/179 [00:08<00:10,  9.61it/s]\u001b[0m \n",
      " 46%|████▌     | 82/179 [00:08<00:10,  9.62it/s]\u001b[0m \n",
      " 46%|████▋     | 83/179 [00:08<00:10,  9.60it/s]\u001b[0m \n",
      " 47%|████▋     | 84/179 [00:09<00:09,  9.60it/s]\u001b[0m \n",
      " 47%|████▋     | 85/179 [00:09<00:09,  9.47it/s]\u001b[0m \n",
      " 48%|████▊     | 86/179 [00:09<00:09,  9.37it/s]\u001b[0m \n",
      " 49%|████▉     | 88/179 [00:09<00:09,  9.65it/s]\u001b[0m \n",
      " 50%|████▉     | 89/179 [00:09<00:09,  9.62it/s]\u001b[0m \n",
      " 50%|█████     | 90/179 [00:09<00:09,  9.61it/s]\u001b[0m \n",
      " 51%|█████     | 91/179 [00:09<00:09,  9.58it/s]\u001b[0m \n",
      " 51%|█████▏    | 92/179 [00:09<00:09,  9.60it/s]\u001b[0m \n",
      " 52%|█████▏    | 93/179 [00:09<00:08,  9.59it/s]\u001b[0m \n",
      " 53%|█████▎    | 94/179 [00:10<00:08,  9.62it/s]\u001b[0m \n",
      " 53%|█████▎    | 95/179 [00:10<00:08,  9.59it/s]\u001b[0m \n",
      " 54%|█████▎    | 96/179 [00:10<00:08,  9.59it/s]\u001b[0m \n",
      " 54%|█████▍    | 97/179 [00:10<00:08,  9.63it/s]\u001b[0m \n",
      " 55%|█████▍    | 98/179 [00:10<00:08,  9.62it/s]\u001b[0m \n",
      " 55%|█████▌    | 99/179 [00:10<00:08,  9.63it/s]\u001b[0m \n",
      " 56%|█████▌    | 100/179 [00:10<00:08,  9.61it/s][0m \n",
      " 56%|█████▋    | 101/179 [00:10<00:08,  9.61it/s][0m \n",
      " 58%|█████▊    | 103/179 [00:10<00:07,  9.84it/s][0m \n",
      " 58%|█████▊    | 104/179 [00:11<00:07,  9.73it/s][0m \n",
      " 59%|█████▊    | 105/179 [00:11<00:07,  9.69it/s][0m \n",
      " 59%|█████▉    | 106/179 [00:11<00:07,  9.67it/s][0m \n",
      " 60%|█████▉    | 107/179 [00:11<00:07,  9.65it/s][0m \n",
      " 60%|██████    | 108/179 [00:11<00:07,  9.63it/s][0m \n",
      " 61%|██████    | 109/179 [00:11<00:07,  9.63it/s][0m \n",
      " 62%|██████▏   | 111/179 [00:11<00:06,  9.86it/s][0m \n",
      " 63%|██████▎   | 112/179 [00:11<00:06,  9.82it/s][0m \n",
      " 63%|██████▎   | 113/179 [00:12<00:06,  9.74it/s][0m \n",
      " 64%|██████▎   | 114/179 [00:12<00:06,  9.68it/s][0m \n",
      " 64%|██████▍   | 115/179 [00:12<00:06,  9.60it/s][0m \n",
      " 65%|██████▍   | 116/179 [00:12<00:06,  9.62it/s][0m \n",
      " 65%|██████▌   | 117/179 [00:12<00:06,  9.64it/s][0m \n",
      " 66%|██████▌   | 118/179 [00:12<00:06,  9.63it/s][0m \n",
      " 66%|██████▋   | 119/179 [00:12<00:06,  9.66it/s][0m \n",
      " 67%|██████▋   | 120/179 [00:12<00:06,  9.67it/s][0m \n",
      " 68%|██████▊   | 121/179 [00:12<00:07,  8.21it/s][0m \n",
      " 68%|██████▊   | 122/179 [00:13<00:06,  8.54it/s][0m \n",
      " 69%|██████▊   | 123/179 [00:13<00:06,  8.83it/s][0m \n",
      " 69%|██████▉   | 124/179 [00:13<00:06,  9.01it/s][0m \n",
      " 70%|██████▉   | 125/179 [00:13<00:05,  9.12it/s][0m \n",
      " 70%|███████   | 126/179 [00:13<00:05,  9.16it/s][0m \n",
      " 71%|███████   | 127/179 [00:13<00:05,  9.29it/s][0m \n",
      " 72%|███████▏  | 128/179 [00:13<00:06,  8.00it/s][0m \n",
      " 72%|███████▏  | 129/179 [00:13<00:06,  8.31it/s][0m \n",
      " 73%|███████▎  | 130/179 [00:13<00:05,  8.65it/s][0m \n",
      " 73%|███████▎  | 131/179 [00:14<00:05,  8.91it/s][0m \n",
      " 74%|███████▎  | 132/179 [00:14<00:05,  9.10it/s][0m \n",
      " 74%|███████▍  | 133/179 [00:14<00:04,  9.22it/s][0m \n",
      " 75%|███████▍  | 134/179 [00:14<00:04,  9.31it/s][0m \n",
      " 75%|███████▌  | 135/179 [00:14<00:04,  9.37it/s][0m \n",
      " 76%|███████▌  | 136/179 [00:14<00:05,  8.04it/s][0m \n",
      " 77%|███████▋  | 137/179 [00:14<00:04,  8.49it/s][0m \n",
      " 77%|███████▋  | 138/179 [00:14<00:04,  8.82it/s][0m \n",
      " 78%|███████▊  | 139/179 [00:14<00:04,  9.00it/s][0m \n",
      " 78%|███████▊  | 140/179 [00:15<00:04,  9.16it/s][0m \n",
      " 79%|███████▉  | 141/179 [00:15<00:04,  9.26it/s][0m \n",
      " 79%|███████▉  | 142/179 [00:15<00:03,  9.33it/s][0m \n",
      " 80%|███████▉  | 143/179 [00:15<00:03,  9.44it/s][0m \n",
      " 80%|████████  | 144/179 [00:15<00:03,  9.44it/s][0m \n",
      " 81%|████████  | 145/179 [00:15<00:03,  9.39it/s][0m \n",
      " 82%|████████▏ | 146/179 [00:15<00:03,  9.34it/s][0m \n",
      " 82%|████████▏ | 147/179 [00:15<00:03,  9.38it/s][0m \n",
      " 83%|████████▎ | 148/179 [00:15<00:03,  8.03it/s][0m \n",
      " 83%|████████▎ | 149/179 [00:16<00:03,  8.39it/s][0m \n",
      " 84%|████████▍ | 150/179 [00:16<00:03,  8.70it/s][0m \n",
      " 84%|████████▍ | 151/179 [00:16<00:03,  8.95it/s][0m \n",
      " 85%|████████▍ | 152/179 [00:16<00:02,  9.16it/s][0m \n",
      " 85%|████████▌ | 153/179 [00:16<00:02,  9.29it/s][0m \n",
      " 86%|████████▌ | 154/179 [00:16<00:02,  9.38it/s][0m \n",
      " 87%|████████▋ | 155/179 [00:16<00:02,  9.34it/s][0m \n",
      " 87%|████████▋ | 156/179 [00:16<00:02,  9.27it/s][0m \n",
      " 88%|████████▊ | 157/179 [00:16<00:02,  9.38it/s][0m \n",
      " 88%|████████▊ | 158/179 [00:16<00:02,  9.42it/s][0m \n",
      " 89%|████████▉ | 159/179 [00:17<00:02,  9.47it/s][0m \n",
      " 89%|████████▉ | 160/179 [00:17<00:02,  9.43it/s][0m \n",
      " 90%|████████▉ | 161/179 [00:17<00:01,  9.36it/s][0m \n",
      " 91%|█████████ | 162/179 [00:17<00:01,  9.46it/s][0m \n",
      " 91%|█████████ | 163/179 [00:17<00:01,  9.49it/s][0m \n",
      " 92%|█████████▏| 164/179 [00:17<00:01,  9.50it/s][0m \n",
      " 92%|█████████▏| 165/179 [00:17<00:01,  9.40it/s][0m \n",
      " 93%|█████████▎| 166/179 [00:17<00:01,  9.41it/s][0m \n",
      " 93%|█████████▎| 167/179 [00:17<00:01,  9.38it/s][0m \n",
      " 94%|█████████▍| 168/179 [00:18<00:01,  9.31it/s][0m \n",
      " 94%|█████████▍| 169/179 [00:18<00:01,  9.30it/s][0m \n",
      " 95%|█████████▍| 170/179 [00:18<00:00,  9.39it/s][0m \n",
      " 96%|█████████▌| 171/179 [00:18<00:00,  9.43it/s][0m \n",
      " 96%|█████████▌| 172/179 [00:18<00:00,  8.10it/s][0m \n",
      " 97%|█████████▋| 173/179 [00:18<00:00,  7.36it/s][0m \n",
      " 97%|█████████▋| 174/179 [00:18<00:00,  6.91it/s][0m \n",
      " 98%|█████████▊| 175/179 [00:18<00:00,  7.50it/s][0m \n",
      " 98%|█████████▊| 176/179 [00:19<00:00,  8.06it/s][0m \n",
      " 99%|█████████▉| 177/179 [00:19<00:00,  8.48it/s][0m \n",
      " 99%|█████████▉| 178/179 [00:19<00:00,  8.84it/s][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m ***** eval metrics *****\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   epoch                             =      2.928\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   eval_viggo-val_loss               =     0.1033\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   eval_viggo-val_runtime            = 0:00:19.75\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   eval_viggo-val_samples_per_second =     36.141\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m   eval_viggo-val_steps_per_second   =      9.061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [00:19<00:00,  9.17it/s][0m \n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m [INFO|modelcard.py:450] 2025-04-09 23:01:47,829 >> Dropping the following result as it does not have all the necessary fields:\n",
      "\u001b[36m(RayTrainWorker pid=50269, ip=10.0.111.111)\u001b[0m {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed after 1 iterations at 2025-04-09 23:01:49. Total running time: 5min 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 23:01:49,780\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora' in 0.0212s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run multinode distributed fine-tuning workload\n",
    "USE_RAY=1 llamafactory-cli train llama3_lora_sft_ray.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b>Ray Train</b> \n",
    "\n",
    "Using [Ray Train](https://docs.ray.io/en/latest/train/train.html) here has several advantages:\n",
    "- automatically handles **multi-node, multi-GPU** setup with no manual SSH setup or hostfile configs. \n",
    "- define **per-worker franctional resource requirements** (e.g., 2 CPUs and 0.5 GPU per worker)\n",
    "- run on **heterogeneous machines** and scale flexibly (e.g., CPU for preprocessing and GPU for training) \n",
    "- built-in **fault tolerance** via retry of failed workers (and continue from last checkpoint).\n",
    "- supports Data Parallel, Model Parallel, Parameter Server, and even custom strategies.\n",
    "- [Ray Compiled graphs](https://docs.ray.io/en/latest/ray-core/compiled-graph/ray-compiled-graph.html) allow us to even define different parallelism for jointly optimizing multipe models (Megatron, Deepspeed, etc. only allow for one global setting).\n",
    "\n",
    "[RayTurbo Train](https://docs.anyscale.com/rayturbo/rayturbo-train) offers even more improvement to the price-performance ratio, performance monitoring and more:\n",
    "- **elastic training** to scale to a dynamic number of workers, continue training on fewer resources (even on spot instances).\n",
    "- **purpose-built dashboard** designed to streamline the debugging of Ray Train workloads\n",
    "    - Monitoring: View the status of training runs and train workers.\n",
    "    - Metrics: See insights on training throughput, training system operation time.\n",
    "    - Profiling: Investigate bottlenecks, hangs, or errors from individual training worker processes.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/train_dashboard.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🔎 Monitoring and Debugging with Ray</b> \n",
    "\n",
    "\n",
    "OSS Ray offers an extensive [observability suite](https://docs.ray.io/en/latest/ray-observability/index.html) that offers logs and an observability dashboard that we can use to monitor and debug. The dashboard includes a lot of different components such as:\n",
    "\n",
    "-  memory, utilization, etc. of the tasks running in our [cluster](https://docs.ray.io/en/latest/ray-observability/getting-started.html#dash-node-view)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/cluster_util.png\" width=700>\n",
    "\n",
    "- views to see all our running tasks, utilization across instance types, autoscaling, etc.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/observability_views.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🔎➕➕ Monitoring and Debugging on Anyscale</b> \n",
    "\n",
    "While OSS Ray comes with an extensive obervability suite, Anyscale takes it many steps further to make it even easier and faster to monitor and debug your workloads.\n",
    "\n",
    "- [unified log viewer](https://docs.anyscale.com/monitoring/accessing-logs/) to see logs from *all* our driver and worker processes\n",
    "- Ray workload specific dashboard (Data, Train, etc.) that can breakdown the tasks. For example, our training workload above can be observed live through the Train specific Ray Workloads dashboard:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/train_dashboard.png\" width=700>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🗂️ Storage on Anyscale</b> \n",
    "\n",
    "We can always store to our data inside [any storage buckets](https://docs.anyscale.com/configuration/storage/#private-storage-buckets) but Anyscale offers a [default storage bucket](https://docs.anyscale.com/configuration/storage/#anyscale-default-storage-bucket) to make things even easier. We also have plenty of other [storage options](https://docs.anyscale.com/configuration/storage/) as well (shared at the cluster, user and cloud levels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Anyscale default storage bucket\n",
    "echo $ANYSCALE_ARTIFACT_STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Save fine-tuning artifacts to cloud storage\n",
    "aws s3 rm $ANYSCALE_ARTIFACT_STORAGE/viggo --recursive --quiet\n",
    "aws s3 cp /mnt/cluster_storage/viggo/outputs $ANYSCALE_ARTIFACT_STORAGE/viggo/outputs --recursive --quiet\n",
    "aws s3 cp $2 /mnt/cluster_storage/viggo/saves $ANYSCALE_ARTIFACT_STORAGE/viggo/saves --recursive --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"p\">{</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;epoch&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">2.928</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.1033112108707428</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_runtime&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">19.7559</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_samples_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">36.141</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_steps_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">9.061</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;total_flos&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">2.805774446441267e+16</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.2595549144691998</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_runtime&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">263.3559</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_samples_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">11.391</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_steps_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.353</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}epoch\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{2.928}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}loss\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.1033112108707428}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}runtime\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{19.7559}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}samples\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{36.141}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}steps\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{9.061}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}total\\PYZus{}flos\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{2.805774446441267e+16}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}loss\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.2595549144691998}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}runtime\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{263.3559}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}samples\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{11.391}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}steps\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.353}\n",
       "\\PY{p}{\\PYZcb{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "{\n",
       "    \"epoch\": 2.928,\n",
       "    \"eval_viggo-val_loss\": 0.1033112108707428,\n",
       "    \"eval_viggo-val_runtime\": 19.7559,\n",
       "    \"eval_viggo-val_samples_per_second\": 36.141,\n",
       "    \"eval_viggo-val_steps_per_second\": 9.061,\n",
       "    \"total_flos\": 2.805774446441267e+16,\n",
       "    \"train_loss\": 0.2595549144691998,\n",
       "    \"train_runtime\": 263.3559,\n",
       "    \"train_samples_per_second\": 11.391,\n",
       "    \"train_steps_per_second\": 0.353\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"/mnt/cluster_storage/viggo/outputs/all_results.json\", language=\"json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(filename=\"/mnt/cluster_storage/viggo/outputs/training_loss.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchTrainer_7c0d8_00000_0_2025-04-09_22-56-02\n",
      "basic-variant-state-2025-04-09_22-56-02.json\n",
      "experiment_state-2025-04-09_22-56-02.json\n",
      "trainer.pkl\n",
      "tuner.pkl\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls /mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_7c0d8_00000_0_2025-04-09_22-56-02/checkpoint_000000/checkpoint\n",
      "s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_7c0d8_00000_0_2025-04-09_22-56-02/checkpoint_000000/checkpoint\n",
      "s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_7c0d8_00000_0_2025-04-09_22-56-02/checkpoint_000000\n",
      "checkpoint\n"
     ]
    }
   ],
   "source": [
    "# LoRA paths\n",
    "save_dir = Path(\"/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora\")\n",
    "trainer_dirs = [d for d in save_dir.iterdir() if d.name.startswith(\"TorchTrainer_\") and d.is_dir()]\n",
    "latest_trainer = max(trainer_dirs, key=lambda d: d.stat().st_mtime, default=None)\n",
    "lora_path = f\"{latest_trainer}/checkpoint_000000/checkpoint\"\n",
    "s3_lora_path = os.path.join(os.getenv(\"ANYSCALE_ARTIFACT_STORAGE\"), lora_path.split(\"/mnt/cluster_storage/\")[-1])\n",
    "dynamic_lora_path, lora_id = s3_lora_path.rsplit(\"/\", 1)\n",
    "print (lora_path)\n",
    "print (s3_lora_path)\n",
    "print (dynamic_lora_path)\n",
    "print (lora_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "adapter_config.json\n",
      "adapter_model.safetensors\n",
      "optimizer.pt\n",
      "rng_state_0.pth\n",
      "rng_state_1.pth\n",
      "rng_state_2.pth\n",
      "rng_state_3.pth\n",
      "scheduler.pt\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "trainer_state.json\n",
      "training_args.bin\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$lora_path\"\n",
    "ls $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference \n",
    "[`Overview`](https://docs.ray.io/en/latest/data/working-with-llms.html) |  [`API reference`](https://docs.ray.io/en/latest/data/api/llm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ray.data.llm` module integrates with key large language model (LLM) inference engines and deployed models to enable LLM batch inference. These llm modules use [Ray Data](https://docs.ray.io/en/latest/data/data.html) under the hood, which makes it extremely easy to distribute our workloads but also ensures that they happen:\n",
    "- **efficiently**: minimize CPU/GPU idletime with hetergenous resource scheduling.\n",
    "- **at scale**: streaming execution to petabyte-scale datasets (especially when [working with LLMs](https://docs.ray.io/en/latest/data/working-with-llms.html))\n",
    "- **reliably** by checkpointing processes, especially when running workloads on spot instanes (with on-demand fallback).\n",
    "- **flexiblibly**: connect to data from any source, apply your transformations and save to any format/location for your next workload.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/ray_data_solution.png\" width=800>\n",
    "\n",
    "[RayTurbo Data](https://docs.anyscale.com/rayturbo/rayturbo-data) has even more functionality on top of Ray Data:\n",
    "- **accelerated metadata fetching** to improve reading first time from large datasets \n",
    "- **optimized autoscaling** where Jobs can kick off before waiting for the entire cluster to start\n",
    "- **high reliabilty** where entire fails jobs (head node, cluster, uncaptured exceptions, etc.) can resume from checkpoints (OSS Ray can only recover from worker node failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the [vLLM engine processor config](https://docs.ray.io/en/latest/data/api/doc/ray.data.llm.vLLMEngineProcessorConfig.html#ray.data.llm.vLLMEngineProcessorConfig) where we can select the model we want to use and the [engine behavior](https://docs.vllm.ai/en/stable/serving/engine_args.html). The model can come from [HuggingFace (HF) Hub](https://huggingface.co/models) or a local model path `/path/to/your/model` (GPTQ, GGUF, or LoRA model formats supported).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/data_llm.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 23:02:04 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = vLLMEngineProcessorConfig(\n",
    "    model_source=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    runtime_env={\"env_vars\": {\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}},\n",
    "    engine_kwargs={\n",
    "        \"enable_lora\": True,\n",
    "        \"max_lora_rank\": 8,\n",
    "        \"max_loras\": 1,\n",
    "        \"pipeline_parallel_size\": 1, \n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"max_num_batched_tokens\": 4096,\n",
    "        # complete list: https://docs.vllm.ai/en/stable/serving/engine_args.html\n",
    "    },\n",
    "    concurrency=1,\n",
    "    batch_size=16,\n",
    "    accelerator_type=\"A10G\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll pass our config to an [llm processor](https://docs.ray.io/en/master/data/api/doc/ray.data.llm.build_llm_processor.html#ray.data.llm.build_llm_processor) where we can define the preprocessing and postprocessing steps around inference. With our base model defined in the processor config, we can define the lora adapter layers as part of the preprocessing step of the llm processor itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 23:02:05,172\tINFO worker.py:1660 -- Connecting to existing Ray cluster at address: 10.0.45.206:6379...\n",
      "2025-04-09 23:02:05,182\tINFO worker.py:1843 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-zt5t77xa58pyp3uy28glg2g24d.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-04-09 23:02:05,188\tINFO packaging.py:367 -- Pushing file package 'gcs://_ray_pkg_e0c21ef4701e68218e82c96d1812778e1ee8b5c9.zip' (2.01MiB) to Ray cluster...\n",
      "2025-04-09 23:02:05,197\tINFO packaging.py:380 -- Successfully pushed file package 'gcs://_ray_pkg_e0c21ef4701e68218e82c96d1812778e1ee8b5c9.zip'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c788d048f1c241798145cc25ec66ff2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=303109)\u001b[0m INFO 04-09 23:02:11 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    }
   ],
   "source": [
    "processor = build_llm_processor(\n",
    "    config,\n",
    "    preprocess=lambda row: dict(\n",
    "        model=lora_path,  # REMOVE this line if doing inference with just the base model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": row[\"input\"]}\n",
    "        ],\n",
    "        sampling_params={\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 250,\n",
    "            # complete list: https://docs.vllm.ai/en/stable/api/inference_params.html\n",
    "        },\n",
    "    ),\n",
    "    postprocess=lambda row: {\n",
    "        **row,  # all contents\n",
    "        \"generated_output\": row[\"generated_text\"],\n",
    "        # add additional outputs\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 23:02:20,279\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-09_22-54-49_487486_292703/logs/ray-data\n",
      "2025-04-09 23:02:20,280\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> ActorPoolMapOperator[Map(_preprocess)->MapBatches(ChatTemplateUDF)] -> ActorPoolMapOperator[MapBatches(TokenizeUDF)] -> ActorPoolMapOperator[MapBatches(vLLMEngineStageUDF)] -> ActorPoolMapOperator[MapBatches(DetokenizeUDF)] -> TaskPoolMapOperator[Map(_postprocess)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2962b57f5c4941985aa40a76a16878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=53538, ip=10.0.111.111)\u001b[0m INFO 04-09 23:02:25 __init__.py:190] Automatically detected platform cuda.\n",
      "\u001b[36m(_MapWorker pid=53630, ip=10.0.111.111)\u001b[0m INFO 04-09 23:02:33 __init__.py:190] Automatically detected platform cuda.\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:02:39 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m Max pending requests is set to 141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:02:48 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m WARNING 04-09 23:02:48 config.py:2162] LoRA with chunked prefill is still experimental and may be unstable.\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:02:48 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:02:50 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:02:51 model_runner.py:1110] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:02:51 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.50it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.04it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.49it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.31it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.49it/s]\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:02:54 model_runner.py:1115] Loading model weights took 14.9634 GB\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:02:54 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:02 worker.py:267] Memory profiling takes 7.10 seconds\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:02 worker.py:267] the current vLLM instance can use total_gpu_memory (21.99GiB) x gpu_memory_utilization (0.90) = 19.79GiB\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:02 worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 3.56GiB.\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:02 executor_base.py:110] # CUDA blocks: 1824, # CPU blocks: 2048\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:02 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 3.56x\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:05 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:01<00:36,  1.09s/it]\n",
      "Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:26,  1.26it/s]\n",
      "Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:22,  1.42it/s]\n",
      "Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]\n",
      "Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]\n",
      "Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:17,  1.62it/s]\n",
      "Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:16,  1.65it/s]\n",
      "Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.67it/s]\n",
      "Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:15,  1.69it/s]\n",
      "Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:14,  1.72it/s]\n",
      "Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:13,  1.74it/s]\n",
      "Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:13,  1.76it/s]\n",
      "Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:07<00:12,  1.78it/s]\n",
      "Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:11,  1.79it/s]\n",
      "Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:11,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:10,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:09,  1.82it/s]\n",
      "Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:10<00:09,  1.82it/s]\n",
      "Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:08,  1.84it/s]\n",
      "Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:11<00:08,  1.85it/s]\n",
      "Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:12<00:07,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:12<00:06,  1.87it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:13<00:06,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:13<00:05,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:14<00:05,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:14<00:04,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:15<00:04,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:16<00:03,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:16<00:03,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:17<00:02,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:17<00:02,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:18<00:01,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:18<00:01,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:19<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:25 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.41 GiB\n",
      "\u001b[36m(_MapWorker pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:25 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 30.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:19<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=53998, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:30 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6a719de3464d3081038d94cdc55aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ListFiles 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987229324b71484ea4131b1e640aa99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadFiles 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84b50229b954f5d8fd3fb95e37e337d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(_preprocess)->MapBatches(ChatTemplateUDF) 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c49a8472f84ad8856d59ff7b87b999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(TokenizeUDF) 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68769a1555c546aebee7132a0f5050c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(vLLMEngineStageUDF) 5: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d334102af6466ba329a43228891e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(DetokenizeUDF) 6: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b172bde3351f4036afa0861e71d7eece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(_postprocess) 7: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:39 metrics.py:455] Avg prompt throughput: 281.2 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 125 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 20.4%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:39 metrics.py:471] Prefix cache hit rate: GPU: 81.47%, CPU: 0.00%\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:44 metrics.py:455] Avg prompt throughput: 4402.7 tokens/s, Avg generation throughput: 276.5 tokens/s, Running: 141 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:44 metrics.py:471] Prefix cache hit rate: GPU: 82.24%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 918285b1c45b4b7fa55bf41bc9d746e9 with size 16: 10.353171740000107\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 91bcb4a8b2054f698fe5e16cc80039d8 with size 16: 10.35420935899674\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 4a758f7d3e8e40049454fb3ae9fcc414 with size 16: 10.52400933899844\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 011f885727804abfa79ab9fdadee641c with size 16: 11.27964028200222\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch fb932d03f4ef465c97d4dce75146f156 with size 16: 11.612004564998642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:49 metrics.py:455] Avg prompt throughput: 5891.5 tokens/s, Avg generation throughput: 1017.6 tokens/s, Running: 138 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.1%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:49 metrics.py:471] Prefix cache hit rate: GPU: 81.66%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 9f065c7eddbc42f492ae8d94f3791d39 with size 16: 13.161949983998056\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 9aa1c739ff714a4b8999503722112468 with size 16: 13.162136987997656\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 18dcff6c9557456fbe745622ad076e2c with size 16: 13.309402849001344\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch a832e44f9dda4c5fa0ba6c6b8a3f809b with size 16: 13.456944176999968\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 8369f84131dc4cf7a868237ab234a178 with size 16: 14.080507365000813\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 3b9983ff7ae94b498740293f01fbca6e with size 16: 14.478418546998\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch eadcd6f92ca34b34bfc11018b9f6a5c3 with size 16: 16.43745206499807\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 253c85bee6ff4e57b271b56210330fa6 with size 16: 16.80404111199823\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 2237de9cb7124a1b8d6ec8039b1ec9f7 with size 16: 17.203546701999585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:54 metrics.py:455] Avg prompt throughput: 6374.7 tokens/s, Avg generation throughput: 941.3 tokens/s, Running: 132 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.1%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:03:54 metrics.py:471] Prefix cache hit rate: GPU: 82.11%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 7f88663ef776484484f71e6d20c2e665 with size 16: 17.43080458699842\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 6b6e54c838574ef294bff43c0575a7ea with size 16: 17.59420379600124\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch e451ee6222994393a1f8501d97fa2254 with size 16: 18.418407609002315\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 60939a8c03bf4e319c6e7a11a1073569 with size 16: 18.76322501300092\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 1631ea3f566e4aae8f204a33ccc121ef with size 16: 18.763804492999043\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch ef47abfd955f4679bd47be8d733c7538 with size 16: 19.08219602999816\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 3f2cfc44ac704e4fa53c67881611061a with size 16: 20.03795431799881\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 0d16e485d6474c03a26b7b4ae6d85dd2 with size 16: 20.038758603001043\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch d22c2d7ded414a8584a858ea8c741565 with size 16: 20.367264937998698\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 999293447eb040c182ef77284ff3fcf8 with size 16: 20.903975241999433\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch a10c9605beba4948abee6293e41b9114 with size 16: 21.06860076999874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:04:00 metrics.py:455] Avg prompt throughput: 6131.1 tokens/s, Avg generation throughput: 1005.2 tokens/s, Running: 131 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.9%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:04:00 metrics.py:471] Prefix cache hit rate: GPU: 82.52%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch f84703ab73e140f9a8f386676f03afcc with size 16: 23.687944266999693\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 7f286b1310494d239593747a89d6bc33 with size 16: 23.688687339999888\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 908e354b12a34310bec4b6d5d0d80d89 with size 16: 23.68880353199711\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 290bec265b1b41a4a817cbbe06262cb7 with size 16: 23.68863207800314\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 9feadbd8e9734922a9e651a4baa105b4 with size 16: 24.038543991999177\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 9df3e0c059654316ba035ff27361dfb9 with size 16: 24.039294924998103\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 75bca2c164c04fc6beed557573511936 with size 16: 24.601128233000054\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 4745fea02aa94065aa435ed7107ea4ba with size 16: 24.937119661000906\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 96e5b8a4e67f44df956dc008a6945ab5 with size 16: 25.270934759999363\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 611972e27a9f463088ec1278a9ed097c with size 16: 25.418463197001984\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 8eceb5d466cb4eb09e6828c886bf45df with size 16: 26.198504480998963\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch a1901d160aad463e94e8c1b48b227833 with size 16: 26.37021351400108\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch ad25c069258f4f24897422b1aab035f1 with size 16: 26.68653205600276\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 9ee4591e0e4b45849bfe1b607d0a8108 with size 16: 27.266298844999255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:04:05 metrics.py:455] Avg prompt throughput: 6692.4 tokens/s, Avg generation throughput: 987.6 tokens/s, Running: 135 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.9%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:04:05 metrics.py:471] Prefix cache hit rate: GPU: 82.80%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch df106911d2604c38b1bdb2046500abf7 with size 16: 28.46336634399995\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch f961ae8d9c1440229e9342d6420ec31e with size 16: 28.566756658001395\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 2c8fe5cb387e43489e3ec32ff102d9e8 with size 16: 28.66843572200014\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 130ab95af6f646f0a76155491532589e with size 16: 29.274612645996967\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch b1c77f95e6674d90b8ea814d469e165b with size 16: 30.057514157997502\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 6ec035d3f27e42409602409dfafbd6eb with size 16: 30.30578209899977\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch e1c7eb4c593441c193902fb04f273563 with size 16: 30.65250596599799\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 493057fba038492bb79cc961fe0327e4 with size 16: 31.314017166998383\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 65c442ed8a43499aa2519605bd151b5d with size 16: 31.64960967799925\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch c4393493642443ae83aae315e987e646 with size 16: 31.650178078001773\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 88521d1c7f1c40bbbb9a92ddd0b9ad8d with size 16: 31.964511432997824\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch e0414995765e469dbf9ef9f86e0b44a4 with size 16: 32.10882667899932\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch de4cc45a47c648cfaa9cac16cb2b5540 with size 16: 32.5882648279985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:04:10 metrics.py:455] Avg prompt throughput: 6996.9 tokens/s, Avg generation throughput: 929.7 tokens/s, Running: 136 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 30.7%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m INFO 04-09 23:04:10 metrics.py:471] Prefix cache hit rate: GPU: 82.76%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 0bfff9d30cc948979cf4a6707cf77e29 with size 16: 34.58387783500075\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch ee17ffce1d384760af11c0ab16438dd4 with size 16: 35.02139270399857\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch a4bc15f5591f48978bc5c630d9431aa5 with size 16: 35.09346866600026\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 8431076551b14a36983333d133bf5152 with size 16: 35.230802549001965\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 2f623997af0547278d8aaa254410e878 with size 16: 35.231007022000995\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch c2d8f3fbf07d418bacacf39aef772c15 with size 16: 35.36496385600185\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 41d16abd15a745c685e1c898826420c2 with size 16: 35.42880639199939\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 4959a202348d4fd5af8812397155e803 with size 16: 35.42929636100234\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 9e0c39b4638b4051b9655d4775c40c5a with size 16: 36.12624201599829\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 8698b35e77ec4a12885ba95c7247888b with size 11: 36.23872414099969\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch d5a83f22a84949acb72d4644bcd4bde1 with size 16: 36.504984057999536\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 1dbd6885fa86498ebd59a03e769fbe64 with size 16: 36.50577864300067\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch d26b8777823a440ea0474ec9821ac998 with size 16: 36.63127029699899\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 0eb8aeae104c4544bfb990fa8700e51e with size 16: 36.755372085997806\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch b4c17b965edb476597507ea7576b9b97 with size 16: 36.911264835998736\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=53717, ip=10.0.111.111)\u001b[0m [vLLM] Elapsed time for batch 4e8094beb1bd46cb89dc3a49b82e8389 with size 16: 36.91174703499928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_uuid': '8369f84131dc4cf7a868237ab234a178',\n",
       " 'embeddings': None,\n",
       " 'generated_text': 'request(specifier[weirdest])',\n",
       " 'generated_tokens': [2079, 39309, 3125, 58, 906, 404, 5086, 2526, 128009],\n",
       " 'input': \"What do you think is the weirdest game you've ever played?\",\n",
       " 'instruction': \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
       " 'messages': [{'content': \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
       "   'role': 'system'},\n",
       "  {'content': \"What do you think is the weirdest game you've ever played?\",\n",
       "   'role': 'user'}],\n",
       " 'metrics': {'arrival_time': 1744265017.3946567,\n",
       "  'finished_time': 1744265024.4818223,\n",
       "  'first_scheduled_time': 1744265018.197879,\n",
       "  'first_token_time': 1744265019.7056925,\n",
       "  'last_token_time': 1744265024.4798005,\n",
       "  'model_execute_time': None,\n",
       "  'model_forward_time': None,\n",
       "  'scheduler_time': 0.048756210006104084,\n",
       "  'time_in_queue': 0.8032224178314209},\n",
       " 'model': '/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_7c0d8_00000_0_2025-04-09_22-56-02/checkpoint_000000/checkpoint',\n",
       " 'num_generated_tokens': 9,\n",
       " 'num_input_tokens': 170,\n",
       " 'output': 'request(specifier[weirdest])',\n",
       " 'params': 'SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.3, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=250, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None)',\n",
       " 'prompt': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nGiven a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat do you think is the weirdest game you've ever played?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
       " 'prompt_token_ids': [128000,\n",
       "  128000,\n",
       "  128006,\n",
       "  9125,\n",
       "  128007,\n",
       "  271,\n",
       "  22818,\n",
       "  264,\n",
       "  2218,\n",
       "  11914,\n",
       "  9429,\n",
       "  279,\n",
       "  16940,\n",
       "  7438,\n",
       "  13340,\n",
       "  315,\n",
       "  279,\n",
       "  1988,\n",
       "  11914,\n",
       "  439,\n",
       "  264,\n",
       "  3254,\n",
       "  734,\n",
       "  449,\n",
       "  8365,\n",
       "  323,\n",
       "  7180,\n",
       "  2819,\n",
       "  13,\n",
       "  1115,\n",
       "  734,\n",
       "  1288,\n",
       "  7664,\n",
       "  279,\n",
       "  2218,\n",
       "  925,\n",
       "  30357,\n",
       "  323,\n",
       "  279,\n",
       "  734,\n",
       "  2011,\n",
       "  387,\n",
       "  832,\n",
       "  315,\n",
       "  279,\n",
       "  2768,\n",
       "  2570,\n",
       "  41540,\n",
       "  518,\n",
       "  364,\n",
       "  2079,\n",
       "  518,\n",
       "  364,\n",
       "  47530,\n",
       "  10499,\n",
       "  37400,\n",
       "  518,\n",
       "  364,\n",
       "  14119,\n",
       "  518,\n",
       "  364,\n",
       "  12728,\n",
       "  17209,\n",
       "  518,\n",
       "  364,\n",
       "  96861,\n",
       "  518,\n",
       "  364,\n",
       "  2079,\n",
       "  2769,\n",
       "  36990,\n",
       "  518,\n",
       "  364,\n",
       "  67689,\n",
       "  518,\n",
       "  364,\n",
       "  2079,\n",
       "  17209,\n",
       "  7352,\n",
       "  578,\n",
       "  8365,\n",
       "  2011,\n",
       "  387,\n",
       "  832,\n",
       "  315,\n",
       "  279,\n",
       "  2768,\n",
       "  25,\n",
       "  2570,\n",
       "  609,\n",
       "  518,\n",
       "  364,\n",
       "  4683,\n",
       "  25596,\n",
       "  4257,\n",
       "  518,\n",
       "  364,\n",
       "  23859,\n",
       "  14987,\n",
       "  518,\n",
       "  364,\n",
       "  35501,\n",
       "  518,\n",
       "  364,\n",
       "  288,\n",
       "  10910,\n",
       "  518,\n",
       "  364,\n",
       "  22696,\n",
       "  518,\n",
       "  364,\n",
       "  65011,\n",
       "  518,\n",
       "  364,\n",
       "  3517,\n",
       "  623,\n",
       "  86191,\n",
       "  518,\n",
       "  364,\n",
       "  4752,\n",
       "  26190,\n",
       "  3517,\n",
       "  518,\n",
       "  364,\n",
       "  16111,\n",
       "  82,\n",
       "  518,\n",
       "  364,\n",
       "  10547,\n",
       "  4570,\n",
       "  1284,\n",
       "  14922,\n",
       "  518,\n",
       "  364,\n",
       "  4752,\n",
       "  78563,\n",
       "  25596,\n",
       "  518,\n",
       "  364,\n",
       "  4752,\n",
       "  23647,\n",
       "  25596,\n",
       "  518,\n",
       "  364,\n",
       "  68351,\n",
       "  663,\n",
       "  128009,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  3923,\n",
       "  656,\n",
       "  499,\n",
       "  1781,\n",
       "  374,\n",
       "  279,\n",
       "  98086,\n",
       "  5086,\n",
       "  1847,\n",
       "  499,\n",
       "  3077,\n",
       "  3596,\n",
       "  6476,\n",
       "  30,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271],\n",
       " 'request_id': 12,\n",
       " 'time_taken_llm': 7.175398040002619,\n",
       " 'generated_output': 'request(specifier[weirdest])'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation on test dataset\n",
    "ds = ray.data.read_json(\"/mnt/cluster_storage/viggo/test.jsonl\")  # complete list: https://docs.ray.io/en/latest/data/api/input_output.html\n",
    "ds = processor(ds)\n",
    "results = ds.take_all()\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7571560480147738"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exact match (strict!)\n",
    "matches = 0\n",
    "for item in results:\n",
    "    if item[\"output\"] == item[\"generated_output\"]:\n",
    "        matches += 1\n",
    "matches / float(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can observe the individual steps in our our batch inference workload through the Anyscale Ray Data dashboard:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/data_dashboard.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "💡 For more advanced guides on topics like optimized model loading, multi-lora, openai-compatible endpoints, etc. check out [more examples](https://docs.ray.io/en/latest/data/working-with-llms.html) and the [API reference](https://docs.ray.io/en/latest/data/api/llm.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online serving\n",
    "[`Overview`](https://docs.ray.io/en/latest/serve/llm/serving-llms.html) | [`API reference`](https://docs.ray.io/en/latest/serve/api/index.html#llm-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/ray_serve.png\" width=600>\n",
    "\n",
    "`ray.serve.llm` APIs allow users to deploy multiple LLM models together with a familiar Ray Serve API, while providing compatibility with the OpenAI API.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/serve_llm.png\" width=500>\n",
    "\n",
    "Ray Serve LLM is designed with the following features:\n",
    "- Automatic scaling and load balancing\n",
    "- Unified multi-node multi-model deployment\n",
    "- OpenAI compatibility\n",
    "- Multi-LoRA support with shared base models\n",
    "- Deep integration with inference engines (vLLM to start)\n",
    "- Composable multi-model LLM pipelines\n",
    "\n",
    "[RayTurbo Serve](https://docs.anyscale.com/rayturbo/rayturbo-serve) on Anyscale has even more functionality on top of Ray Serve:\n",
    "- **fast autoscaling and model loading** to get our services up and running even faster ([5x improvements](https://www.anyscale.com/blog/autoscale-large-ai-models-faster) even for LLMs)\n",
    "- 54% **higher QPS** and up-to 3x **streaming tokens per second** for high traffic serving use-cases\n",
    "- **replica compaction** into fewer nodes where possible to reduce resource fragmentation and improve hardware utilization\n",
    "- **zero-downtime** [incremental rollouts](https://docs.anyscale.com/platform/services/update-a-service/#resource-constrained-updates) so your service is never interrupted\n",
    "- [**different environments**](https://docs.anyscale.com/platform/services/multi-app/#multiple-applications-in-different-containers) for each service in a multi-serve application\n",
    "- **multi availability-zone** aware scheduling of Ray Serve replicas to provide higher redundancy to availability zone failures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI  # to use openai api format\n",
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, build_openai_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an [LLM config](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) where we can define where our model comes from, it's [autoscaling behavior](https://docs.ray.io/en/latest/serve/autoscaling-guide.html#serve-autoscaling), what hardware to use and [engine arguments](https://docs.vllm.ai/en/stable/serving/engine_args.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config\n",
    "model_id = \"llama-3-8b-instruct\"  # call it whatever you want\n",
    "model_source = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # HF model ID, S3 mirror config, or GCS mirror config\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config={\n",
    "        \"model_id\": model_id,\n",
    "        \"model_source\": model_source\n",
    "    },\n",
    "    lora_config={  # REMOVE this section if you are only using a base model\n",
    "        \"dynamic_lora_loading_path\": dynamic_lora_path,\n",
    "        \"max_num_adapters_per_replica\": 16,  # we only have 1\n",
    "    },\n",
    "    runtime_env={\"env_vars\": {\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}},\n",
    "    deployment_config={\n",
    "        \"autoscaling_config\": {\n",
    "            \"min_replicas\": 1, \n",
    "            \"max_replicas\": 2,\n",
    "            # complete list: https://docs.ray.io/en/latest/serve/autoscaling-guide.html#serve-autoscaling\n",
    "        }\n",
    "    },\n",
    "    accelerator_type=\"A10G\",\n",
    "    engine_kwargs={\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"enable_lora\": True,\n",
    "        # complete list: https://docs.vllm.ai/en/stable/serving/engine_args.html\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll deploy our llm config as an application. And since this is all built on top of [Ray Serve](https://docs.ray.io/en/latest/serve/index.html), we can have advanvced service logic around composing models together, deploying multiple applications, model multiplexing, observability, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-04-09 23:04:38,444 serve 300688 -- Started Serve in namespace \"serve\".\n",
      "\u001b[36m(ServeController pid=303792)\u001b[0m INFO 2025-04-09 23:04:38,457 controller 303792 -- Deploying new version of Deployment(name='LLMDeployment:llama-3-8b-instruct', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=303792)\u001b[0m INFO 2025-04-09 23:04:38,458 controller 303792 -- Deploying new version of Deployment(name='LLMRouter', app='default') (initial target replicas: 2).\n",
      "\u001b[36m(ProxyActor pid=303855)\u001b[0m INFO 2025-04-09 23:04:38,364 proxy 10.0.45.206 -- Proxy starting on node 375cb438a81c8af7ab75ca4792b317278a709f4ea793eac6b4ddd264 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=303855)\u001b[0m INFO 2025-04-09 23:04:38,412 proxy 10.0.45.206 -- Got updated endpoints: {}.\n",
      "\u001b[36m(ProxyActor pid=303855)\u001b[0m INFO 2025-04-09 23:04:38,461 proxy 10.0.45.206 -- Got updated endpoints: {Deployment(name='LLMRouter', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ServeController pid=303792)\u001b[0m INFO 2025-04-09 23:04:38,562 controller 303792 -- Adding 1 replica to Deployment(name='LLMDeployment:llama-3-8b-instruct', app='default').\n",
      "\u001b[36m(ProxyActor pid=303855)\u001b[0m INFO 2025-04-09 23:04:38,477 proxy 10.0.45.206 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7d647b402630>.\n",
      "\u001b[36m(ServeController pid=303792)\u001b[0m INFO 2025-04-09 23:04:38,564 controller 303792 -- Adding 2 replicas to Deployment(name='LLMRouter', app='default').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 04-09 23:04:43 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:04:44,746 default_LLMDeployment:llama-3-8b-instruct vllfluch -- No cloud storage mirror configured\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:04:44,746 default_LLMDeployment:llama-3-8b-instruct vllfluch -- Downloading the tokenizer for meta-llama/Meta-Llama-3-8B-Instruct\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ProxyActor pid=54413, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:04:45,652 proxy 10.0.111.111 -- Proxy starting on node 60601da1390eefd14bfef564c865ae4654f6cb7d86cf019cf75ef9d8 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=54413, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:04:45,705 proxy 10.0.111.111 -- Got updated endpoints: {Deployment(name='LLMRouter', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=54413, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:04:45,720 proxy 10.0.111.111 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7b18416946b0>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 04-09 23:04:54 config.py:542] This model supports multiple tasks: {'embed', 'classify', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "\u001b[36m(ServeReplica:default:LLMRouter pid=54415, ip=10.0.111.111)\u001b[0m INFO 04-09 23:04:44 __init__.py:190] Automatically detected platform cuda.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:04:55,162 default_LLMDeployment:llama-3-8b-instruct vllfluch -- [STATUS] Getting the server ready ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:00 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:05:00,840 serve 54924 -- Clearing the current platform cache ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:00 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m Connecting to existing Ray cluster at address: 10.0.45.206:6379...\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:01 ray_distributed_executor.py:149] use_ray_spmd_worker: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:05:05,212 default_LLMDeployment:llama-3-8b-instruct vllfluch -- [STATUS] Waiting for engine process ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=55013, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:06 __init__.py:190] Automatically detected platform cuda.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:08 cuda.py:230] Using Flash Attention backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=303792)\u001b[0m WARNING 2025-04-09 23:05:08,652 controller 303792 -- Deployment 'LLMDeployment:llama-3-8b-instruct' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=303792)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=303792)\u001b[0m WARNING 2025-04-09 23:05:08,653 controller 303792 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=303792)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:09 model_runner.py:1110] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:09 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.54it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.06it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.50it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.51it/s]\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:12 model_runner.py:1115] Loading model weights took 14.9634 GB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:12 punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:05:16,263 default_LLMDeployment:llama-3-8b-instruct vllfluch -- [STATUS] Waiting for engine process ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:18 worker.py:267] Memory profiling takes 5.97 seconds\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:18 worker.py:267] the current vLLM instance can use total_gpu_memory (21.99GiB) x gpu_memory_utilization (0.90) = 19.79GiB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:18 worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.24GiB; the rest of the memory reserved for KV Cache is 3.53GiB.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:19 executor_base.py:110] # CUDA blocks: 1808, # CPU blocks: 2048\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:19 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 3.53x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:22 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:29,  1.14it/s]\n",
      "Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:23,  1.39it/s]\n",
      "Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]\n",
      "Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]\n",
      "Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]\n",
      "Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]\n",
      "Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:05:27,315 default_LLMDeployment:llama-3-8b-instruct vllfluch -- [STATUS] Waiting for engine process ...\n",
      "Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.61it/s]\n",
      "Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:15,  1.64it/s]\n",
      "Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.67it/s]\n",
      "Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.69it/s]\n",
      "Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:13,  1.71it/s]\n",
      "Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:12,  1.72it/s]\n",
      "Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:12,  1.72it/s]\n",
      "Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:11,  1.73it/s]\n",
      "Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.73it/s]\n",
      "Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:10,  1.75it/s]\n",
      "Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:10<00:09,  1.77it/s]\n",
      "Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:08,  1.78it/s]\n",
      "Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:08,  1.79it/s]\n",
      "Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:12<00:07,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:07,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:13<00:06,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:14<00:05,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:15<00:04,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:15<00:04,  1.80it/s]\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:05:38,359 default_LLMDeployment:llama-3-8b-instruct vllfluch -- [STATUS] Waiting for engine process ...\n",
      "Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:16<00:03,  1.81it/s]\n",
      "\u001b[36m(ServeController pid=303792)\u001b[0m WARNING 2025-04-09 23:05:38,728 controller 303792 -- Deployment 'LLMDeployment:llama-3-8b-instruct' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=303792)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=303792)\u001b[0m WARNING 2025-04-09 23:05:38,729 controller 303792 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=303792)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:16<00:03,  1.81it/s]\n",
      "Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:17<00:02,  1.81it/s]\n",
      "Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:18<00:02,  1.81it/s]\n",
      "Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:18<00:01,  1.81it/s]\n",
      "Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:19<00:01,  1.81it/s]\n",
      "Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:19<00:00,  1.81it/s]\n",
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:20<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:42 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.41 GiB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:05:42 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 29.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:05:42,645 default_LLMDeployment:llama-3-8b-instruct vllfluch -- [STATUS] Server is ready.\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:05:42,645 default_LLMDeployment:llama-3-8b-instruct vllfluch -- Started vLLM engine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m WARNING: 4 PYTHON worker processes have been started on node: 375cb438a81c8af7ab75ca4792b317278a709f4ea793eac6b4ddd264 with address: 10.0.45.206. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[36m(pid=304273)\u001b[0m INFO 04-09 23:05:49 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:05:50,345 default_LLMDeployment:llama-3-8b-instruct vllfluch 34974a51-acd9-4696-b485-52c948100833 -- CALL llm_config OK 300.6ms\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:05:50,351 default_LLMDeployment:llama-3-8b-instruct vllfluch 2f5142ec-0a45-45c7-88a4-f80ded7c39bd -- CALL llm_config OK 305.4ms\n",
      "INFO 2025-04-09 23:05:51,701 serve 300688 -- Application 'default' is ready at http://127.0.0.1:8000/.\n",
      "INFO 2025-04-09 23:05:51,708 serve 300688 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x767d1c0dd760>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeploymentHandle(deployment='LLMRouter')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})\n",
    "serve.run(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:06:28,939 default_LLMDeployment:llama-3-8b-instruct vllfluch 4f3bbf11-2236-4268-80bc-3fdc928c86fa -- Received streaming request 4f3bbf11-2236-4268-80bc-3fdc928c86fa\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:06:28,940 default_LLMDeployment:llama-3-8b-instruct vllfluch 4f3bbf11-2236-4268-80bc-3fdc928c86fa -- Request 4f3bbf11-2236-4268-80bc-3fdc928c86fa started. Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m \n",
      "{\"asctime\": \"2025-04-09 23:06:29,044\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"job_id\": \"04000000\", \"worker_id\": \"04000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"375cb438a81c8af7ab75ca4792b317278a709f4ea793eac6b4ddd264\", \"timestamp_ns\": 1744265189044618374}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:06:28 engine.py:275] Added request 4f3bbf11-2236-4268-80bc-3fdc928c86fa.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:06:29 metrics.py:455] Avg prompt throughput: 24.2 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.\n",
      "give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:06:29 engine.py:293] Aborted request 4f3bbf11-2236-4268-80bc-3fdc928c86fa.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMRouter pid=54414, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:06:29,896 default_LLMRouter md1yf1gj 4f3bbf11-2236-4268-80bc-3fdc928c86fa -- POST /v1/chat/completions 200 964.8ms\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:06:29,892 default_LLMDeployment:llama-3-8b-instruct vllfluch 4f3bbf11-2236-4268-80bc-3fdc928c86fa -- Request 4f3bbf11-2236-4268-80bc-3fdc928c86fa finished (stop). Total time: 0.9516414559984696s, Queue time: 0.0012445449829101562s, Generation+async time: 0.9503969110155595s, Input tokens: 187, Generated tokens: 26, tokens/s: 224.11689003954777, generated tokens/s: 27.35699127243306.\n",
      "\u001b[36m(ServeReplica:default:LLMDeployment:llama-3-8b-instruct pid=54412, ip=10.0.111.111)\u001b[0m INFO 2025-04-09 23:06:29,893 default_LLMDeployment:llama-3-8b-instruct vllfluch 4f3bbf11-2236-4268-80bc-3fdc928c86fa -- CALL /v1/chat/completions OK 954.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:06:39 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=54924, ip=10.0.111.111)\u001b[0m INFO 04-09 23:06:49 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "# Initialize client\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"fake-key\")\n",
    "response = client.chat.completions.create(\n",
    "    model=f\"{model_id}:{lora_id}\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\"},\n",
    "        {\"role\": \"user\", \"content\": \"Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.\"},\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can observe our running service (deployments and metrics like QPS, latency, etc.) through the [Ray Dashboard](https://docs.ray.io/en/latest/ray-observability/getting-started.html)'s [Serve view](https://docs.ray.io/en/latest/ray-observability/getting-started.html#dash-serve-view):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/serve_dashboard.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "💡 For more advanced guides on topics like structured outputs (ex. json), vision LMs, multi-lora on shared base models, using other inference engines (ex. sglang), etc. fast model loading, etc. check out [more examples](https://docs.ray.io/en/latest/serve/llm/overview.html) and the [API reference](https://docs.ray.io/en/latest/serve/llm/api.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production\n",
    "\n",
    "Seamlessly integrate with your existing CI/CD pipelines by leveraging the Anyscale [CLI](https://docs.anyscale.com/reference/quickstart-cli) or [SDK](https://docs.anyscale.com/reference/quickstart-sdk) to run [reliable batch jobs](https://docs.anyscale.com/platform/jobs) and deploy [highly available services](https://docs.anyscale.com/platform/services). Given we've been developing in an environment that's almost identical to production (multinode cluster), this should drastically speed up our dev → prod velocity.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/cicd.png\" width=600>\n",
    "\n",
    "[Anyscale Jobs](https://docs.anyscale.com/platform/jobs/) ([API ref](https://docs.anyscale.com/reference/job-api/)) allows us to execute discrete workloads in production such as batch inference, embeddings generation, or model fine-tuning.\n",
    "- [define and manage](https://docs.anyscale.com/platform/jobs/manage-jobs) our Jobs in many different ways (CLI, Python SDK)\n",
    "- set up [queues](https://docs.anyscale.com/platform/jobs/job-queues) and [schedules](https://docs.anyscale.com/platform/jobs/schedules)\n",
    "- set up all the [observability, alerting, etc.](https://docs.anyscale.com/platform/jobs/monitoring-and-debugging) around our Jobs\n",
    "\n",
    "[Anyscale Services](https://docs.anyscale.com/platform/services/) ([API ref](https://docs.anyscale.com/reference/service-api/)) offers an extremely fault tolerant, scalable and optimized way to serve our Ray Serve applications.\n",
    "- we can [rollout and update](https://docs.anyscale.com/platform/services/update-a-service) our services with canary deployment (zero-downtime upgrades)\n",
    "- [monitor](https://docs.anyscale.com/platform/services/monitoring) our Services through a dedicated Service page, unified log viewer, tracing, set up alerts, etc.\n",
    "- scale a service (`num_replicas=auto`) and utilize replica compaction to consolidate nodes that are fractionally utilized\n",
    "- [head node fault tolerance](https://docs.anyscale.com/platform/services/production-best-practices#head-node-ft) (OSS Ray recovers from failed workers and replicas but not head node crashes)\n",
    "- serving [muliple applications](https://docs.anyscale.com/platform/services/multi-app) in a single Service\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/canary.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# clean up\n",
    "rm -rf /mnt/cluster_storage/viggo  # clean up\n",
    "aws s3 rm $ANYSCALE_ARTIFACT_STORAGE/viggo --recursive --quiet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
