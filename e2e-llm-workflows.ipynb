{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Recognition with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune an LLM to perform batch inference and online serving for entity recognition. \n",
    "\n",
    "**Note**: the intent of this tutorial is to show how Ray can be use to implement end-to-end LLM workflows that can extend to any use case. Also the objective of fine-tuning here is not to create the most performant model (increase `num_train_epochs` if you want to though) but to show it can be leveraged for downstream workloads (batch inference and online serving) at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import textwrap\n",
    "from IPython.display import Code, Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by downloading our data from cloud storage to local shared storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://viggo-ds/train.jsonl to ../../../mnt/cluster_storage/viggo/train.jsonl\n",
      "download: s3://viggo-ds/val.jsonl to ../../../mnt/cluster_storage/viggo/val.jsonl\n",
      "download: s3://viggo-ds/test.jsonl to ../../../mnt/cluster_storage/viggo/test.jsonl\n",
      "download: s3://viggo-ds/dataset_info.json to ../../../mnt/cluster_storage/viggo/dataset_info.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf /mnt/cluster_storage/viggo  # clean up\n",
    "aws s3 cp  s3://viggo-ds/train.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/val.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/test.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/dataset_info.json /mnt/cluster_storage/viggo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"instruction\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
      "  \"input\": \"Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.\",\n",
      "  \"output\": \"give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 1 /mnt/cluster_storage/viggo/train.jsonl | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a target sentence construct the underlying meaning representation of the\n",
      "input sentence as a single function with attributes and attribute values. This\n",
      "function should describe the target string accurately and the function must be\n",
      "one of the following ['inform', 'request', 'give_opinion', 'confirm',\n",
      "'verify_attribute', 'suggest', 'request_explanation', 'recommend',\n",
      "'request_attribute']. The attributes must be one of the following: ['name',\n",
      "'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres',\n",
      "'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam',\n",
      "'has_linux_release', 'has_mac_release', 'specifier']\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/cluster_storage/viggo/train.jsonl\", \"r\") as fp:\n",
    "    first_line = fp.readline()\n",
    "    item = json.loads(first_line)\n",
    "system_content = item[\"instruction\"]\n",
    "print(textwrap.fill(system_content, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have an info file that identifies the datasets and format --- alpaca and sharegpt (great for multimodal tasks) formats are supported --- to use for post training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"p\">{</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;viggo-train&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;file_name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;/mnt/cluster_storage/viggo/train.jsonl&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;formatting&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;alpaca&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;columns&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;prompt&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;instruction&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;query&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;response&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;output&quot;</span>\n",
       "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">},</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;viggo-val&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;file_name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;/mnt/cluster_storage/viggo/val.jsonl&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;formatting&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;alpaca&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;columns&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;prompt&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;instruction&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;query&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;response&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;output&quot;</span>\n",
       "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}viggo\\PYZhy{}train\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}file\\PYZus{}name\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}/mnt/cluster\\PYZus{}storage/viggo/train.jsonl\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}formatting\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}alpaca\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}columns\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}prompt\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}instruction\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}query\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}input\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}response\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}output\\PYZdq{}}\n",
       "\\PY{+w}{        }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{},}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}viggo\\PYZhy{}val\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}file\\PYZus{}name\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}/mnt/cluster\\PYZus{}storage/viggo/val.jsonl\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}formatting\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}alpaca\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}columns\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}prompt\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}instruction\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}query\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}input\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}response\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}output\\PYZdq{}}\n",
       "\\PY{+w}{        }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{p}{\\PYZcb{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "{\n",
       "    \"viggo-train\": {\n",
       "        \"file_name\": \"/mnt/cluster_storage/viggo/train.jsonl\",\n",
       "        \"formatting\": \"alpaca\",\n",
       "        \"columns\": {\n",
       "            \"prompt\": \"instruction\",\n",
       "            \"query\": \"input\",\n",
       "            \"response\": \"output\"\n",
       "        }\n",
       "    },\n",
       "    \"viggo-val\": {\n",
       "        \"file_name\": \"/mnt/cluster_storage/viggo/val.jsonl\",\n",
       "        \"formatting\": \"alpaca\",\n",
       "        \"columns\": {\n",
       "            \"prompt\": \"instruction\",\n",
       "            \"query\": \"input\",\n",
       "            \"response\": \"output\"\n",
       "        }\n",
       "    }\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"/mnt/cluster_storage/viggo/dataset_info.json\", language=\"json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [Ray Train](https://docs.ray.io/en/latest/train/train.html) + [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) to peform multinode training. The parameters for our training workload -- post-training method, dataset location, train/val details, etc. --- can be found in the `llama3_lora_sft_ray.yaml` config file. Check out recipes for even more post-training methods (sft, pretraining, ppo, dpo, kto, etc.) [here](https://github.com/hiyouga/LLaMA-Factory/tree/main/examples).\n",
    "\n",
    "**Note**: We also support using other tools like [axolotl](https://axolotl-ai-cloud.github.io/axolotl/docs/ray-integration.html) or even [Ray Train + HF Accelreate + FSDP/Deepspeed](https://docs.ray.io/en/latest/train/huggingface-accelerate.html) directly for complete control of your post-training workloads.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/distributed_training.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\">### model</span>\n",
       "<span class=\"nt\">model_name_or_path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">meta-llama/Meta-Llama-3-8B-Instruct</span>\n",
       "<span class=\"nt\">trust_remote_code</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "\n",
       "<span class=\"c1\">### method</span>\n",
       "<span class=\"nt\">stage</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">sft</span>\n",
       "<span class=\"nt\">do_train</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">finetuning_type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">lora</span>\n",
       "<span class=\"nt\">lora_rank</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">8</span>\n",
       "<span class=\"nt\">lora_target</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">all</span>\n",
       "\n",
       "<span class=\"c1\">### dataset</span>\n",
       "<span class=\"nt\">dataset</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">viggo-train</span>\n",
       "<span class=\"nt\">dataset_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo</span><span class=\"w\">  </span><span class=\"c1\"># shared storage workers have access to</span>\n",
       "<span class=\"nt\">template</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">llama3</span>\n",
       "<span class=\"nt\">cutoff_len</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">2048</span>\n",
       "<span class=\"nt\">max_samples</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1000</span>\n",
       "<span class=\"nt\">overwrite_cache</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">preprocessing_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">16</span>\n",
       "<span class=\"nt\">dataloader_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">4</span>\n",
       "\n",
       "<span class=\"c1\">### output</span>\n",
       "<span class=\"nt\">output_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo/outputs</span><span class=\"w\">  </span><span class=\"c1\"># should be somewhere workers have access to (ex. s3, nfs)</span>\n",
       "<span class=\"nt\">logging_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">10</span>\n",
       "<span class=\"nt\">save_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">500</span>\n",
       "<span class=\"nt\">plot_loss</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">overwrite_output_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">save_only_model</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">false</span>\n",
       "\n",
       "<span class=\"c1\">### ray</span>\n",
       "<span class=\"nt\">ray_run_name</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">llama3_8b_sft_lora</span>\n",
       "<span class=\"nt\">ray_storage_path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo/saves</span><span class=\"w\">  </span><span class=\"c1\"># should be somewhere workers have access to (ex. s3, nfs)</span>\n",
       "<span class=\"nt\">ray_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">4</span><span class=\"w\">  </span><span class=\"c1\"># number of GPUs to use</span>\n",
       "<span class=\"nt\">resources_per_worker</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">GPU</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"nt\">placement_strategy</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">PACK</span>\n",
       "\n",
       "<span class=\"c1\">### train</span>\n",
       "<span class=\"nt\">per_device_train_batch_size</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"nt\">gradient_accumulation_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">8</span>\n",
       "<span class=\"nt\">learning_rate</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1.0e-4</span>\n",
       "<span class=\"nt\">num_train_epochs</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">5.0</span>\n",
       "<span class=\"nt\">lr_scheduler_type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">cosine</span>\n",
       "<span class=\"nt\">warmup_ratio</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">0.1</span>\n",
       "<span class=\"nt\">bf16</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">ddp_timeout</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">180000000</span>\n",
       "<span class=\"nt\">resume_from_checkpoint</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">null</span>\n",
       "\n",
       "<span class=\"c1\">### eval</span>\n",
       "<span class=\"nt\">eval_dataset</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">viggo-val</span><span class=\"w\">  </span><span class=\"c1\"># uses same dataset_dir as training data</span>\n",
       "<span class=\"c1\"># val_size: 0.1  # only if using part of training data for validation</span>\n",
       "<span class=\"nt\">per_device_eval_batch_size</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"nt\">eval_strategy</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">steps</span>\n",
       "<span class=\"nt\">eval_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">500</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} model}\n",
       "\\PY{n+nt}{model\\PYZus{}name\\PYZus{}or\\PYZus{}path}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{meta\\PYZhy{}llama/Meta\\PYZhy{}Llama\\PYZhy{}3\\PYZhy{}8B\\PYZhy{}Instruct}\n",
       "\\PY{n+nt}{trust\\PYZus{}remote\\PYZus{}code}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} method}\n",
       "\\PY{n+nt}{stage}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{sft}\n",
       "\\PY{n+nt}{do\\PYZus{}train}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{finetuning\\PYZus{}type}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{lora}\n",
       "\\PY{n+nt}{lora\\PYZus{}rank}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{8}\n",
       "\\PY{n+nt}{lora\\PYZus{}target}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{all}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} dataset}\n",
       "\\PY{n+nt}{dataset}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{viggo\\PYZhy{}train}\n",
       "\\PY{n+nt}{dataset\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} shared storage workers have access to}\n",
       "\\PY{n+nt}{template}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{llama3}\n",
       "\\PY{n+nt}{cutoff\\PYZus{}len}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{2048}\n",
       "\\PY{n+nt}{max\\PYZus{}samples}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1000}\n",
       "\\PY{n+nt}{overwrite\\PYZus{}cache}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{preprocessing\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{16}\n",
       "\\PY{n+nt}{dataloader\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{4}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} output}\n",
       "\\PY{n+nt}{output\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo/outputs}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} should be somewhere workers have access to (ex. s3, nfs)}\n",
       "\\PY{n+nt}{logging\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{10}\n",
       "\\PY{n+nt}{save\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{500}\n",
       "\\PY{n+nt}{plot\\PYZus{}loss}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{overwrite\\PYZus{}output\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{save\\PYZus{}only\\PYZus{}model}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{false}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} ray}\n",
       "\\PY{n+nt}{ray\\PYZus{}run\\PYZus{}name}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{llama3\\PYZus{}8b\\PYZus{}sft\\PYZus{}lora}\n",
       "\\PY{n+nt}{ray\\PYZus{}storage\\PYZus{}path}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo/saves}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} should be somewhere workers have access to (ex. s3, nfs)}\n",
       "\\PY{n+nt}{ray\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{4}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} number of GPUs to use}\n",
       "\\PY{n+nt}{resources\\PYZus{}per\\PYZus{}worker}\\PY{p}{:}\n",
       "\\PY{+w}{  }\\PY{n+nt}{GPU}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{n+nt}{placement\\PYZus{}strategy}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{PACK}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} train}\n",
       "\\PY{n+nt}{per\\PYZus{}device\\PYZus{}train\\PYZus{}batch\\PYZus{}size}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{n+nt}{gradient\\PYZus{}accumulation\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{8}\n",
       "\\PY{n+nt}{learning\\PYZus{}rate}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1.0e\\PYZhy{}4}\n",
       "\\PY{n+nt}{num\\PYZus{}train\\PYZus{}epochs}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{5.0}\n",
       "\\PY{n+nt}{lr\\PYZus{}scheduler\\PYZus{}type}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{cosine}\n",
       "\\PY{n+nt}{warmup\\PYZus{}ratio}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{0.1}\n",
       "\\PY{n+nt}{bf16}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{ddp\\PYZus{}timeout}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{180000000}\n",
       "\\PY{n+nt}{resume\\PYZus{}from\\PYZus{}checkpoint}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{null}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} eval}\n",
       "\\PY{n+nt}{eval\\PYZus{}dataset}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{viggo\\PYZhy{}val}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} uses same dataset\\PYZus{}dir as training data}\n",
       "\\PY{c+c1}{\\PYZsh{} val\\PYZus{}size: 0.1  \\PYZsh{} only if using part of training data for validation}\n",
       "\\PY{n+nt}{per\\PYZus{}device\\PYZus{}eval\\PYZus{}batch\\PYZus{}size}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{n+nt}{eval\\PYZus{}strategy}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{steps}\n",
       "\\PY{n+nt}{eval\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{500}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "### model\n",
       "model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct\n",
       "trust_remote_code: true\n",
       "\n",
       "### method\n",
       "stage: sft\n",
       "do_train: true\n",
       "finetuning_type: lora\n",
       "lora_rank: 8\n",
       "lora_target: all\n",
       "\n",
       "### dataset\n",
       "dataset: viggo-train\n",
       "dataset_dir: /mnt/cluster_storage/viggo  # shared storage workers have access to\n",
       "template: llama3\n",
       "cutoff_len: 2048\n",
       "max_samples: 1000\n",
       "overwrite_cache: true\n",
       "preprocessing_num_workers: 16\n",
       "dataloader_num_workers: 4\n",
       "\n",
       "### output\n",
       "output_dir: /mnt/cluster_storage/viggo/outputs  # should be somewhere workers have access to (ex. s3, nfs)\n",
       "logging_steps: 10\n",
       "save_steps: 500\n",
       "plot_loss: true\n",
       "overwrite_output_dir: true\n",
       "save_only_model: false\n",
       "\n",
       "### ray\n",
       "ray_run_name: llama3_8b_sft_lora\n",
       "ray_storage_path: /mnt/cluster_storage/viggo/saves  # should be somewhere workers have access to (ex. s3, nfs)\n",
       "ray_num_workers: 4  # number of GPUs to use\n",
       "resources_per_worker:\n",
       "  GPU: 1\n",
       "placement_strategy: PACK\n",
       "\n",
       "### train\n",
       "per_device_train_batch_size: 1\n",
       "gradient_accumulation_steps: 8\n",
       "learning_rate: 1.0e-4\n",
       "num_train_epochs: 5.0\n",
       "lr_scheduler_type: cosine\n",
       "warmup_ratio: 0.1\n",
       "bf16: true\n",
       "ddp_timeout: 180000000\n",
       "resume_from_checkpoint: null\n",
       "\n",
       "### eval\n",
       "eval_dataset: viggo-val  # uses same dataset_dir as training data\n",
       "# val_size: 0.1  # only if using part of training data for validation\n",
       "per_device_eval_batch_size: 1\n",
       "eval_strategy: steps\n",
       "eval_steps: 500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"llama3_lora_sft_ray.yaml\", language=\"yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-06 15:06:44 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 15:06:47,584\tINFO worker.py:1660 -- Connecting to existing Ray cluster at address: 10.0.47.147:6379...\n",
      "2025-04-06 15:06:47,594\tINFO worker.py:1843 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-zt5t77xa58pyp3uy28glg2g24d.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-04-06 15:06:47,596\tINFO packaging.py:367 -- Pushing file package 'gcs://_ray_pkg_26bd71cec9f23efe437dd6a3ea41321d6be9496e.zip' (0.17MiB) to Ray cluster...\n",
      "2025-04-06 15:06:47,597\tINFO packaging.py:380 -- Successfully pushed file package 'gcs://_ray_pkg_26bd71cec9f23efe437dd6a3ea41321d6be9496e.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "View detailed results here: /mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-04-06_14-58-14_355114_2341/artifacts/2025-04-06_15-06-49/llama3_8b_sft_lora/driver_artifacts`\n",
      "\u001b[36m(autoscaler +13s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[36m(autoscaler +13s)\u001b[0m [autoscaler] [4xA10G:48CPU-192GB] Attempting to add 1 node(s) to the cluster (increasing from 0 to 1).\n",
      "\u001b[36m(autoscaler +13s)\u001b[0m [autoscaler] [4xA10G:48CPU-192GB] Launched 1 instances.\n",
      "\n",
      "Training started with configuration:\n",
      "╭───────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training config                                                           │\n",
      "├───────────────────────────────────────────────────────────────────────────┤\n",
      "│ train_loop_config/args/bf16                                          True │\n",
      "│ train_loop_config/args/cutoff_len                                    2048 │\n",
      "│ train_loop_config/args/dataloader_num_workers                           4 │\n",
      "│ train_loop_config/args/dataset                                viggo-train │\n",
      "│ train_loop_config/args/dataset_dir                   ...ter_storage/viggo │\n",
      "│ train_loop_config/args/ddp_timeout                              180000000 │\n",
      "│ train_loop_config/args/do_train                                      True │\n",
      "│ train_loop_config/args/eval_dataset                             viggo-val │\n",
      "│ train_loop_config/args/eval_steps                                     500 │\n",
      "│ train_loop_config/args/eval_strategy                                steps │\n",
      "│ train_loop_config/args/finetuning_type                               lora │\n",
      "│ train_loop_config/args/gradient_accumulation_steps                      8 │\n",
      "│ train_loop_config/args/learning_rate                               0.0001 │\n",
      "│ train_loop_config/args/logging_steps                                   10 │\n",
      "│ train_loop_config/args/lora_rank                                        8 │\n",
      "│ train_loop_config/args/lora_target                                    all │\n",
      "│ train_loop_config/args/lr_scheduler_type                           cosine │\n",
      "│ train_loop_config/args/max_samples                                   1000 │\n",
      "│ train_loop_config/args/model_name_or_path            ...ama-3-8B-Instruct │\n",
      "│ train_loop_config/args/num_train_epochs                               5.0 │\n",
      "│ train_loop_config/args/output_dir                    ...age/viggo/outputs │\n",
      "│ train_loop_config/args/overwrite_cache                               True │\n",
      "│ train_loop_config/args/overwrite_output_dir                          True │\n",
      "│ train_loop_config/args/per_device_eval_batch_size                       1 │\n",
      "│ train_loop_config/args/per_device_train_batch_size                      1 │\n",
      "│ train_loop_config/args/placement_strategy                            PACK │\n",
      "│ train_loop_config/args/plot_loss                                     True │\n",
      "│ train_loop_config/args/preprocessing_num_workers                       16 │\n",
      "│ train_loop_config/args/ray_num_workers                                  4 │\n",
      "│ train_loop_config/args/ray_run_name                    llama3_8b_sft_lora │\n",
      "│ train_loop_config/args/ray_storage_path              ...orage/viggo/saves │\n",
      "│ train_loop_config/args/resources_per_worker/GPU                         1 │\n",
      "│ train_loop_config/args/resume_from_checkpoint                             │\n",
      "│ train_loop_config/args/save_only_model                              False │\n",
      "│ train_loop_config/args/save_steps                                     500 │\n",
      "│ train_loop_config/args/stage                                          sft │\n",
      "│ train_loop_config/args/template                                    llama3 │\n",
      "│ train_loop_config/args/trust_remote_code                             True │\n",
      "│ train_loop_config/args/warmup_ratio                                   0.1 │\n",
      "│ train_loop_config/callbacks                          ... 0x7e0139f07fe0>] │\n",
      "╰───────────────────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(TorchTrainer pid=3322, ip=10.0.144.159)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=3322, ip=10.0.144.159)\u001b[0m - (node_id=439685917749802c5a7b3150eafecacbf785728db0e7e53bcedd6cb0, ip=10.0.144.159, pid=3451) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=3322, ip=10.0.144.159)\u001b[0m - (node_id=439685917749802c5a7b3150eafecacbf785728db0e7e53bcedd6cb0, ip=10.0.144.159, pid=3450) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=3322, ip=10.0.144.159)\u001b[0m - (node_id=439685917749802c5a7b3150eafecacbf785728db0e7e53bcedd6cb0, ip=10.0.144.159, pid=3449) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=3322, ip=10.0.144.159)\u001b[0m - (node_id=439685917749802c5a7b3150eafecacbf785728db0e7e53bcedd6cb0, ip=10.0.144.159, pid=3448) world_rank=3, local_rank=3, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [WARNING|2025-04-06 15:07:59] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:07:59] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:00,502 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:00,502 >> loading file tokenizer.model from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:00,502 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:00,502 >> loading file special_tokens_map.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:00,502 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:00,502 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2323] 2025-04-06 15:08:00,903 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|configuration_utils.py:699] 2025-04-06 15:08:01,720 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|configuration_utils.py:771] 2025-04-06 15:08:01,721 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"transformers_version\": \"4.50.0\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:01,834 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:01,834 >> loading file tokenizer.model from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:01,834 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:01,834 >> loading file special_tokens_map.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:01,834 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2060] 2025-04-06 15:08:01,834 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3448, ip=10.0.144.159)\u001b[0m [rank3]:[W406 15:08:02.323364442 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:08:02] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:08:02] llamafactory.data.template:143 >> Add <|eot_id|>,<|eom_id|> to stop words.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [WARNING|2025-04-06 15:08:02] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:08:02] llamafactory.data.loader:143 >> Loading dataset /mnt/cluster_storage/viggo/train.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2323] 2025-04-06 15:08:02,209 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 10206 examples [00:00, 99768.72 examples/s] \n",
      "Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 5273.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:08:03] llamafactory.data.loader:143 >> Loading dataset /mnt/cluster_storage/viggo/val.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 714 examples [00:00, 99655.02 examples/s]\n",
      "Converting format of dataset (num_proc=16):   0%|          | 0/714 [00:00<?, ? examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 714/714 [00:00<00:00, 4043.50 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:12, 73.41 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  13%|█▎        | 126/1000 [00:00<00:05, 145.72 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  19%|█▉        | 189/1000 [00:01<00:04, 176.48 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  32%|███▏      | 315/1000 [00:01<00:02, 326.03 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  38%|███▊      | 378/1000 [00:01<00:02, 282.21 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  50%|█████     | 504/1000 [00:01<00:01, 410.70 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 566/1000 [00:02<00:01, 337.99 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 690/1000 [00:02<00:00, 454.66 examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [rank0]:[W406 15:08:03.082780279 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 814/1000 [00:02<00:00, 473.09 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 876/1000 [00:02<00:00, 464.13 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 938/1000 [00:02<00:00, 419.00 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:02<00:00, 450.88 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:03<00:00, 330.34 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m training example:\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [128000, 128006, 882, 128007, 271, 22818, 264, 2218, 11914, 9429, 279, 16940, 7438, 13340, 315, 279, 1988, 11914, 439, 264, 3254, 734, 449, 8365, 323, 7180, 2819, 13, 1115, 734, 1288, 7664, 279, 2218, 925, 30357, 323, 279, 734, 2011, 387, 832, 315, 279, 2768, 2570, 41540, 518, 364, 2079, 518, 364, 47530, 10499, 37400, 518, 364, 14119, 518, 364, 12728, 17209, 518, 364, 96861, 518, 364, 2079, 2769, 36990, 518, 364, 67689, 518, 364, 2079, 17209, 7352, 578, 8365, 2011, 387, 832, 315, 279, 2768, 25, 2570, 609, 518, 364, 4683, 25596, 4257, 518, 364, 23859, 14987, 518, 364, 35501, 518, 364, 288, 10910, 518, 364, 22696, 518, 364, 65011, 518, 364, 3517, 623, 86191, 518, 364, 4752, 26190, 3517, 518, 364, 16111, 82, 518, 364, 10547, 4570, 1284, 14922, 518, 364, 4752, 78563, 25596, 518, 364, 4752, 23647, 25596, 518, 364, 68351, 4532, 5028, 39248, 4892, 374, 10213, 459, 17339, 16131, 11, 719, 814, 6004, 74337, 8105, 369, 279, 7553, 323, 779, 430, 41802, 279, 1847, 505, 17339, 311, 1695, 304, 856, 1684, 13, 128009, 128006, 78191, 128007, 271, 47530, 10499, 37400, 3232, 58, 22427, 32560, 8105, 1145, 16131, 58, 5028, 39248, 4892, 1145, 10959, 58, 19045, 1145, 706, 23647, 25596, 58, 9891, 2526, 128009]\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m inputs:\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform','request', 'give_opinion', 'confirm','verify_attribute','suggest','request_explanation','recommend','request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date','release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release','specifier']\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])<|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 47530, 10499, 37400, 3232, 58, 22427, 32560, 8105, 1145, 16131, 58, 5028, 39248, 4892, 1145, 10959, 58, 19045, 1145, 706, 23647, 25596, 58, 9891, 2526, 128009]\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m labels:\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])<|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=3448, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:07:59] llamafactory.hparams.parser:383 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset (num_proc=16):   0%|          | 0/714 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   6%|▋         | 45/714 [00:00<00:13, 50.70 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  13%|█▎        | 90/714 [00:01<00:06, 97.85 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  19%|█▉        | 135/714 [00:01<00:04, 139.95 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  25%|██▌       | 180/714 [00:01<00:02, 182.02 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  32%|███▏      | 225/714 [00:01<00:02, 217.88 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  38%|███▊      | 270/714 [00:01<00:01, 247.04 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  44%|████▍     | 315/714 [00:01<00:01, 267.83 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  50%|█████     | 360/714 [00:01<00:01, 289.30 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 405/714 [00:02<00:01, 302.13 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 450/714 [00:02<00:00, 309.96 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 494/714 [00:02<00:00, 288.40 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 538/714 [00:02<00:00, 317.56 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 626/714 [00:02<00:00, 342.41 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 670/714 [00:02<00:00, 333.15 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 714/714 [00:02<00:00, 340.87 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 714/714 [00:03<00:00, 236.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m eval example:\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [128000, 128006, 882, 128007, 271, 22818, 264, 2218, 11914, 9429, 279, 16940, 7438, 13340, 315, 279, 1988, 11914, 439, 264, 3254, 734, 449, 8365, 323, 7180, 2819, 13, 1115, 734, 1288, 7664, 279, 2218, 925, 30357, 323, 279, 734, 2011, 387, 832, 315, 279, 2768, 2570, 41540, 518, 364, 2079, 518, 364, 47530, 10499, 37400, 518, 364, 14119, 518, 364, 12728, 17209, 518, 364, 96861, 518, 364, 2079, 2769, 36990, 518, 364, 67689, 518, 364, 2079, 17209, 7352, 578, 8365, 2011, 387, 832, 315, 279, 2768, 25, 2570, 609, 518, 364, 4683, 25596, 4257, 518, 364, 23859, 14987, 518, 364, 35501, 518, 364, 288, 10910, 518, 364, 22696, 518, 364, 65011, 518, 364, 3517, 623, 86191, 518, 364, 4752, 26190, 3517, 518, 364, 16111, 82, 518, 364, 10547, 4570, 1284, 14922, 518, 364, 4752, 78563, 25596, 518, 364, 4752, 23647, 25596, 518, 364, 68351, 4532, 30128, 19085, 220, 18, 374, 264, 5128, 3958, 1847, 13, 578, 16131, 74404, 20763, 11871, 374, 9539, 264, 15860, 315, 912, 2442, 69261, 60884, 11, 323, 220, 679, 22, 574, 264, 17936, 1060, 369, 3953, 13971, 13, 128009, 128006, 78191, 128007, 271, 47530, 10499, 37400, 3232, 58, 30128, 19085, 220, 18, 1145, 4984, 14987, 58, 679, 22, 1145, 16131, 73183, 6417, 20763, 11871, 1145, 10959, 58, 5481, 269, 2526, 128009]\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m inputs:\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform','request', 'give_opinion', 'confirm','verify_attribute','suggest','request_explanation','recommend','request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date','release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release','specifier']\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m SpellForce 3 is a pretty bad game. The developer Grimlore Games is clearly a bunch of no-talent hacks, and 2017 was a terrible year for games anyway.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m give_opinion(name[SpellForce 3], release_year[2017], developer[Grimlore Games], rating[poor])<|eot_id|>\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 47530, 10499, 37400, 3232, 58, 30128, 19085, 220, 18, 1145, 4984, 14987, 58, 679, 22, 1145, 16131, 73183, 6417, 20763, 11871, 1145, 10959, 58, 5481, 269, 2526, 128009]\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m labels:\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m give_opinion(name[SpellForce 3], release_year[2017], developer[Grimlore Games], rating[poor])<|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|configuration_utils.py:699] 2025-04-06 15:08:11,967 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|configuration_utils.py:771] 2025-04-06 15:08:11,968 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"transformers_version\": \"4.50.0\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|modeling_utils.py:1154] 2025-04-06 15:08:12,718 >> loading weights file model.safetensors from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/model.safetensors.index.json\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|modeling_utils.py:2170] 2025-04-06 15:08:31,221 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|configuration_utils.py:1139] 2025-04-06 15:08:31,223 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"eos_token_id\": 128009\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|modeling_utils.py:4987] 2025-04-06 15:08:34,642 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|modeling_utils.py:4995] 2025-04-06 15:08:34,642 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:08:34] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:08:34] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:08:34] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:08:34] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:08:34] llamafactory.model.model_utils.misc:143 >> Found linear modules: down_proj,o_proj,up_proj,v_proj,q_proj,gate_proj,k_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|configuration_utils.py:1094] 2025-04-06 15:08:34,822 >> loading configuration file generation_config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/generation_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|configuration_utils.py:1139] 2025-04-06 15:08:34,822 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"do_sample\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"eos_token_id\": [\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m     128001,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m     128009\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"max_length\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"temperature\": 0.6,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"top_p\": 0.9\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|2025-04-06 15:08:35] llamafactory.model.loader:143 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:748] 2025-04-06 15:08:35,240 >> Using auto half precision backend\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [WARNING|trainer.py:783] 2025-04-06 15:08:35,241 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[36m(RayTrainWorker pid=3448, ip=10.0.144.159)\u001b[0m No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:2409] 2025-04-06 15:08:41,752 >> ***** Running training *****\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:2410] 2025-04-06 15:08:41,752 >>   Num examples = 1,000\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:2411] 2025-04-06 15:08:41,752 >>   Num Epochs = 5\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:2412] 2025-04-06 15:08:41,752 >>   Instantaneous batch size per device = 1\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:2415] 2025-04-06 15:08:41,752 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:2416] 2025-04-06 15:08:41,752 >>   Gradient Accumulation steps = 8\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:2417] 2025-04-06 15:08:41,752 >>   Total optimization steps = 155\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:2418] 2025-04-06 15:08:41,756 >>   Number of trainable parameters = 20,971,520\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3450, ip=10.0.144.159)\u001b[0m No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "  0%|          | 0/155 [00:00<?, ?it/s]144.159)\u001b[0m \n",
      "  1%|          | 1/155 [00:03<09:52,  3.85s/it]\u001b[0m \n",
      "  1%|▏         | 2/155 [00:06<08:06,  3.18s/it]\u001b[0m \n",
      "  2%|▏         | 3/155 [00:09<07:35,  3.00s/it]\u001b[0m \n",
      "  3%|▎         | 4/155 [00:12<07:20,  2.92s/it]\u001b[0m \n",
      "  3%|▎         | 5/155 [00:14<07:01,  2.81s/it]\u001b[0m \n",
      "  4%|▍         | 6/155 [00:17<07:02,  2.83s/it]\u001b[0m \n",
      "  5%|▍         | 7/155 [00:20<06:48,  2.76s/it]\u001b[0m \n",
      "  5%|▌         | 8/155 [00:23<06:52,  2.81s/it]\u001b[0m \n",
      "  6%|▌         | 9/155 [00:25<06:51,  2.82s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 1.9137, 'grad_norm': 3.7507615089416504, 'learning_rate': 6.25e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 10/155 [00:28<06:51,  2.84s/it][0m \n",
      "  7%|▋         | 11/155 [00:31<06:42,  2.80s/it][0m \n",
      "  8%|▊         | 12/155 [00:34<06:43,  2.82s/it][0m \n",
      "  8%|▊         | 13/155 [00:37<06:43,  2.84s/it][0m \n",
      "  9%|▉         | 14/155 [00:40<06:40,  2.84s/it][0m \n",
      " 10%|▉         | 15/155 [00:42<06:30,  2.79s/it][0m \n",
      " 10%|█         | 16/155 [00:45<06:32,  2.83s/it][0m \n",
      " 11%|█         | 17/155 [00:48<06:23,  2.78s/it][0m \n",
      " 12%|█▏        | 18/155 [00:51<06:15,  2.74s/it][0m \n",
      " 12%|█▏        | 19/155 [00:53<06:15,  2.76s/it][0m \n",
      " 13%|█▎        | 20/155 [00:56<06:24,  2.85s/it][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.5527, 'grad_norm': 1.3114871978759766, 'learning_rate': 9.979581007037776e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 20/155 [00:56<06:24,  2.85s/it][0m \n",
      " 14%|█▎        | 21/155 [00:59<06:26,  2.88s/it][0m \n",
      " 14%|█▍        | 22/155 [01:02<06:22,  2.88s/it][0m \n",
      " 15%|█▍        | 23/155 [01:05<06:16,  2.85s/it][0m \n",
      " 15%|█▌        | 24/155 [01:08<06:14,  2.86s/it][0m \n",
      " 16%|█▌        | 25/155 [01:11<06:11,  2.86s/it][0m \n",
      " 17%|█▋        | 26/155 [01:14<06:08,  2.86s/it][0m \n",
      " 17%|█▋        | 27/155 [01:16<05:57,  2.80s/it][0m \n",
      " 18%|█▊        | 28/155 [01:19<05:49,  2.75s/it][0m \n",
      " 19%|█▊        | 29/155 [01:22<05:43,  2.72s/it][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.1674, 'grad_norm': 0.7684409022331238, 'learning_rate': 9.751778332739033e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 30/155 [01:24<05:45,  2.76s/it][0m \n",
      " 20%|██        | 31/155 [01:27<05:39,  2.74s/it][0m \n",
      " 21%|██        | 32/155 [01:28<04:32,  2.22s/it][0m \n",
      " 21%|██▏       | 33/155 [01:31<05:03,  2.49s/it][0m \n",
      " 22%|██▏       | 34/155 [01:34<05:07,  2.54s/it][0m \n",
      " 23%|██▎       | 35/155 [01:37<05:17,  2.64s/it][0m \n",
      " 23%|██▎       | 36/155 [01:40<05:20,  2.69s/it][0m \n",
      " 24%|██▍       | 37/155 [01:42<05:16,  2.69s/it][0m \n",
      " 25%|██▍       | 38/155 [01:45<05:14,  2.69s/it][0m \n",
      " 25%|██▌       | 39/155 [01:48<05:14,  2.71s/it][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0672, 'grad_norm': 0.6747345328330994, 'learning_rate': 9.282275574435281e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 40/155 [01:51<05:15,  2.74s/it][0m \n",
      " 26%|██▋       | 41/155 [01:53<05:15,  2.76s/it][0m \n",
      " 27%|██▋       | 42/155 [01:56<05:16,  2.80s/it][0m \n",
      " 28%|██▊       | 43/155 [01:59<05:08,  2.75s/it][0m \n",
      " 28%|██▊       | 44/155 [02:02<05:02,  2.72s/it][0m \n",
      " 29%|██▉       | 45/155 [02:05<05:10,  2.82s/it][0m \n",
      " 30%|██▉       | 46/155 [02:07<05:02,  2.78s/it][0m \n",
      " 30%|███       | 47/155 [02:10<05:00,  2.78s/it][0m \n",
      " 31%|███       | 48/155 [02:13<04:58,  2.79s/it][0m \n",
      " 32%|███▏      | 49/155 [02:16<04:56,  2.79s/it][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0435, 'grad_norm': 0.8540425896644592, 'learning_rate': 8.594954076788736e-05, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 50/155 [02:18<04:48,  2.75s/it][0m \n",
      " 33%|███▎      | 51/155 [02:21<04:41,  2.71s/it][0m \n",
      " 34%|███▎      | 52/155 [02:24<04:43,  2.75s/it][0m \n",
      " 34%|███▍      | 53/155 [02:27<04:43,  2.77s/it][0m \n",
      " 35%|███▍      | 54/155 [02:30<04:43,  2.81s/it][0m \n",
      " 35%|███▌      | 55/155 [02:33<04:47,  2.88s/it][0m \n",
      " 36%|███▌      | 56/155 [02:35<04:39,  2.82s/it][0m \n",
      " 37%|███▋      | 57/155 [02:38<04:36,  2.82s/it][0m \n",
      " 37%|███▋      | 58/155 [02:41<04:35,  2.84s/it][0m \n",
      " 38%|███▊      | 59/155 [02:44<04:31,  2.83s/it][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0286, 'grad_norm': 0.6202750205993652, 'learning_rate': 7.724774574936188e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 60/155 [02:46<04:24,  2.78s/it][0m \n",
      " 39%|███▉      | 61/155 [02:49<04:25,  2.82s/it][0m \n",
      " 40%|████      | 62/155 [02:52<04:19,  2.79s/it][0m \n",
      " 41%|████      | 63/155 [02:55<04:17,  2.80s/it][0m \n",
      " 41%|████▏     | 64/155 [02:56<03:25,  2.26s/it][0m \n",
      " 42%|████▏     | 65/155 [02:59<03:40,  2.45s/it][0m \n",
      " 43%|████▎     | 66/155 [03:02<03:49,  2.58s/it][0m \n",
      " 43%|████▎     | 67/155 [03:05<03:55,  2.67s/it][0m \n",
      " 44%|████▍     | 68/155 [03:07<03:56,  2.72s/it][0m \n",
      " 45%|████▍     | 69/155 [03:10<03:58,  2.77s/it][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0143, 'grad_norm': 0.5829706788063049, 'learning_rate': 6.715998910228296e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 70/155 [03:13<03:57,  2.79s/it][0m \n",
      " 46%|████▌     | 71/155 [03:16<03:55,  2.81s/it][0m \n",
      " 46%|████▋     | 72/155 [03:19<03:54,  2.82s/it][0m \n",
      " 47%|████▋     | 73/155 [03:22<03:52,  2.83s/it][0m \n",
      " 48%|████▊     | 74/155 [03:25<03:50,  2.85s/it][0m \n",
      " 48%|████▊     | 75/155 [03:27<03:46,  2.83s/it][0m \n",
      " 49%|████▉     | 76/155 [03:30<03:40,  2.79s/it][0m \n",
      " 50%|████▉     | 77/155 [03:33<03:40,  2.83s/it][0m \n",
      " 50%|█████     | 78/155 [03:36<03:33,  2.77s/it][0m \n",
      " 51%|█████     | 79/155 [03:38<03:28,  2.74s/it][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0094, 'grad_norm': 0.6517273187637329, 'learning_rate': 5.619938643480561e-05, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 80/155 [03:41<03:27,  2.76s/it][0m \n",
      " 52%|█████▏    | 81/155 [03:44<03:22,  2.74s/it][0m \n",
      " 53%|█████▎    | 82/155 [03:46<03:19,  2.73s/it][0m \n",
      " 54%|█████▎    | 83/155 [03:49<03:14,  2.70s/it][0m \n",
      " 54%|█████▍    | 84/155 [03:52<03:20,  2.83s/it][0m \n",
      " 55%|█████▍    | 85/155 [03:55<03:14,  2.78s/it][0m \n",
      " 55%|█████▌    | 86/155 [03:58<03:09,  2.75s/it][0m \n",
      " 56%|█████▌    | 87/155 [04:00<03:05,  2.73s/it][0m \n",
      " 57%|█████▋    | 88/155 [04:03<03:01,  2.70s/it][0m \n",
      " 57%|█████▋    | 89/155 [04:06<03:01,  2.76s/it][0m \n",
      " 58%|█████▊    | 90/155 [04:09<03:05,  2.86s/it][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0137, 'grad_norm': 0.3563719689846039, 'learning_rate': 4.4923450829394605e-05, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 90/155 [04:09<03:05,  2.86s/it][0m \n",
      " 59%|█████▊    | 91/155 [04:12<03:06,  2.91s/it][0m \n",
      " 59%|█████▉    | 92/155 [04:15<02:58,  2.83s/it][0m \n",
      " 60%|██████    | 93/155 [04:17<02:55,  2.83s/it][0m \n",
      " 61%|██████    | 94/155 [04:20<02:55,  2.88s/it][0m \n",
      " 61%|██████▏   | 95/155 [04:23<02:49,  2.83s/it][0m \n",
      " 62%|██████▏   | 96/155 [04:24<02:11,  2.22s/it][0m \n",
      " 63%|██████▎   | 97/155 [04:27<02:23,  2.48s/it][0m \n",
      " 63%|██████▎   | 98/155 [04:30<02:27,  2.59s/it][0m \n",
      " 64%|██████▍   | 99/155 [04:33<02:29,  2.67s/it][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0054, 'grad_norm': 0.2933689057826996, 'learning_rate': 3.390573483674142e-05, 'epoch': 3.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 100/155 [04:36<02:30,  2.74s/it]0m \n",
      " 65%|██████▌   | 101/155 [04:38<02:29,  2.77s/it]0m \n",
      " 66%|██████▌   | 102/155 [04:41<02:24,  2.73s/it]0m \n",
      " 66%|██████▋   | 103/155 [04:44<02:20,  2.71s/it]0m \n",
      " 67%|██████▋   | 104/155 [04:47<02:20,  2.75s/it]0m \n",
      " 68%|██████▊   | 105/155 [04:49<02:16,  2.72s/it]0m \n",
      " 68%|██████▊   | 106/155 [04:52<02:14,  2.75s/it]0m \n",
      " 69%|██████▉   | 107/155 [04:55<02:13,  2.79s/it]0m \n",
      " 70%|██████▉   | 108/155 [04:58<02:10,  2.78s/it]0m \n",
      " 70%|███████   | 109/155 [05:01<02:09,  2.81s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0043, 'grad_norm': 0.2936941087245941, 'learning_rate': 2.3706656619162278e-05, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 110/155 [05:03<02:06,  2.81s/it]0m \n",
      " 72%|███████▏  | 111/155 [05:06<02:03,  2.81s/it]0m \n",
      " 72%|███████▏  | 112/155 [05:09<01:59,  2.77s/it]0m \n",
      " 73%|███████▎  | 113/155 [05:12<01:54,  2.73s/it]0m \n",
      " 74%|███████▎  | 114/155 [05:14<01:53,  2.78s/it]0m \n",
      " 74%|███████▍  | 115/155 [05:17<01:49,  2.74s/it]0m \n",
      " 75%|███████▍  | 116/155 [05:20<01:47,  2.76s/it]0m \n",
      " 75%|███████▌  | 117/155 [05:22<01:43,  2.71s/it]0m \n",
      " 76%|███████▌  | 118/155 [05:25<01:43,  2.80s/it]0m \n",
      " 77%|███████▋  | 119/155 [05:28<01:39,  2.76s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0047, 'grad_norm': 0.19297488033771515, 'learning_rate': 1.484499417709087e-05, 'epoch': 3.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 120/155 [05:31<01:38,  2.82s/it]0m \n",
      " 78%|███████▊  | 121/155 [05:34<01:34,  2.78s/it]0m \n",
      " 79%|███████▊  | 122/155 [05:37<01:32,  2.81s/it]0m \n",
      " 79%|███████▉  | 123/155 [05:40<01:30,  2.84s/it]0m \n",
      " 80%|████████  | 124/155 [05:42<01:27,  2.84s/it]0m \n",
      " 81%|████████  | 125/155 [05:45<01:27,  2.90s/it]0m \n",
      " 81%|████████▏ | 126/155 [05:48<01:21,  2.83s/it]0m \n",
      " 82%|████████▏ | 127/155 [05:51<01:19,  2.82s/it]0m \n",
      " 83%|████████▎ | 128/155 [05:52<00:59,  2.22s/it]0m \n",
      " 83%|████████▎ | 129/155 [05:55<01:04,  2.48s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0049, 'grad_norm': 0.05336451530456543, 'learning_rate': 7.77149761010898e-06, 'epoch': 4.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 130/155 [05:58<01:04,  2.58s/it]0m \n",
      " 85%|████████▍ | 131/155 [06:00<01:03,  2.66s/it]0m \n",
      " 85%|████████▌ | 132/155 [06:03<01:02,  2.71s/it]0m \n",
      " 86%|████████▌ | 133/155 [06:06<01:00,  2.73s/it]0m \n",
      " 86%|████████▋ | 134/155 [06:09<00:57,  2.75s/it]0m \n",
      " 87%|████████▋ | 135/155 [06:11<00:54,  2.71s/it]0m \n",
      " 88%|████████▊ | 136/155 [06:14<00:52,  2.75s/it]0m \n",
      " 88%|████████▊ | 137/155 [06:17<00:50,  2.78s/it]0m \n",
      " 89%|████████▉ | 138/155 [06:20<00:47,  2.79s/it]0m \n",
      " 90%|████████▉ | 139/155 [06:23<00:45,  2.85s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0014, 'grad_norm': 0.08628690987825394, 'learning_rate': 2.8459616297395466e-06, 'epoch': 4.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 140/155 [06:26<00:42,  2.87s/it]0m \n",
      " 91%|█████████ | 141/155 [06:29<00:41,  2.93s/it]0m \n",
      " 92%|█████████▏| 142/155 [06:32<00:37,  2.91s/it]0m \n",
      " 92%|█████████▏| 143/155 [06:34<00:33,  2.82s/it]0m \n",
      " 93%|█████████▎| 144/155 [06:37<00:31,  2.83s/it]0m \n",
      " 94%|█████████▎| 145/155 [06:40<00:27,  2.77s/it]0m \n",
      " 94%|█████████▍| 146/155 [06:43<00:25,  2.79s/it]0m \n",
      " 95%|█████████▍| 147/155 [06:46<00:22,  2.80s/it]0m \n",
      " 95%|█████████▌| 148/155 [06:48<00:19,  2.83s/it]0m \n",
      " 96%|█████████▌| 149/155 [06:51<00:17,  2.84s/it]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'loss': 0.0026, 'grad_norm': 0.15772362053394318, 'learning_rate': 3.1892453488058803e-07, 'epoch': 4.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 150/155 [06:54<00:14,  2.85s/it]0m \n",
      " 97%|█████████▋| 151/155 [06:57<00:11,  2.80s/it]0m \n",
      " 98%|█████████▊| 152/155 [07:00<00:08,  2.81s/it]0m \n",
      " 99%|█████████▊| 153/155 [07:03<00:05,  2.80s/it]0m \n",
      " 99%|█████████▉| 154/155 [07:05<00:02,  2.81s/it]0m \n",
      "100%|██████████| 155/155 [07:08<00:00,  2.81s/it][INFO|trainer.py:3966] 2025-04-06 15:15:51,023 >> Saving model checkpoint to /mnt/cluster_storage/viggo/outputs/checkpoint-155\n",
      "\u001b[36m(RayTrainWorker pid=3448, ip=10.0.144.159)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_6ffa1_00000_0_2025-04-06_15-06-49/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|configuration_utils.py:699] 2025-04-06 15:15:51,327 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|configuration_utils.py:771] 2025-04-06 15:15:51,328 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"transformers_version\": \"4.50.0\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2510] 2025-04-06 15:15:51,722 >> tokenizer config file saved in /mnt/cluster_storage/viggo/outputs/checkpoint-155/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2519] 2025-04-06 15:15:51,731 >> Special tokens file saved in /mnt/cluster_storage/viggo/outputs/checkpoint-155/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 1 at 2025-04-06 15:15:54. Total running time: 9min 5s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000000 │\n",
      "│ time_this_iter_s              482.24643 │\n",
      "│ time_total_s                  482.24643 │\n",
      "│ training_iteration                    1 │\n",
      "│ epoch                             4.704 │\n",
      "│ grad_norm                       0.15772 │\n",
      "│ learning_rate                        0. │\n",
      "│ loss                             0.0026 │\n",
      "│ step                                150 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 1 at: (local)/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_6ffa1_00000_0_2025-04-06_15-06-49/checkpoint_000000\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'train_runtime': 432.7067, 'train_samples_per_second': 11.555, 'train_steps_per_second': 0.358, 'train_loss': 0.1828844992744346, 'epoch': 4.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:2665] 2025-04-06 15:15:54,462 >> \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "100%|██████████| 155/155 [07:12<00:00,  2.79s/it]0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:3966] 2025-04-06 15:15:54,595 >> Saving model checkpoint to /mnt/cluster_storage/viggo/outputs\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|configuration_utils.py:699] 2025-04-06 15:15:54,883 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|configuration_utils.py:771] 2025-04-06 15:15:54,884 >> Model config LlamaConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m     \"LlamaForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"attention_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"bos_token_id\": 128000,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"eos_token_id\": 128009,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"head_dim\": 128,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"hidden_size\": 4096,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"intermediate_size\": 14336,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"max_position_embeddings\": 8192,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"mlp_bias\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"model_type\": \"llama\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_attention_heads\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_hidden_layers\": 32,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"num_key_value_heads\": 8,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"pretraining_tp\": 1,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rms_norm_eps\": 1e-05,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"rope_theta\": 500000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"transformers_version\": \"4.50.0\",\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   \"vocab_size\": 128256\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2510] 2025-04-06 15:15:55,258 >> tokenizer config file saved in /mnt/cluster_storage/viggo/outputs/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|tokenization_utils_base.py:2519] 2025-04-06 15:15:55,266 >> Special tokens file saved in /mnt/cluster_storage/viggo/outputs/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m ***** train metrics *****\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   epoch                    =      4.864\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   total_flos               = 43426535GF\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   train_loss               =     0.1829\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   train_runtime            = 0:07:12.70\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   train_samples_per_second =     11.555\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   train_steps_per_second   =      0.358\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m Figure saved at: /mnt/cluster_storage/viggo/outputs/training_loss.png\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [WARNING|2025-04-06 15:15:55] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [WARNING|2025-04-06 15:15:55] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:4289] 2025-04-06 15:15:55,709 >> \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m ***** Running Evaluation *****\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:4291] 2025-04-06 15:15:55,709 >>   Num examples = 714\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|trainer.py:4294] 2025-04-06 15:15:55,710 >>   Batch size = 1\n",
      "  0%|          | 0/179 [00:00<?, ?it/s]144.159)\u001b[0m \n",
      "  1%|          | 2/179 [00:00<00:09, 18.33it/s]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_6ffa1_00000_0_2025-04-06_15-06-49/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "  2%|▏         | 4/179 [00:00<00:14, 11.93it/s]\u001b[0m \n",
      "  3%|▎         | 6/179 [00:00<00:16, 10.70it/s]\u001b[0m \n",
      "  4%|▍         | 8/179 [00:00<00:16, 10.26it/s]\u001b[0m \n",
      "  6%|▌         | 10/179 [00:00<00:16,  9.99it/s][0m \n",
      "  7%|▋         | 12/179 [00:01<00:16,  9.85it/s][0m \n",
      "  8%|▊         | 14/179 [00:01<00:16,  9.77it/s][0m \n",
      "  8%|▊         | 15/179 [00:01<00:16,  9.72it/s][0m \n",
      "  9%|▉         | 16/179 [00:01<00:16,  9.72it/s][0m \n",
      "  9%|▉         | 17/179 [00:01<00:16,  9.72it/s][0m \n",
      " 10%|█         | 18/179 [00:01<00:16,  9.65it/s][0m \n",
      " 11%|█         | 19/179 [00:01<00:16,  9.64it/s][0m \n",
      " 11%|█         | 20/179 [00:01<00:16,  9.62it/s][0m \n",
      " 12%|█▏        | 21/179 [00:02<00:16,  9.53it/s][0m \n",
      " 13%|█▎        | 23/179 [00:02<00:15,  9.83it/s][0m \n",
      " 13%|█▎        | 24/179 [00:02<00:18,  8.54it/s][0m \n",
      " 14%|█▍        | 25/179 [00:02<00:17,  8.77it/s][0m \n",
      " 15%|█▍        | 26/179 [00:02<00:17,  8.97it/s][0m \n",
      " 15%|█▌        | 27/179 [00:02<00:19,  7.89it/s][0m \n",
      " 16%|█▌        | 28/179 [00:02<00:20,  7.22it/s][0m \n",
      " 16%|█▌        | 29/179 [00:03<00:19,  7.77it/s][0m \n",
      " 17%|█▋        | 31/179 [00:03<00:19,  7.71it/s][0m \n",
      " 18%|█▊        | 33/179 [00:03<00:17,  8.55it/s][0m \n",
      " 19%|█▉        | 34/179 [00:03<00:16,  8.77it/s][0m \n",
      " 20%|█▉        | 35/179 [00:03<00:16,  8.98it/s][0m \n",
      " 20%|██        | 36/179 [00:03<00:15,  9.13it/s][0m \n",
      " 21%|██        | 37/179 [00:03<00:15,  9.25it/s][0m \n",
      " 21%|██        | 38/179 [00:04<00:15,  9.36it/s][0m \n",
      " 22%|██▏       | 39/179 [00:04<00:14,  9.41it/s][0m \n",
      " 22%|██▏       | 40/179 [00:04<00:14,  9.45it/s][0m \n",
      " 23%|██▎       | 41/179 [00:04<00:14,  9.49it/s][0m \n",
      " 23%|██▎       | 42/179 [00:04<00:14,  9.52it/s][0m \n",
      " 24%|██▍       | 43/179 [00:04<00:14,  9.51it/s][0m \n",
      " 25%|██▍       | 44/179 [00:04<00:14,  9.55it/s][0m \n",
      " 25%|██▌       | 45/179 [00:04<00:13,  9.58it/s][0m \n",
      " 26%|██▌       | 46/179 [00:04<00:13,  9.57it/s][0m \n",
      " 26%|██▋       | 47/179 [00:05<00:13,  9.52it/s][0m \n",
      " 27%|██▋       | 48/179 [00:05<00:13,  9.45it/s][0m \n",
      " 27%|██▋       | 49/179 [00:05<00:13,  9.46it/s][0m \n",
      " 28%|██▊       | 51/179 [00:05<00:13,  9.80it/s][0m \n",
      " 29%|██▉       | 52/179 [00:05<00:12,  9.78it/s][0m \n",
      " 30%|██▉       | 53/179 [00:05<00:12,  9.71it/s][0m \n",
      " 30%|███       | 54/179 [00:05<00:12,  9.66it/s][0m \n",
      " 31%|███       | 55/179 [00:05<00:13,  9.53it/s][0m \n",
      " 31%|███▏      | 56/179 [00:05<00:12,  9.54it/s][0m \n",
      " 32%|███▏      | 57/179 [00:06<00:12,  9.53it/s][0m \n",
      " 32%|███▏      | 58/179 [00:06<00:12,  9.53it/s][0m \n",
      " 33%|███▎      | 59/179 [00:06<00:12,  9.56it/s][0m \n",
      " 34%|███▎      | 60/179 [00:06<00:12,  9.59it/s][0m \n",
      " 34%|███▍      | 61/179 [00:06<00:12,  9.57it/s][0m \n",
      " 35%|███▍      | 62/179 [00:06<00:12,  9.58it/s][0m \n",
      " 35%|███▌      | 63/179 [00:06<00:12,  9.60it/s][0m \n",
      " 36%|███▌      | 64/179 [00:06<00:11,  9.61it/s][0m \n",
      " 36%|███▋      | 65/179 [00:06<00:11,  9.60it/s][0m \n",
      " 37%|███▋      | 66/179 [00:07<00:11,  9.51it/s][0m \n",
      " 37%|███▋      | 67/179 [00:07<00:13,  8.11it/s][0m \n",
      " 38%|███▊      | 68/179 [00:07<00:15,  7.34it/s][0m \n",
      " 39%|███▊      | 69/179 [00:07<00:13,  7.88it/s][0m \n",
      " 39%|███▉      | 70/179 [00:07<00:13,  8.30it/s][0m \n",
      " 40%|███▉      | 71/179 [00:07<00:12,  8.65it/s][0m \n",
      " 40%|████      | 72/179 [00:07<00:12,  8.91it/s][0m \n",
      " 41%|████      | 73/179 [00:07<00:11,  9.11it/s][0m \n",
      " 41%|████▏     | 74/179 [00:07<00:11,  9.19it/s][0m \n",
      " 42%|████▏     | 75/179 [00:08<00:11,  9.27it/s][0m \n",
      " 42%|████▏     | 76/179 [00:08<00:10,  9.40it/s][0m \n",
      " 44%|████▎     | 78/179 [00:08<00:10,  9.73it/s][0m \n",
      " 44%|████▍     | 79/179 [00:08<00:10,  9.62it/s][0m \n",
      " 45%|████▍     | 80/179 [00:08<00:10,  9.60it/s][0m \n",
      " 45%|████▌     | 81/179 [00:08<00:10,  9.57it/s][0m \n",
      " 46%|████▌     | 82/179 [00:08<00:10,  9.59it/s][0m \n",
      " 46%|████▋     | 83/179 [00:08<00:10,  9.57it/s][0m \n",
      " 47%|████▋     | 84/179 [00:08<00:09,  9.56it/s][0m \n",
      " 47%|████▋     | 85/179 [00:09<00:09,  9.45it/s][0m \n",
      " 48%|████▊     | 86/179 [00:09<00:09,  9.38it/s][0m \n",
      " 49%|████▉     | 88/179 [00:09<00:09,  9.67it/s][0m \n",
      " 50%|████▉     | 89/179 [00:09<00:09,  9.63it/s][0m \n",
      " 50%|█████     | 90/179 [00:09<00:09,  9.61it/s][0m \n",
      " 51%|█████     | 91/179 [00:09<00:09,  9.62it/s][0m \n",
      " 51%|█████▏    | 92/179 [00:09<00:09,  9.63it/s][0m \n",
      " 52%|█████▏    | 93/179 [00:09<00:08,  9.62it/s][0m \n",
      " 53%|█████▎    | 94/179 [00:10<00:08,  9.61it/s][0m \n",
      " 53%|█████▎    | 95/179 [00:10<00:08,  9.60it/s][0m \n",
      " 54%|█████▎    | 96/179 [00:10<00:08,  9.59it/s][0m \n",
      " 54%|█████▍    | 97/179 [00:10<00:08,  9.62it/s][0m \n",
      " 55%|█████▍    | 98/179 [00:10<00:08,  9.62it/s][0m \n",
      " 55%|█████▌    | 99/179 [00:10<00:08,  9.62it/s][0m \n",
      " 56%|█████▌    | 100/179 [00:10<00:08,  9.58it/s]0m \n",
      " 56%|█████▋    | 101/179 [00:10<00:08,  9.59it/s]0m \n",
      " 58%|█████▊    | 103/179 [00:10<00:07,  9.81it/s]0m \n",
      " 58%|█████▊    | 104/179 [00:11<00:07,  9.71it/s]0m \n",
      " 59%|█████▊    | 105/179 [00:11<00:07,  9.67it/s]0m \n",
      " 59%|█████▉    | 106/179 [00:11<00:07,  9.63it/s]0m \n",
      " 60%|█████▉    | 107/179 [00:11<00:07,  9.62it/s]0m \n",
      " 60%|██████    | 108/179 [00:11<00:07,  9.61it/s]0m \n",
      " 61%|██████    | 109/179 [00:11<00:07,  9.60it/s]0m \n",
      " 62%|██████▏   | 111/179 [00:11<00:06,  9.82it/s]0m \n",
      " 63%|██████▎   | 112/179 [00:11<00:06,  9.78it/s]0m \n",
      " 63%|██████▎   | 113/179 [00:12<00:06,  9.71it/s]0m \n",
      " 64%|██████▎   | 114/179 [00:12<00:06,  9.67it/s]0m \n",
      " 64%|██████▍   | 115/179 [00:12<00:06,  9.60it/s]0m \n",
      " 65%|██████▍   | 116/179 [00:12<00:06,  9.62it/s]0m \n",
      " 65%|██████▌   | 117/179 [00:12<00:06,  9.64it/s]0m \n",
      " 66%|██████▌   | 118/179 [00:12<00:06,  9.63it/s]0m \n",
      " 66%|██████▋   | 119/179 [00:12<00:06,  9.67it/s]0m \n",
      " 67%|██████▋   | 120/179 [00:12<00:06,  9.68it/s]0m \n",
      " 68%|██████▊   | 121/179 [00:12<00:07,  8.21it/s]0m \n",
      " 68%|██████▊   | 122/179 [00:13<00:06,  8.56it/s]0m \n",
      " 69%|██████▊   | 123/179 [00:13<00:06,  8.85it/s]0m \n",
      " 69%|██████▉   | 124/179 [00:13<00:06,  9.03it/s]0m \n",
      " 70%|██████▉   | 125/179 [00:13<00:05,  9.16it/s]0m \n",
      " 70%|███████   | 126/179 [00:13<00:05,  9.19it/s]0m \n",
      " 71%|███████   | 127/179 [00:13<00:05,  9.32it/s]0m \n",
      " 72%|███████▏  | 128/179 [00:13<00:06,  7.99it/s]0m \n",
      " 72%|███████▏  | 129/179 [00:13<00:06,  8.29it/s]0m \n",
      " 73%|███████▎  | 130/179 [00:13<00:05,  8.27it/s]0m \n",
      " 73%|███████▎  | 131/179 [00:14<00:05,  8.61it/s]0m \n",
      " 74%|███████▎  | 132/179 [00:14<00:05,  8.91it/s]0m \n",
      " 74%|███████▍  | 133/179 [00:14<00:05,  9.09it/s]0m \n",
      " 75%|███████▍  | 134/179 [00:14<00:04,  9.22it/s]0m \n",
      " 75%|███████▌  | 135/179 [00:14<00:04,  9.32it/s]0m \n",
      " 76%|███████▌  | 136/179 [00:14<00:05,  8.00it/s]0m \n",
      " 77%|███████▋  | 137/179 [00:14<00:04,  8.45it/s]0m \n",
      " 77%|███████▋  | 138/179 [00:14<00:04,  8.79it/s]0m \n",
      " 78%|███████▊  | 139/179 [00:14<00:04,  9.00it/s]0m \n",
      " 78%|███████▊  | 140/179 [00:15<00:04,  9.16it/s]0m \n",
      " 79%|███████▉  | 141/179 [00:15<00:04,  9.27it/s]0m \n",
      " 79%|███████▉  | 142/179 [00:15<00:03,  9.33it/s]0m \n",
      " 80%|███████▉  | 143/179 [00:15<00:03,  9.43it/s]0m \n",
      " 80%|████████  | 144/179 [00:15<00:03,  9.45it/s]0m \n",
      " 81%|████████  | 145/179 [00:15<00:03,  9.39it/s]0m \n",
      " 82%|████████▏ | 146/179 [00:15<00:03,  9.34it/s]0m \n",
      " 82%|████████▏ | 147/179 [00:15<00:03,  9.39it/s]0m \n",
      " 83%|████████▎ | 148/179 [00:15<00:03,  8.01it/s]0m \n",
      " 83%|████████▎ | 149/179 [00:16<00:03,  8.40it/s]0m \n",
      " 84%|████████▍ | 150/179 [00:16<00:03,  8.72it/s]0m \n",
      " 84%|████████▍ | 151/179 [00:16<00:03,  8.98it/s]0m \n",
      " 85%|████████▍ | 152/179 [00:16<00:02,  9.18it/s]0m \n",
      " 85%|████████▌ | 153/179 [00:16<00:02,  9.30it/s]0m \n",
      " 86%|████████▌ | 154/179 [00:16<00:02,  9.38it/s]0m \n",
      " 87%|████████▋ | 155/179 [00:16<00:02,  9.32it/s]0m \n",
      " 87%|████████▋ | 156/179 [00:16<00:02,  9.28it/s]0m \n",
      " 88%|████████▊ | 157/179 [00:16<00:02,  9.38it/s]0m \n",
      " 88%|████████▊ | 158/179 [00:16<00:02,  9.42it/s]0m \n",
      " 89%|████████▉ | 159/179 [00:17<00:02,  9.46it/s]0m \n",
      " 89%|████████▉ | 160/179 [00:17<00:02,  9.41it/s]0m \n",
      " 90%|████████▉ | 161/179 [00:17<00:01,  9.38it/s]0m \n",
      " 91%|█████████ | 162/179 [00:17<00:01,  9.44it/s]0m \n",
      " 91%|█████████ | 163/179 [00:17<00:01,  9.48it/s]0m \n",
      " 92%|█████████▏| 164/179 [00:17<00:01,  9.50it/s]0m \n",
      " 92%|█████████▏| 165/179 [00:17<00:01,  9.41it/s]0m \n",
      " 93%|█████████▎| 166/179 [00:17<00:01,  9.39it/s]0m \n",
      " 93%|█████████▎| 167/179 [00:17<00:01,  9.35it/s]0m \n",
      " 94%|█████████▍| 168/179 [00:18<00:01,  9.28it/s]0m \n",
      " 94%|█████████▍| 169/179 [00:18<00:01,  9.29it/s]0m \n",
      " 95%|█████████▍| 170/179 [00:18<00:00,  9.39it/s]0m \n",
      " 96%|█████████▌| 171/179 [00:18<00:00,  9.44it/s]0m \n",
      " 96%|█████████▌| 172/179 [00:18<00:00,  8.11it/s]0m \n",
      " 97%|█████████▋| 173/179 [00:18<00:00,  7.37it/s]0m \n",
      " 97%|█████████▋| 174/179 [00:18<00:00,  6.91it/s]0m \n",
      " 98%|█████████▊| 175/179 [00:18<00:00,  7.51it/s]0m \n",
      " 98%|█████████▊| 176/179 [00:19<00:00,  8.06it/s]0m \n",
      " 99%|█████████▉| 177/179 [00:19<00:00,  8.48it/s]0m \n",
      " 99%|█████████▉| 178/179 [00:19<00:00,  8.82it/s]0m \n",
      "100%|██████████| 179/179 [00:19<00:00,  7.14it/s]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m ***** eval metrics *****\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   epoch                             =      4.864\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   eval_viggo-val_loss               =     0.1168\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   eval_viggo-val_runtime            = 0:00:19.83\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   eval_viggo-val_samples_per_second =     36.005\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m   eval_viggo-val_steps_per_second   =      9.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [00:19<00:00,  9.17it/s]0m \n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m [INFO|modelcard.py:449] 2025-04-06 15:16:15,604 >> Dropping the following result as it does not have all the necessary fields:\n",
      "\u001b[36m(RayTrainWorker pid=3451, ip=10.0.144.159)\u001b[0m {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed after 1 iterations at 2025-04-06 15:16:17. Total running time: 9min 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 15:16:17,517\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora' in 0.0217s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run multinode distributed fine-tuning workload\n",
    "USE_RAY=1 llamafactory-cli train llama3_lora_sft_ray.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🔎 Monitoring and Debugging with Ray</b> \n",
    "\n",
    "\n",
    "OSS Ray offers an extensive [observability suite](https://docs.ray.io/en/latest/ray-observability/index.html) that offers logs and an observability dashboard that we can use to monitor and debug. The dashboard includes a lot of different components such as:\n",
    "\n",
    "-  memory, utilization, etc. of the tasks running in our [cluster](https://docs.ray.io/en/latest/ray-observability/getting-started.html#dash-node-view)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/cluster_util.png\" width=700>\n",
    "\n",
    "- views to see all our running tasks, utilization across instance types, autoscaling, etc.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/observability_views.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🔎➕➕ Monitoring and Debugging on Anyscale</b> \n",
    "\n",
    "While OSS Ray comes with an extensive obervability suite, Anyscale takes it many steps further to make it even easier and faster to monitor and debug your workloads.\n",
    "\n",
    "- [unified log viewer](https://docs.anyscale.com/monitoring/accessing-logs/) to see logs from *all* our driver and worker processes\n",
    "- Ray workload specific dashboard (Data, Train, etc.) that can breakdown the tasks. For example, our training workload above can be observed live through the Train specific Ray Workloads dashboard:\n",
    "\n",
    "<img src=\"images/train_dashboard.png\" width=700>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🗂️ Storage on Anyscale</b> \n",
    "\n",
    "We can always store to our data inside [any storage buckets](https://docs.anyscale.com/configuration/storage/#private-storage-buckets) but Anyscale offers a [default storage bucket](https://docs.anyscale.com/configuration/storage/#anyscale-default-storage-bucket) to make things even easier. We also have plenty of other [storage options](https://docs.anyscale.com/configuration/storage/) as well (shared at the cluster, user and cloud levels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Anyscale default storage bucket\n",
    "echo $ANYSCALE_ARTIFACT_STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Save fine-tuning artifacts to cloud storage\n",
    "aws s3 rm $ANYSCALE_ARTIFACT_STORAGE/viggo --recursive --quiet\n",
    "aws s3 cp /mnt/cluster_storage/viggo/outputs $ANYSCALE_ARTIFACT_STORAGE/viggo/outputs --recursive --quiet\n",
    "aws s3 cp $2 /mnt/cluster_storage/viggo/saves $ANYSCALE_ARTIFACT_STORAGE/viggo/saves --recursive --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"p\">{</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;epoch&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">4.864</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.11676677316427231</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_runtime&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">19.8306</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_samples_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">36.005</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_steps_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">9.026</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;total_flos&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">4.662888690089984e+16</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.1828844992744346</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_runtime&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">432.7067</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_samples_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">11.555</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_steps_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.358</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}epoch\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{4.864}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}loss\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.11676677316427231}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}runtime\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{19.8306}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}samples\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{36.005}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}steps\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{9.026}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}total\\PYZus{}flos\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{4.662888690089984e+16}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}loss\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.1828844992744346}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}runtime\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{432.7067}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}samples\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{11.555}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}steps\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.358}\n",
       "\\PY{p}{\\PYZcb{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "{\n",
       "    \"epoch\": 4.864,\n",
       "    \"eval_viggo-val_loss\": 0.11676677316427231,\n",
       "    \"eval_viggo-val_runtime\": 19.8306,\n",
       "    \"eval_viggo-val_samples_per_second\": 36.005,\n",
       "    \"eval_viggo-val_steps_per_second\": 9.026,\n",
       "    \"total_flos\": 4.662888690089984e+16,\n",
       "    \"train_loss\": 0.1828844992744346,\n",
       "    \"train_runtime\": 432.7067,\n",
       "    \"train_samples_per_second\": 11.555,\n",
       "    \"train_steps_per_second\": 0.358\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"/mnt/cluster_storage/viggo/outputs/all_results.json\", language=\"json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(filename=\"/mnt/cluster_storage/viggo/outputs/training_loss.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchTrainer_6ffa1_00000_0_2025-04-06_15-06-49\n",
      "basic-variant-state-2025-04-06_15-06-49.json\n",
      "experiment_state-2025-04-06_15-06-49.json\n",
      "trainer.pkl\n",
      "tuner.pkl\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls /mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_6ffa1_00000_0_2025-04-06_15-06-49/checkpoint_000000/checkpoint\n",
      "s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_6ffa1_00000_0_2025-04-06_15-06-49/checkpoint_000000/checkpoint\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora\")\n",
    "trainer_dirs = [d for d in save_dir.iterdir() if d.name.startswith(\"TorchTrainer_\") and d.is_dir()]\n",
    "latest_trainer = max(trainer_dirs, key=lambda d: d.stat().st_mtime, default=None)\n",
    "lora_path = f\"{latest_trainer}/checkpoint_000000/checkpoint\"\n",
    "s3_lora_path = os.path.join(os.getenv(\"ANYSCALE_ARTIFACT_STORAGE\"), lora_path.split(\"/mnt/cluster_storage/\")[-1])\n",
    "print (lora_path)\n",
    "print (s3_lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "adapter_config.json\n",
      "adapter_model.safetensors\n",
      "optimizer.pt\n",
      "rng_state_0.pth\n",
      "rng_state_1.pth\n",
      "rng_state_2.pth\n",
      "rng_state_3.pth\n",
      "scheduler.pt\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "trainer_state.json\n",
      "training_args.bin\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$lora_path\"\n",
    "ls $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference \n",
    "[`Overview`](https://docs.ray.io/en/latest/data/working-with-llms.html) |  [`API reference`](https://docs.ray.io/en/latest/data/api/llm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ray.data.llm` module integrates with key large language model (LLM) inference engines and deployed models to enable LLM batch inference. These llm modules use [Ray Data](https://docs.ray.io/en/latest/data/data.html) under the hood, which makes it extremely easy to distribute our workloads but also ensures that they happen:\n",
    "- **efficiently**: minimize CPU/GPU idletime with hetergenous resource scheduling.\n",
    "- **at scale**: streaming execution to petabyte-scale datasets (especially when [working with LLMs](https://docs.ray.io/en/latest/data/working-with-llms.html))\n",
    "- **reliably** by checkpointing processes, especially when running workloads on spot instanes (with on-demand fallback).\n",
    "- **flexiblibly**: connect to data from any source, apply your transformations and save to any format/location for your next workload.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/ray_data_solution.png\" width=800>\n",
    "\n",
    "[RayTurbo Data](https://docs.anyscale.com/rayturbo/rayturbo-data) has even more functionality on top of Ray Data:\n",
    "- **accelerated metadata fetching** to improve reading first time from large datasets \n",
    "- **optimized autoscaling** where Jobs can kick off before waiting for the entire cluster to start\n",
    "- **high reliabilty** where entire fails jobs (head node, cluster, uncaptured exceptions, etc.) can resume from checkpoints (OSS Ray can only recover from worker node failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the [vLLM engine processor config](https://docs.ray.io/en/latest/data/api/doc/ray.data.llm.vLLMEngineProcessorConfig.html#ray.data.llm.vLLMEngineProcessorConfig) where we can select the model we want to use and the [engine behavior](https://docs.vllm.ai/en/stable/serving/engine_args.html). The model can come from [HuggingFace (HF) Hub](https://huggingface.co/models) or a local model path `/path/to/your/model` (GPTQ, GGUF, or LoRA model formats supported).\n",
    "\n",
    "<img src=\"images/data_llm.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-06 15:22:00 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = vLLMEngineProcessorConfig(\n",
    "    model_source=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    runtime_env={\"env_vars\": {\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}},\n",
    "    engine_kwargs={\n",
    "        \"enable_lora\": True,\n",
    "        \"max_lora_rank\": 8,\n",
    "        \"max_loras\": 1,\n",
    "        \"pipeline_parallel_size\": 1, \n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"max_num_batched_tokens\": 4096,\n",
    "        # complete list: https://docs.vllm.ai/en/stable/serving/engine_args.html\n",
    "    },\n",
    "    concurrency=1,\n",
    "    batch_size=16,\n",
    "    accelerator_type=\"A10G\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll pass our config to an [llm processor](https://docs.ray.io/en/master/data/api/doc/ray.data.llm.build_llm_processor.html#ray.data.llm.build_llm_processor) where we can define the preprocessing and postprocessing steps around inference. With our base model defined in the processor config, we can define the lora adapter layers as part of the preprocessing step of the llm processor itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 15:22:01,282\tINFO worker.py:1660 -- Connecting to existing Ray cluster at address: 10.0.47.147:6379...\n",
      "2025-04-06 15:22:01,292\tINFO worker.py:1843 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-zt5t77xa58pyp3uy28glg2g24d.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-04-06 15:22:01,296\tINFO packaging.py:367 -- Pushing file package 'gcs://_ray_pkg_e28850f3e30418125898e8349c048b7ea11faee6.zip' (1.21MiB) to Ray cluster...\n",
      "2025-04-06 15:22:01,301\tINFO packaging.py:380 -- Successfully pushed file package 'gcs://_ray_pkg_e28850f3e30418125898e8349c048b7ea11faee6.zip'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f6e3ba85cf4f83bd555607824629b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=10235)\u001b[0m INFO 04-06 15:22:07 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    }
   ],
   "source": [
    "processor = build_llm_processor(\n",
    "    config,\n",
    "    preprocess=lambda row: dict(\n",
    "        model=lora_path,  # REMOVE this line if doing inference with just the base model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": row[\"input\"]}\n",
    "        ],\n",
    "        sampling_params={\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 250,\n",
    "            # complete list: https://docs.vllm.ai/en/stable/api/inference_params.html\n",
    "        },\n",
    "    ),\n",
    "    postprocess=lambda row: {\n",
    "        **row,  # all contents\n",
    "        \"generated_output\": row[\"generated_text\"],\n",
    "        # add additional outputs\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 15:22:08,347\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-04-06_14-58-14_355114_2341/logs/ray-data\n",
      "2025-04-06 15:22:08,347\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> ActorPoolMapOperator[Map(_preprocess)->MapBatches(ChatTemplateUDF)] -> ActorPoolMapOperator[MapBatches(TokenizeUDF)] -> ActorPoolMapOperator[MapBatches(vLLMEngineStageUDF)] -> ActorPoolMapOperator[MapBatches(DetokenizeUDF)] -> TaskPoolMapOperator[Map(_postprocess)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b779efa0a7f049a1a05d26998ac8b8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +16s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[36m(autoscaler +16s)\u001b[0m [autoscaler] [4xA10G:48CPU-192GB] Attempting to add 1 node(s) to the cluster (increasing from 0 to 1).\n",
      "\u001b[36m(autoscaler +21s)\u001b[0m [autoscaler] [4xA10G:48CPU-192GB] Launched 1 instances.\n",
      "\u001b[36m(_MapWorker pid=3147, ip=10.0.18.165)\u001b[0m INFO 04-06 15:23:06 __init__.py:190] Automatically detected platform cuda.\n",
      "\u001b[36m(_MapWorker pid=3297, ip=10.0.18.165)\u001b[0m INFO 04-06 15:23:16 __init__.py:190] Automatically detected platform cuda.\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:23:22 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m Max pending requests is set to 141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:23:32 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m WARNING 04-06 15:23:32 config.py:2162] LoRA with chunked prefill is still experimental and may be unstable.\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:23:32 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:23:33 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:23:34 model_runner.py:1110] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:23:34 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.47it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.25it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.19it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.52it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.42it/s]\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:03 model_runner.py:1115] Loading model weights took 14.9634 GB\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:03 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:10 worker.py:267] Memory profiling takes 7.27 seconds\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:10 worker.py:267] the current vLLM instance can use total_gpu_memory (21.99GiB) x gpu_memory_utilization (0.90) = 19.79GiB\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:10 worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 3.56GiB.\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:11 executor_base.py:110] # CUDA blocks: 1824, # CPU blocks: 2048\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:11 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 3.56x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:14 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:01<00:35,  1.05s/it]\n",
      "Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:26,  1.26it/s]\n",
      "Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:22,  1.41it/s]\n",
      "Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]\n",
      "Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]\n",
      "Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:17,  1.62it/s]\n",
      "Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:16,  1.65it/s]\n",
      "Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.67it/s]\n",
      "Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:15,  1.70it/s]\n",
      "Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:14,  1.72it/s]\n",
      "Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:13,  1.74it/s]\n",
      "Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:13,  1.76it/s]\n",
      "Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:12,  1.77it/s]\n",
      "Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:11,  1.78it/s]\n",
      "Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:11,  1.79it/s]\n",
      "Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:10,  1.79it/s]\n",
      "Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:09,  1.82it/s]\n",
      "Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:10<00:09,  1.84it/s]\n",
      "Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:08,  1.86it/s]\n",
      "Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:11<00:08,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:12<00:07,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:12<00:06,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:13<00:06,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:13<00:05,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:14<00:05,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:14<00:04,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:15<00:04,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:16<00:03,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:16<00:03,  1.82it/s]\n",
      "Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:17<00:02,  1.84it/s]\n",
      "Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:17<00:02,  1.85it/s]\n",
      "Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:18<00:01,  1.86it/s]\n",
      "Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:18<00:01,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:19<00:00,  1.87it/s]\n",
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:19<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:33 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.41 GiB\n",
      "\u001b[36m(_MapWorker pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:33 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 30.47 seconds\n",
      "\u001b[36m(_MapWorker pid=4510, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:39 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03712aafffb4e3bba984552154f6ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ListFiles 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c90f81f6e2941c5a8914470a5485a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadFiles 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24182891101e447893f3e93594a0c333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(_preprocess)->MapBatches(ChatTemplateUDF) 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031963291c024d2b82572d10b2e578c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(TokenizeUDF) 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfdad688a984cf888778e989100d53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(vLLMEngineStageUDF) 5: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d85333341e4b6db5b3f24aa7a21c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(DetokenizeUDF) 6: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3e9d0ac2b94b12807189fc9b335133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(_postprocess) 7: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:48 metrics.py:455] Avg prompt throughput: 281.1 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 125 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 20.4%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:48 metrics.py:471] Prefix cache hit rate: GPU: 81.47%, CPU: 0.00%\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:53 metrics.py:455] Avg prompt throughput: 4509.6 tokens/s, Avg generation throughput: 302.3 tokens/s, Running: 140 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.7%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:53 metrics.py:471] Prefix cache hit rate: GPU: 82.24%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 553737609bc04a53b545eaf9d4919117 with size 16: 10.307345370999997\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 52291568e0c346c4b123ebe0e3530d0a with size 16: 10.30840715299999\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 12a2f8a8687a421fb3844dfe1b6d5db4 with size 16: 10.474550902999994\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 596785fac78245329ae211286a19fade with size 16: 11.551732603000005\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 4e90d9706bd241308a1b41cf10c53f0c with size 16: 11.800730044999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:58 metrics.py:455] Avg prompt throughput: 5925.5 tokens/s, Avg generation throughput: 1015.9 tokens/s, Running: 138 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.6%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:24:58 metrics.py:471] Prefix cache hit rate: GPU: 81.69%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch adbd0996026646ab861c33e4b222faf3 with size 16: 13.106287923000025\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 60e51df98f714d3aac779e5a2c47bc3e with size 16: 13.10677405300001\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch b268c9856159475b8495235be6439ff4 with size 16: 13.270294061000016\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch c614bd7b96d346a78e397bea49a325e5 with size 16: 13.560436906000007\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch af410ea03e304120a33df0571e5fef0f with size 16: 14.043222192999991\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 800c5d363d954b1ba28f571529462b12 with size 16: 14.438946963999996\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch ddcb00cd005d4f5a939d4116dbaf7e86 with size 16: 16.45007998599999\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch e49a3a240fd34d4cb0124acc0300caec with size 16: 16.815621601999993\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch ba28705f64d9451d99c78c24b47c053c with size 16: 17.241661370999992\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch d844820e2e664a2c8e08bb72088df730 with size 16: 17.411920874000003\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 274a7a36163b431793d182ec52ab6d6e with size 16: 17.60015464700001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:25:03 metrics.py:455] Avg prompt throughput: 6404.2 tokens/s, Avg generation throughput: 925.2 tokens/s, Running: 141 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.9%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:25:03 metrics.py:471] Prefix cache hit rate: GPU: 82.13%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 11b478992f48484ea4c0c4bac31376f3 with size 16: 18.42661037299999\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 470ea94dd5984c8a88689a8111348af0 with size 16: 18.818971277000003\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 11d8a05ad82f45a69eded4165a1eec50 with size 16: 18.819725673000022\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch da2e5f146f9247cbb7df7aa0270c91e7 with size 16: 18.983983173999974\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch f725ddec3f9744c797bc8fa66cf48219 with size 16: 20.087967562000017\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch f90d7e9179e147ba868bc6ba794b864a with size 16: 20.088472422000024\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch d8fd199dc7b7480f9426231d0c864f5b with size 16: 20.38188092300001\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 3e9ef0c83640407093854d7f57bd500c with size 16: 20.908009178999976\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 87b002849b1f4d6b8cecd175ccd01eb7 with size 16: 20.90825500400001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:25:08 metrics.py:455] Avg prompt throughput: 6238.3 tokens/s, Avg generation throughput: 1004.1 tokens/s, Running: 139 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.5%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:25:08 metrics.py:471] Prefix cache hit rate: GPU: 82.60%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch ccbd0ebe14214366a507fec429e1de44 with size 16: 23.53848504200002\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 5a76a2613672461b85456993b84d86c3 with size 16: 23.53901242699999\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch ca6f08f7099447a88502dac108114aae with size 16: 23.779834394000005\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 9b7ef24079864eebbc93eab71cd20e78 with size 16: 23.78044970100001\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch d60fca5dd204476c80754cf66bf488d6 with size 16: 23.780559471000004\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch ed27880fc03746479c26953131ca25cb with size 16: 24.170701027000007\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 2562e29055314449a8ef504d9ddf1504 with size 16: 24.746575514\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch ceb0c0f865a74606bb1feb434f3098e2 with size 16: 24.89533550600001\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 3b3a16e1133245a1ab60da6a7bec6dbc with size 16: 25.368240698999983\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch f11edc2dd68e4c4fa0046ac58cdb6af0 with size 16: 25.538355973999984\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch ee6f50f8d9e340ba8375fe3efb6a1a0e with size 16: 26.249347881999995\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch a62fb7a0c74145ddb38f35e243491d71 with size 16: 26.416258156999987\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 363041412bbb4f0ba3910bbf6415d371 with size 16: 26.896629290999982\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch cb9e264abac74af393593a23dedc5491 with size 16: 27.167112809000002\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch e49e5a65e6ff424385cf9d48f64b0e74 with size 16: 27.679155667000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:25:14 metrics.py:455] Avg prompt throughput: 6675.4 tokens/s, Avg generation throughput: 988.2 tokens/s, Running: 133 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.7%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:25:14 metrics.py:471] Prefix cache hit rate: GPU: 82.78%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 494108422fdb4c1186bd0757a3b23aea with size 16: 28.563080303999982\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch c573d94efae54417aededb36dde94a43 with size 16: 28.70725019599999\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch aa4534fae9ab48b8a08ca8d31dee259e with size 16: 29.16996366699999\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch f832087ca46f432db7898a4c27949bed with size 16: 29.782041362\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 679e1f0dc1374b59bcd55fc978b572fb with size 16: 29.885283079000004\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 68827e0cd49f4105a9d70b0e769ae1ae with size 16: 30.277618915999994\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 82b9c6ec191f4895a4373a63c1272dae with size 16: 31.333721370000006\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 13f6575912b54a21992c56db834c0d33 with size 16: 31.498721104999987\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch ff554fafe1534ad2b9d19b0315fd8b05 with size 16: 31.688192649\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch c4cef1e57eb14b4d8d7663f734641414 with size 16: 31.88972955\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch b3d303e9f0bf4f13837c9a1599dcbad2 with size 16: 31.889796289999992\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 787f4db6dce243b784443916970f33d8 with size 16: 32.372775630999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:25:19 metrics.py:455] Avg prompt throughput: 7026.0 tokens/s, Avg generation throughput: 948.9 tokens/s, Running: 137 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 31.4%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m INFO 04-06 15:25:19 metrics.py:471] Prefix cache hit rate: GPU: 82.77%, CPU: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 64a855a54da042e985400045ee577661 with size 16: 34.343725799\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 62f83d8867de4fdb94854b02513a9c3a with size 16: 34.41983118499999\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 47382363302049f196ac294ded1cc674 with size 16: 34.79314973300001\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 44e6e17c8b9643c081cf81e71f366d17 with size 16: 34.930388986000025\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch d375b3c667184065892f70ef80add1d5 with size 16: 34.99969145\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 5bf0743b8a1a4c858a49a1f03bcd0cdb with size 16: 35.06523592300002\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 5e8923bffb5b47648cc85c305a7b879b with size 16: 35.12783555599998\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch f4963fe4684748a197cc252c53d13d6f with size 16: 35.128218532999995\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 6fbfcc7f68a845aba5e693c96b4eae0e with size 16: 35.820882783\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 9db4f4f043f6457d95bf1113006d2e3b with size 11: 35.96331726700001\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch cfa9614e8e8b40108d7b5b3f34433a7a with size 16: 36.13711240499998\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch e4c64aa78b4f4c5db006a67722932c0d with size 16: 36.13766053500001\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 52ada4031a1f4809b0b578f72e5ca02f with size 16: 36.26270315900001\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 4578de3a4ccd4765adea12a9fab08f29 with size 16: 36.386555060000006\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch 1290557529f54f12ab3a97cae5bfc764 with size 16: 36.54276241299999\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=3387, ip=10.0.18.165)\u001b[0m [vLLM] Elapsed time for batch ed293067785f44569c2d9ce4f1776594 with size 16: 36.58766907099999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_uuid': 'af410ea03e304120a33df0571e5fef0f',\n",
       " 'embeddings': None,\n",
       " 'generated_text': 'request(specifier[weirdest])',\n",
       " 'generated_tokens': [2079, 39309, 3125, 58, 906, 404, 5086, 2526, 128009],\n",
       " 'input': \"What do you think is the weirdest game you've ever played?\",\n",
       " 'instruction': \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
       " 'messages': [{'content': \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
       "   'role': 'system'},\n",
       "  {'content': \"What do you think is the weirdest game you've ever played?\",\n",
       "   'role': 'user'}],\n",
       " 'metrics': {'arrival_time': 1743978286.1226327,\n",
       "  'finished_time': 1743978293.194894,\n",
       "  'first_scheduled_time': 1743978286.9711804,\n",
       "  'first_token_time': 1743978288.479632,\n",
       "  'last_token_time': 1743978293.1929276,\n",
       "  'model_execute_time': None,\n",
       "  'model_forward_time': None,\n",
       "  'scheduler_time': 0.04725466399997913,\n",
       "  'time_in_queue': 0.8485476970672607},\n",
       " 'model': '/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_6ffa1_00000_0_2025-04-06_15-06-49/checkpoint_000000/checkpoint',\n",
       " 'num_generated_tokens': 9,\n",
       " 'num_input_tokens': 170,\n",
       " 'output': 'request(specifier[weirdest])',\n",
       " 'params': 'SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.3, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=250, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None)',\n",
       " 'prompt': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nGiven a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat do you think is the weirdest game you've ever played?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
       " 'prompt_token_ids': [128000,\n",
       "  128000,\n",
       "  128006,\n",
       "  9125,\n",
       "  128007,\n",
       "  271,\n",
       "  22818,\n",
       "  264,\n",
       "  2218,\n",
       "  11914,\n",
       "  9429,\n",
       "  279,\n",
       "  16940,\n",
       "  7438,\n",
       "  13340,\n",
       "  315,\n",
       "  279,\n",
       "  1988,\n",
       "  11914,\n",
       "  439,\n",
       "  264,\n",
       "  3254,\n",
       "  734,\n",
       "  449,\n",
       "  8365,\n",
       "  323,\n",
       "  7180,\n",
       "  2819,\n",
       "  13,\n",
       "  1115,\n",
       "  734,\n",
       "  1288,\n",
       "  7664,\n",
       "  279,\n",
       "  2218,\n",
       "  925,\n",
       "  30357,\n",
       "  323,\n",
       "  279,\n",
       "  734,\n",
       "  2011,\n",
       "  387,\n",
       "  832,\n",
       "  315,\n",
       "  279,\n",
       "  2768,\n",
       "  2570,\n",
       "  41540,\n",
       "  518,\n",
       "  364,\n",
       "  2079,\n",
       "  518,\n",
       "  364,\n",
       "  47530,\n",
       "  10499,\n",
       "  37400,\n",
       "  518,\n",
       "  364,\n",
       "  14119,\n",
       "  518,\n",
       "  364,\n",
       "  12728,\n",
       "  17209,\n",
       "  518,\n",
       "  364,\n",
       "  96861,\n",
       "  518,\n",
       "  364,\n",
       "  2079,\n",
       "  2769,\n",
       "  36990,\n",
       "  518,\n",
       "  364,\n",
       "  67689,\n",
       "  518,\n",
       "  364,\n",
       "  2079,\n",
       "  17209,\n",
       "  7352,\n",
       "  578,\n",
       "  8365,\n",
       "  2011,\n",
       "  387,\n",
       "  832,\n",
       "  315,\n",
       "  279,\n",
       "  2768,\n",
       "  25,\n",
       "  2570,\n",
       "  609,\n",
       "  518,\n",
       "  364,\n",
       "  4683,\n",
       "  25596,\n",
       "  4257,\n",
       "  518,\n",
       "  364,\n",
       "  23859,\n",
       "  14987,\n",
       "  518,\n",
       "  364,\n",
       "  35501,\n",
       "  518,\n",
       "  364,\n",
       "  288,\n",
       "  10910,\n",
       "  518,\n",
       "  364,\n",
       "  22696,\n",
       "  518,\n",
       "  364,\n",
       "  65011,\n",
       "  518,\n",
       "  364,\n",
       "  3517,\n",
       "  623,\n",
       "  86191,\n",
       "  518,\n",
       "  364,\n",
       "  4752,\n",
       "  26190,\n",
       "  3517,\n",
       "  518,\n",
       "  364,\n",
       "  16111,\n",
       "  82,\n",
       "  518,\n",
       "  364,\n",
       "  10547,\n",
       "  4570,\n",
       "  1284,\n",
       "  14922,\n",
       "  518,\n",
       "  364,\n",
       "  4752,\n",
       "  78563,\n",
       "  25596,\n",
       "  518,\n",
       "  364,\n",
       "  4752,\n",
       "  23647,\n",
       "  25596,\n",
       "  518,\n",
       "  364,\n",
       "  68351,\n",
       "  663,\n",
       "  128009,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  3923,\n",
       "  656,\n",
       "  499,\n",
       "  1781,\n",
       "  374,\n",
       "  279,\n",
       "  98086,\n",
       "  5086,\n",
       "  1847,\n",
       "  499,\n",
       "  3077,\n",
       "  3596,\n",
       "  6476,\n",
       "  30,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271],\n",
       " 'request_id': 12,\n",
       " 'time_taken_llm': 7.160030726999992,\n",
       " 'generated_output': 'request(specifier[weirdest])'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation on test dataset\n",
    "ds = ray.data.read_json(\"/mnt/cluster_storage/viggo/test.jsonl\")  # complete list: https://docs.ray.io/en/latest/data/api/input_output.html\n",
    "ds = processor(ds)\n",
    "results = ds.take_all()\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7996306555863343"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +7m36s)\u001b[0m [autoscaler] Downscaling node i-0325269f43c140a67 (node IP: 10.0.18.165) due to node idle termination.\n"
     ]
    }
   ],
   "source": [
    "# Exact match (strict!)\n",
    "matches = 0\n",
    "for item in results:\n",
    "    if item[\"output\"] == item[\"generated_output\"]:\n",
    "        matches += 1\n",
    "matches / float(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can observe the individual steps in our our batch inference workload through the Anyscale Ray Data dashboard:\n",
    "\n",
    "<img src=\"images/data_dashboard.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "💡 For more advanced guides on topics like optimized model loading, multi-lora, openai-compatible endpoints, etc. check out [more examples](https://docs.ray.io/en/latest/data/working-with-llms.html) and the [API reference](https://docs.ray.io/en/latest/data/api/llm.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online serving\n",
    "[`Overview`](https://docs.ray.io/en/latest/serve/llm/serving-llms.html) | [`API reference`](https://docs.ray.io/en/latest/serve/api/index.html#llm-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ray.serve.llm` APIs allow users to deploy multiple LLM models together with a familiar Ray Serve API, while providing compatibility with the OpenAI API.\n",
    "\n",
    "<img src=\"images/serve_llm.png\" width=500>\n",
    "\n",
    "Ray Serve LLM is designed with the following features:\n",
    "- Automatic scaling and load balancing\n",
    "- Unified multi-node multi-model deployment\n",
    "- OpenAI compatibility\n",
    "- Multi-LoRA support with shared base models\n",
    "- Deep integration with inference engines (vLLM to start)\n",
    "- Composable multi-model LLM pipelines\n",
    "\n",
    "[RayTurbo Serve](https://docs.anyscale.com/rayturbo/rayturbo-serve) on Anyscale has even more functionality on top of Ray Serve:\n",
    "- **fast autoscaling and model loading** to get our services up and running even faster ([5x improvements](https://www.anyscale.com/blog/autoscale-large-ai-models-faster) even for LLMs)\n",
    "- 54% **higher QPS** and up-to 3x **streaming tokens per second** for high traffic serving use-cases\n",
    "- **replica compaction** into fewer nodes where possible to reduce resource fragmentation and improve hardware utilization\n",
    "- **zero-downtime** [incremental rollouts](https://docs.anyscale.com/platform/services/update-a-service/#resource-constrained-updates) so your service is never interrupted\n",
    "- [**different environments**](https://docs.anyscale.com/platform/services/multi-app/#multiple-applications-in-different-containers) for each service in a multi-serve application\n",
    "- **multi availability-zone** aware scheduling of Ray Serve replicas to provide higher redundancy to availability zone failures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI  # to use openai api format\n",
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, LLMServer, LLMRouter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an [LLM config](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) where we can define where our model comes from, it's [autoscaling behavior](https://docs.ray.io/en/latest/serve/autoscaling-guide.html#serve-autoscaling), what hardware to use and [engine arguments](https://docs.vllm.ai/en/stable/serving/engine_args.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config\n",
    "model_id = \"llama-3-8b-instruct\"  # call it whatever you want\n",
    "model_source = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # HF model ID, S3 mirror config, or GCS mirror config\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config={\n",
    "        \"model_id\": model_id,\n",
    "        \"model_source\": model_source\n",
    "    },\n",
    "    lora_config={  # REMOVE this if you are only using a base model\n",
    "        \"dynamic_lora_loading_path\": s3_lora_path,\n",
    "        \"max_num_adapters_per_replica\": 16,  # we only have 1\n",
    "    },\n",
    "    runtime_env={\"env_vars\": {\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}},\n",
    "    deployment_config={\n",
    "        \"autoscaling_config\": {\n",
    "            \"min_replicas\": 1, \n",
    "            \"max_replicas\": 2,\n",
    "            # complete list: https://docs.ray.io/en/latest/serve/autoscaling-guide.html#serve-autoscaling\n",
    "        }\n",
    "    },\n",
    "    accelerator_type=\"A10G\",\n",
    "    engine_kwargs={\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        # complete list: https://docs.vllm.ai/en/stable/serving/engine_args.html\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll deploy our llm config as an application. And since this is all built on top of [Ray Serve](https://docs.ray.io/en/latest/serve/index.html), we can have advanvced service logic around composing models together, deploying multiple applications, model multiplexing, observability, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ProxyActor pid=22873)\u001b[0m INFO 2025-04-06 14:45:54,295 proxy 10.0.23.207 -- Proxy starting on node 4e6eb0bd757d7fbd341bf47e4259871a0b6dfd27e02443d8088a9cae (HTTP port: 8000).\n",
      "INFO 2025-04-06 14:45:54,384 serve 14736 -- Started Serve in namespace \"serve\".\n",
      "\u001b[36m(ProxyActor pid=22873)\u001b[0m INFO 2025-04-06 14:45:54,351 proxy 10.0.23.207 -- Got updated endpoints: {}.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m INFO 2025-04-06 14:45:54,415 controller 22811 -- Deploying new version of Deployment(name='VLLM:llama-3-8b-instruct', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m INFO 2025-04-06 14:45:54,416 controller 22811 -- Deploying new version of Deployment(name='LLMRouter', app='default') (initial target replicas: 2).\n",
      "\u001b[36m(ProxyActor pid=22873)\u001b[0m INFO 2025-04-06 14:45:54,419 proxy 10.0.23.207 -- Got updated endpoints: {Deployment(name='LLMRouter', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=22873)\u001b[0m INFO 2025-04-06 14:45:54,437 proxy 10.0.23.207 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x74acfc5baba0>.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m INFO 2025-04-06 14:45:54,519 controller 22811 -- Adding 1 replica to Deployment(name='VLLM:llama-3-8b-instruct', app='default').\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m INFO 2025-04-06 14:45:54,521 controller 22811 -- Adding 2 replicas to Deployment(name='LLMRouter', app='default').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +9m54s)\u001b[0m [autoscaler] [4xA10G:48CPU-192GB] Attempting to add 1 node(s) to the cluster (increasing from 0 to 1).\n",
      "\u001b[36m(autoscaler +9m54s)\u001b[0m [autoscaler] [4xA10G:48CPU-192GB] Launched 1 instances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:46:24,592 controller 22811 -- Deployment 'VLLM:llama-3-8b-instruct' in application 'default' has 1 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: [{\"CPU\": 1.0}, {\"GPU\": 1.0, \"accelerator_type:A10G\": 0.001}], total resources available: {}. Use `ray status` for more details.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:46:24,593 controller 22811 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:46:54,628 controller 22811 -- Deployment 'VLLM:llama-3-8b-instruct' in application 'default' has 1 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: [{\"CPU\": 1.0}, {\"GPU\": 1.0, \"accelerator_type:A10G\": 0.001}], total resources available: {}. Use `ray status` for more details.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:46:54,629 controller 22811 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {\"CPU\": 45.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMRouter pid=3146, ip=10.0.109.239)\u001b[0m INFO 04-06 14:46:55 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:46:56,711 default_VLLM:llama-3-8b-instruct pxm9ni61 -- No cloud storage mirror configured\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:46:56,711 default_VLLM:llama-3-8b-instruct pxm9ni61 -- Downloading the tokenizer for meta-llama/Meta-Llama-3-8B-Instruct\n",
      "\u001b[36m(ProxyActor pid=3148, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:46:57,636 proxy 10.0.109.239 -- Proxy starting on node 578d2be5fea1addd112065824ea02cfc6cd374ada91da022e4dc5409 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=3148, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:46:57,732 proxy 10.0.109.239 -- Got updated endpoints: {Deployment(name='LLMRouter', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=3148, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:46:57,747 proxy 10.0.109.239 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7227c88a66c0>.\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:11 config.py:542] This model supports multiple tasks: {'generate', 'embed', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 04-06 14:46:55 __init__.py:190] Automatically detected platform cuda.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:11,744 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Getting the server ready ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:16 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:17,257 serve 4023 -- Clearing the current platform cache ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:17 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m Connecting to existing Ray cluster at address: 10.0.23.207:6379...\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:18 ray_distributed_executor.py:149] use_ray_spmd_worker: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:21,792 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Waiting for engine process ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=4117, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:23 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:47:24,653 controller 22811 -- Deployment 'VLLM:llama-3-8b-instruct' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:47:24,654 controller 22811 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:24 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:25 model_runner.py:1110] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:26 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:32,842 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Waiting for engine process ...\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:43,892 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Waiting for engine process ...\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.47it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.24it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.19it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.52it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.41it/s]\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:48 model_runner.py:1115] Loading model weights took 14.9595 GB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:50 worker.py:267] Memory profiling takes 2.36 seconds\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:50 worker.py:267] the current vLLM instance can use total_gpu_memory (21.99GiB) x gpu_memory_utilization (0.90) = 19.79GiB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:50 worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 3.54GiB.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:51 executor_base.py:110] # CUDA blocks: 1812, # CPU blocks: 2048\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:51 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 3.54x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:54 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:47:54,704 controller 22811 -- Deployment 'VLLM:llama-3-8b-instruct' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:47:54,705 controller 22811 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:19,  1.70it/s]\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:54,923 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Waiting for engine process ...\n",
      "Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:18,  1.74it/s]\n",
      "Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:18,  1.77it/s]\n",
      "Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:17,  1.78it/s]\n",
      "Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:16,  1.79it/s]\n",
      "Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:16,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:15,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:14,  1.81it/s]\n",
      "Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:14,  1.84it/s]\n",
      "Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:05<00:13,  1.86it/s]\n",
      "Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:12,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:06<00:12,  1.89it/s]\n",
      "Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:07<00:11,  1.90it/s]\n",
      "Capturing CUDA graph shapes:  40%|████      | 14/35 [00:07<00:11,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:08<00:10,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:08<00:10,  1.89it/s]\n",
      "Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:09<00:09,  1.93it/s]\n",
      "Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:09<00:08,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:10<00:08,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:10<00:07,  2.00it/s]\n",
      "Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:11<00:06,  2.01it/s]\n",
      "Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:11<00:06,  2.03it/s]\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:05,954 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Waiting for engine process ...\n",
      "Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:12<00:05,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:12<00:05,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:13<00:04,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:13<00:04,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:14<00:03,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:14<00:03,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:15<00:02,  2.07it/s]\n",
      "Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:15<00:02,  2.08it/s]\n",
      "Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:15<00:01,  2.09it/s]\n",
      "Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:16<00:01,  2.09it/s]\n",
      "Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:16<00:00,  2.09it/s]\n",
      "Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:17<00:00,  2.10it/s]\n",
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:17<00:00,  1.96it/s]\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:12,146 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Server is ready.\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:12,146 default_VLLM:llama-3-8b-instruct pxm9ni61 -- Started vLLM engine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:12 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.26 GiB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:12 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 23.63 seconds\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 4 PYTHON worker processes have been started on node: 4e6eb0bd757d7fbd341bf47e4259871a0b6dfd27e02443d8088a9cae with address: 10.0.23.207. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[36m(pid=23638)\u001b[0m INFO 04-06 14:48:18 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:19,750 default_VLLM:llama-3-8b-instruct pxm9ni61 bae18a66-746b-4309-a973-1ee6ba112ac8 -- CALL llm_config OK 294.7ms\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:19,756 default_VLLM:llama-3-8b-instruct pxm9ni61 d9ddf5ed-6eb0-4732-8402-27f14522523e -- CALL llm_config OK 300.1ms\n",
      "INFO 2025-04-06 14:48:21,786 serve 14736 -- Application 'default' is ready at http://127.0.0.1:8000/.\n",
      "INFO 2025-04-06 14:48:21,794 serve 14736 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7284b02a6930>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeploymentHandle(deployment='LLMRouter')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy\n",
    "deployment = LLMServer.as_deployment(llm_config.get_serve_options(name_prefix=\"VLLM:\")).bind(llm_config)\n",
    "llm_app = LLMRouter.as_deployment().bind([deployment])\n",
    "serve.run(llm_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:21,937 default_VLLM:llama-3-8b-instruct pxm9ni61 4d96b857-3f09-4a88-9f7d-0a7c362aa16d -- Received streaming request 4d96b857-3f09-4a88-9f7d-0a7c362aa16d\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:21,941 default_VLLM:llama-3-8b-instruct pxm9ni61 4d96b857-3f09-4a88-9f7d-0a7c362aa16d -- Request 4d96b857-3f09-4a88-9f7d-0a7c362aa16d started. Prompt: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m Tell me a joke.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m \n",
      "{\"asctime\": \"2025-04-06 14:48:21,997\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"job_id\": \"04000000\", \"worker_id\": \"04000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"4e6eb0bd757d7fbd341bf47e4259871a0b6dfd27e02443d8088a9cae\", \"timestamp_ns\": 1743976101997094933}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:21 engine.py:275] Added request 4d96b857-3f09-4a88-9f7d-0a7c362aa16d.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:21 metrics.py:455] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you smile!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:22 engine.py:293] Aborted request 4d96b857-3f09-4a88-9f7d-0a7c362aa16d.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMRouter pid=3145, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:22,909 default_LLMRouter bgqxqemo 4d96b857-3f09-4a88-9f7d-0a7c362aa16d -- POST /v1/chat/completions 200 979.5ms\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:22,905 default_VLLM:llama-3-8b-instruct pxm9ni61 4d96b857-3f09-4a88-9f7d-0a7c362aa16d -- Request 4d96b857-3f09-4a88-9f7d-0a7c362aa16d finished (stop). Total time: 0.963570107999999s, Queue time: 0.0028231143951416016s, Generation+async time: 0.9607469936048574s, Input tokens: 16, Generated tokens: 28, tokens/s: 45.797697305203975, generated tokens/s: 29.143989194220712.\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:22,907 default_VLLM:llama-3-8b-instruct pxm9ni61 4d96b857-3f09-4a88-9f7d-0a7c362aa16d -- CALL /v1/chat/completions OK 970.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "# Initialize client\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"fake-key\")\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n",
    "    stream=True\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can observe our running service (deployments and metrics like QPS, latency, etc.) through the [Ray Dashboard](https://docs.ray.io/en/latest/ray-observability/getting-started.html)'s [Serve view](https://docs.ray.io/en/latest/ray-observability/getting-started.html#dash-serve-view):\n",
    "\n",
    "<img src=\"images/serve_dashboard.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "💡 For more advanced guides on topics like structured outputs (ex. json), vision LMs, multi-lora on shared base models, using other inference engines (ex. sglang), etc. fast model loading, etc. check out [more examples](https://docs.ray.io/en/latest/serve/llm/overview.html) and the [API reference](https://docs.ray.io/en/latest/serve/llm/api.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production\n",
    "\n",
    "Seamlessly integrate with your existing CI/CD pipelines by leveraging the Anyscale [CLI](https://docs.anyscale.com/reference/quickstart-cli) or [SDK](https://docs.anyscale.com/reference/quickstart-sdk) to deploy [highly available services](https://docs.anyscale.com/platform/services) and run [reliable batch jobs](https://docs.anyscale.com/platform/jobs). Given we've been developing in an environment that's almost identical to production (multinode cluster), this should drastically speed up our dev → prod velocity.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/cicd.png\" width=600>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4054, ip=10.0.150.21)\u001b[0m INFO 04-05 19:35:53 engine.py:293] Aborted request b96829c1-808c-4aa8-b1d5-304f98361560.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=73936)\u001b[0m INFO 2025-04-05 19:45:56,754 controller 73936 -- Downscaling Deployment(name='LLMRouter', app='default') from 2 to 0 replicas. Current ongoing requests: 0.00, current running replicas: 2.\n",
      "\u001b[36m(ServeController pid=73936)\u001b[0m INFO 2025-04-05 19:45:56,755 controller 73936 -- Removing 2 replicas from Deployment(name='LLMRouter', app='default').\n",
      "\u001b[36m(ServeController pid=73936)\u001b[0m INFO 2025-04-05 19:45:58,782 controller 73936 -- Replica(id='ur53jp9k', deployment='LLMRouter', app='default') is stopped.\n",
      "\u001b[36m(ServeController pid=73936)\u001b[0m INFO 2025-04-05 19:45:58,782 controller 73936 -- Replica(id='df9zz1e9', deployment='LLMRouter', app='default') is stopped.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# clean up\n",
    "rm -rf /mnt/cluster_storage/viggo  # clean up\n",
    "aws s3 rm $ANYSCALE_ARTIFACT_STORAGE/viggo --recursive --quiet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
