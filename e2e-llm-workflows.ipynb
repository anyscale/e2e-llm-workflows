{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Recognition with LLMs\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/🚀 Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/e2e-llm-workflows\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune an LLM to perform batch inference and online serving for entity recognition. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/e2e_llm.png\" width=800>\n",
    "\n",
    "**Note**: the intent of this tutorial is to show how Ray can be use to implement end-to-end LLM workflows that can extend to any use case. Also the objective of fine-tuning here is not to create the most performant model (increase `num_train_epochs` if you want to though) but to show it can be leveraged for downstream workloads (batch inference and online serving) at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "If you're on [Anyscale](https://console.anyscale.com/), you can run this entire tutorial for free (all dependencies are setup and the necessary compute will autoscale). Otherwise be sure to install the dependencies from the [`containerfile`](containerfile) and provision the appropriate GPU resources (4xA10s).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/compute.png\" width=500>\n",
    "\n",
    "**Note**: Be sure to add your [HuggingFace token](https://huggingface.co/settings/tokens) (`HF_TOKEN=<HF_TOKEN>`) (with access to the model you want to use) and `HF_HUB_ENABLE_HF_TRANSFER=1` (enbales faster uploads and downloads from HF hub) to a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env vars on head node and all future worker nodes\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ray.init(runtime_env={\n",
    "    \"HF_TOKEN\": os.getenv(\"HF_TOKEN\"),\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER\": os.getenv(\"HF_HUB_ENABLE_HF_TRANSFER\"),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import textwrap\n",
    "from IPython.display import Code, Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by downloading our data from cloud storage to local shared storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://viggo-ds/train.jsonl to ../../../mnt/cluster_storage/viggo/train.jsonl\n",
      "download: s3://viggo-ds/val.jsonl to ../../../mnt/cluster_storage/viggo/val.jsonl\n",
      "download: s3://viggo-ds/test.jsonl to ../../../mnt/cluster_storage/viggo/test.jsonl\n",
      "download: s3://viggo-ds/dataset_info.json to ../../../mnt/cluster_storage/viggo/dataset_info.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf /mnt/cluster_storage/viggo  # clean up\n",
    "aws s3 cp  s3://viggo-ds/train.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/val.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/test.jsonl /mnt/cluster_storage/viggo/\n",
    "aws s3 cp  s3://viggo-ds/dataset_info.json /mnt/cluster_storage/viggo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"instruction\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
      "  \"input\": \"Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.\",\n",
      "  \"output\": \"give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 1 /mnt/cluster_storage/viggo/train.jsonl | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a target sentence construct the underlying meaning representation of the\n",
      "input sentence as a single function with attributes and attribute values. This\n",
      "function should describe the target string accurately and the function must be\n",
      "one of the following ['inform', 'request', 'give_opinion', 'confirm',\n",
      "'verify_attribute', 'suggest', 'request_explanation', 'recommend',\n",
      "'request_attribute']. The attributes must be one of the following: ['name',\n",
      "'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres',\n",
      "'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam',\n",
      "'has_linux_release', 'has_mac_release', 'specifier']\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/cluster_storage/viggo/train.jsonl\", \"r\") as fp:\n",
    "    first_line = fp.readline()\n",
    "    item = json.loads(first_line)\n",
    "system_content = item[\"instruction\"]\n",
    "print(textwrap.fill(system_content, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have an info file that identifies the datasets and format --- alpaca and sharegpt (great for multimodal tasks) formats are supported --- to use for post training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"p\">{</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;viggo-train&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;file_name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;/mnt/cluster_storage/viggo/train.jsonl&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;formatting&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;alpaca&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;columns&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;prompt&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;instruction&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;query&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;response&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;output&quot;</span>\n",
       "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">},</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;viggo-val&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;file_name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;/mnt/cluster_storage/viggo/val.jsonl&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;formatting&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;alpaca&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">&quot;columns&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;prompt&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;instruction&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;query&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">            </span><span class=\"nt\">&quot;response&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;output&quot;</span>\n",
       "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
       "<span class=\"w\">    </span><span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}viggo\\PYZhy{}train\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}file\\PYZus{}name\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}/mnt/cluster\\PYZus{}storage/viggo/train.jsonl\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}formatting\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}alpaca\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}columns\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}prompt\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}instruction\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}query\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}input\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}response\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}output\\PYZdq{}}\n",
       "\\PY{+w}{        }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{},}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}viggo\\PYZhy{}val\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}file\\PYZus{}name\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}/mnt/cluster\\PYZus{}storage/viggo/val.jsonl\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}formatting\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}alpaca\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{        }\\PY{n+nt}{\\PYZdq{}columns\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}prompt\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}instruction\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}query\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}input\\PYZdq{}}\\PY{p}{,}\n",
       "\\PY{+w}{            }\\PY{n+nt}{\\PYZdq{}response\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s+s2}{\\PYZdq{}output\\PYZdq{}}\n",
       "\\PY{+w}{        }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{+w}{    }\\PY{p}{\\PYZcb{}}\n",
       "\\PY{p}{\\PYZcb{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "{\n",
       "    \"viggo-train\": {\n",
       "        \"file_name\": \"/mnt/cluster_storage/viggo/train.jsonl\",\n",
       "        \"formatting\": \"alpaca\",\n",
       "        \"columns\": {\n",
       "            \"prompt\": \"instruction\",\n",
       "            \"query\": \"input\",\n",
       "            \"response\": \"output\"\n",
       "        }\n",
       "    },\n",
       "    \"viggo-val\": {\n",
       "        \"file_name\": \"/mnt/cluster_storage/viggo/val.jsonl\",\n",
       "        \"formatting\": \"alpaca\",\n",
       "        \"columns\": {\n",
       "            \"prompt\": \"instruction\",\n",
       "            \"query\": \"input\",\n",
       "            \"response\": \"output\"\n",
       "        }\n",
       "    }\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"/mnt/cluster_storage/viggo/dataset_info.json\", language=\"json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [Ray Train](https://docs.ray.io/en/latest/train/train.html) + [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) to peform multinode training. The parameters for our training workload -- post-training method, dataset location, train/val details, etc. --- can be found in the `llama3_lora_sft_ray.yaml` config file. Check out recipes for even more post-training methods (sft, pretraining, ppo, dpo, kto, etc.) [here](https://github.com/hiyouga/LLaMA-Factory/tree/main/examples).\n",
    "\n",
    "**Note**: We also support using other tools like [axolotl](https://axolotl-ai-cloud.github.io/axolotl/docs/ray-integration.html) or even [Ray Train + HF Accelreate + FSDP/Deepspeed](https://docs.ray.io/en/latest/train/huggingface-accelerate.html) directly for complete control of your post-training workloads.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/distributed_training.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\">### model</span>\n",
       "<span class=\"nt\">model_name_or_path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">meta-llama/Meta-Llama-3-8B-Instruct</span>\n",
       "<span class=\"nt\">trust_remote_code</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "\n",
       "<span class=\"c1\">### method</span>\n",
       "<span class=\"nt\">stage</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">sft</span>\n",
       "<span class=\"nt\">do_train</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">finetuning_type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">lora</span>\n",
       "<span class=\"nt\">lora_rank</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">8</span>\n",
       "<span class=\"nt\">lora_target</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">all</span>\n",
       "\n",
       "<span class=\"c1\">### dataset</span>\n",
       "<span class=\"nt\">dataset</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">viggo-train</span>\n",
       "<span class=\"nt\">dataset_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo</span><span class=\"w\">  </span><span class=\"c1\"># shared storage workers have access to</span>\n",
       "<span class=\"nt\">template</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">llama3</span>\n",
       "<span class=\"nt\">cutoff_len</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">2048</span>\n",
       "<span class=\"nt\">max_samples</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1000</span>\n",
       "<span class=\"nt\">overwrite_cache</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">preprocessing_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">16</span>\n",
       "<span class=\"nt\">dataloader_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">4</span>\n",
       "\n",
       "<span class=\"c1\">### output</span>\n",
       "<span class=\"nt\">output_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo/outputs</span><span class=\"w\">  </span><span class=\"c1\"># should be somewhere workers have access to (ex. s3, nfs)</span>\n",
       "<span class=\"nt\">logging_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">10</span>\n",
       "<span class=\"nt\">save_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">500</span>\n",
       "<span class=\"nt\">plot_loss</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">overwrite_output_dir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">save_only_model</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">false</span>\n",
       "\n",
       "<span class=\"c1\">### ray</span>\n",
       "<span class=\"nt\">ray_run_name</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">llama3_8b_sft_lora</span>\n",
       "<span class=\"nt\">ray_storage_path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/mnt/cluster_storage/viggo/saves</span><span class=\"w\">  </span><span class=\"c1\"># should be somewhere workers have access to (ex. s3, nfs)</span>\n",
       "<span class=\"nt\">ray_num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">4</span><span class=\"w\">  </span><span class=\"c1\"># number of GPUs to use</span>\n",
       "<span class=\"nt\">resources_per_worker</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">GPU</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"nt\">placement_strategy</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">PACK</span>\n",
       "\n",
       "<span class=\"c1\">### train</span>\n",
       "<span class=\"nt\">per_device_train_batch_size</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"nt\">gradient_accumulation_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">8</span>\n",
       "<span class=\"nt\">learning_rate</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1.0e-4</span>\n",
       "<span class=\"nt\">num_train_epochs</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">5.0</span>\n",
       "<span class=\"nt\">lr_scheduler_type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">cosine</span>\n",
       "<span class=\"nt\">warmup_ratio</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">0.1</span>\n",
       "<span class=\"nt\">bf16</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n",
       "<span class=\"nt\">ddp_timeout</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">180000000</span>\n",
       "<span class=\"nt\">resume_from_checkpoint</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">null</span>\n",
       "\n",
       "<span class=\"c1\">### eval</span>\n",
       "<span class=\"nt\">eval_dataset</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">viggo-val</span><span class=\"w\">  </span><span class=\"c1\"># uses same dataset_dir as training data</span>\n",
       "<span class=\"c1\"># val_size: 0.1  # only if using part of training data for validation</span>\n",
       "<span class=\"nt\">per_device_eval_batch_size</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "<span class=\"nt\">eval_strategy</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">steps</span>\n",
       "<span class=\"nt\">eval_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">500</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} model}\n",
       "\\PY{n+nt}{model\\PYZus{}name\\PYZus{}or\\PYZus{}path}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{meta\\PYZhy{}llama/Meta\\PYZhy{}Llama\\PYZhy{}3\\PYZhy{}8B\\PYZhy{}Instruct}\n",
       "\\PY{n+nt}{trust\\PYZus{}remote\\PYZus{}code}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} method}\n",
       "\\PY{n+nt}{stage}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{sft}\n",
       "\\PY{n+nt}{do\\PYZus{}train}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{finetuning\\PYZus{}type}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{lora}\n",
       "\\PY{n+nt}{lora\\PYZus{}rank}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{8}\n",
       "\\PY{n+nt}{lora\\PYZus{}target}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{all}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} dataset}\n",
       "\\PY{n+nt}{dataset}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{viggo\\PYZhy{}train}\n",
       "\\PY{n+nt}{dataset\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} shared storage workers have access to}\n",
       "\\PY{n+nt}{template}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{llama3}\n",
       "\\PY{n+nt}{cutoff\\PYZus{}len}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{2048}\n",
       "\\PY{n+nt}{max\\PYZus{}samples}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1000}\n",
       "\\PY{n+nt}{overwrite\\PYZus{}cache}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{preprocessing\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{16}\n",
       "\\PY{n+nt}{dataloader\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{4}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} output}\n",
       "\\PY{n+nt}{output\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo/outputs}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} should be somewhere workers have access to (ex. s3, nfs)}\n",
       "\\PY{n+nt}{logging\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{10}\n",
       "\\PY{n+nt}{save\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{500}\n",
       "\\PY{n+nt}{plot\\PYZus{}loss}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{overwrite\\PYZus{}output\\PYZus{}dir}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{save\\PYZus{}only\\PYZus{}model}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{false}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} ray}\n",
       "\\PY{n+nt}{ray\\PYZus{}run\\PYZus{}name}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{llama3\\PYZus{}8b\\PYZus{}sft\\PYZus{}lora}\n",
       "\\PY{n+nt}{ray\\PYZus{}storage\\PYZus{}path}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/mnt/cluster\\PYZus{}storage/viggo/saves}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} should be somewhere workers have access to (ex. s3, nfs)}\n",
       "\\PY{n+nt}{ray\\PYZus{}num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{4}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} number of GPUs to use}\n",
       "\\PY{n+nt}{resources\\PYZus{}per\\PYZus{}worker}\\PY{p}{:}\n",
       "\\PY{+w}{  }\\PY{n+nt}{GPU}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{n+nt}{placement\\PYZus{}strategy}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{PACK}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} train}\n",
       "\\PY{n+nt}{per\\PYZus{}device\\PYZus{}train\\PYZus{}batch\\PYZus{}size}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{n+nt}{gradient\\PYZus{}accumulation\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{8}\n",
       "\\PY{n+nt}{learning\\PYZus{}rate}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1.0e\\PYZhy{}4}\n",
       "\\PY{n+nt}{num\\PYZus{}train\\PYZus{}epochs}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{5.0}\n",
       "\\PY{n+nt}{lr\\PYZus{}scheduler\\PYZus{}type}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{cosine}\n",
       "\\PY{n+nt}{warmup\\PYZus{}ratio}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{0.1}\n",
       "\\PY{n+nt}{bf16}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{true}\n",
       "\\PY{n+nt}{ddp\\PYZus{}timeout}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{180000000}\n",
       "\\PY{n+nt}{resume\\PYZus{}from\\PYZus{}checkpoint}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{null}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} eval}\n",
       "\\PY{n+nt}{eval\\PYZus{}dataset}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{viggo\\PYZhy{}val}\\PY{+w}{  }\\PY{c+c1}{\\PYZsh{} uses same dataset\\PYZus{}dir as training data}\n",
       "\\PY{c+c1}{\\PYZsh{} val\\PYZus{}size: 0.1  \\PYZsh{} only if using part of training data for validation}\n",
       "\\PY{n+nt}{per\\PYZus{}device\\PYZus{}eval\\PYZus{}batch\\PYZus{}size}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\n",
       "\\PY{n+nt}{eval\\PYZus{}strategy}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{steps}\n",
       "\\PY{n+nt}{eval\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{500}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "### model\n",
       "model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct\n",
       "trust_remote_code: true\n",
       "\n",
       "### method\n",
       "stage: sft\n",
       "do_train: true\n",
       "finetuning_type: lora\n",
       "lora_rank: 8\n",
       "lora_target: all\n",
       "\n",
       "### dataset\n",
       "dataset: viggo-train\n",
       "dataset_dir: /mnt/cluster_storage/viggo  # shared storage workers have access to\n",
       "template: llama3\n",
       "cutoff_len: 2048\n",
       "max_samples: 1000\n",
       "overwrite_cache: true\n",
       "preprocessing_num_workers: 16\n",
       "dataloader_num_workers: 4\n",
       "\n",
       "### output\n",
       "output_dir: /mnt/cluster_storage/viggo/outputs  # should be somewhere workers have access to (ex. s3, nfs)\n",
       "logging_steps: 10\n",
       "save_steps: 500\n",
       "plot_loss: true\n",
       "overwrite_output_dir: true\n",
       "save_only_model: false\n",
       "\n",
       "### ray\n",
       "ray_run_name: llama3_8b_sft_lora\n",
       "ray_storage_path: /mnt/cluster_storage/viggo/saves  # should be somewhere workers have access to (ex. s3, nfs)\n",
       "ray_num_workers: 4  # number of GPUs to use\n",
       "resources_per_worker:\n",
       "  GPU: 1\n",
       "placement_strategy: PACK\n",
       "\n",
       "### train\n",
       "per_device_train_batch_size: 1\n",
       "gradient_accumulation_steps: 8\n",
       "learning_rate: 1.0e-4\n",
       "num_train_epochs: 5.0\n",
       "lr_scheduler_type: cosine\n",
       "warmup_ratio: 0.1\n",
       "bf16: true\n",
       "ddp_timeout: 180000000\n",
       "resume_from_checkpoint: null\n",
       "\n",
       "### eval\n",
       "eval_dataset: viggo-val  # uses same dataset_dir as training data\n",
       "# val_size: 0.1  # only if using part of training data for validation\n",
       "per_device_eval_batch_size: 1\n",
       "eval_strategy: steps\n",
       "eval_steps: 500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"llama3_lora_sft_ray.yaml\", language=\"yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO 04-06 15:06:44 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "\n",
      "Training finished iteration 1 at 2025-04-06 15:15:54. Total running time: 9min 5s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000000 │\n",
      "│ time_this_iter_s              482.24643 │\n",
      "│ time_total_s                  482.24643 │\n",
      "│ training_iteration                    1 │\n",
      "│ epoch                             4.704 │\n",
      "│ grad_norm                       0.15772 │\n",
      "│ learning_rate                        0. │\n",
      "│ loss                             0.0026 │\n",
      "│ step                                150 │\n",
      "╰─────────────────────────────────────────╯\n",
      "\n",
      "2025-04-06 15:16:17,517\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora' in 0.0217s.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run multinode distributed fine-tuning workload\n",
    "USE_RAY=1 llamafactory-cli train llama3_lora_sft_ray.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b>Ray Train</b> \n",
    "\n",
    "Using [Ray Train](https://docs.ray.io/en/latest/train/train.html) here has several advantages:\n",
    "- automatically handles **multi-node, multi-GPU** setup with no manual SSH setup or hostfile configs. \n",
    "- define **per-worker franctional resource requirements** (e.g., 2 CPUs and 0.5 GPU per worker)\n",
    "- run on **heterogeneous machines** and scale flexibly (e.g., CPU for preprocessing and GPU for training) \n",
    "- built-in **fault tolerance** via retry of failed workers (and continue from last checkpoint).\n",
    "- supports Data Parallel, Model Parallel, Parameter Server, and even custom strategies.\n",
    "- [Ray Compiled graphs](https://docs.ray.io/en/latest/ray-core/compiled-graph/ray-compiled-graph.html) allow us to even define different parallelism for jointly optimizing multipe models (Megatron, Deepspeed, etc. only allow for one global setting).\n",
    "\n",
    "[RayTurbo Train](https://docs.anyscale.com/rayturbo/rayturbo-train) offers even more improvement to the price-performance ratio, performance monitoring and more:\n",
    "- **elastic training** to scale to a dynamic number of workers, continue training on fewer resources (even on spot instances).\n",
    "- **purpose-built dashboard** designed to streamline the debugging of Ray Train workloads\n",
    "    - Monitoring: View the status of training runs and train workers.\n",
    "    - Metrics: See insights on training throughput, training system operation time.\n",
    "    - Profiling: Investigate bottlenecks, hangs, or errors from individual training worker processes.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/train_dashboard.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🔎 Monitoring and Debugging with Ray</b> \n",
    "\n",
    "\n",
    "OSS Ray offers an extensive [observability suite](https://docs.ray.io/en/latest/ray-observability/index.html) that offers logs and an observability dashboard that we can use to monitor and debug. The dashboard includes a lot of different components such as:\n",
    "\n",
    "-  memory, utilization, etc. of the tasks running in our [cluster](https://docs.ray.io/en/latest/ray-observability/getting-started.html#dash-node-view)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/cluster_util.png\" width=700>\n",
    "\n",
    "- views to see all our running tasks, utilization across instance types, autoscaling, etc.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/observability_views.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🔎➕➕ Monitoring and Debugging on Anyscale</b> \n",
    "\n",
    "While OSS Ray comes with an extensive obervability suite, Anyscale takes it many steps further to make it even easier and faster to monitor and debug your workloads.\n",
    "\n",
    "- [unified log viewer](https://docs.anyscale.com/monitoring/accessing-logs/) to see logs from *all* our driver and worker processes\n",
    "- Ray workload specific dashboard (Data, Train, etc.) that can breakdown the tasks. For example, our training workload above can be observed live through the Train specific Ray Workloads dashboard:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/train_dashboard.png\" width=700>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> 🗂️ Storage on Anyscale</b> \n",
    "\n",
    "We can always store to our data inside [any storage buckets](https://docs.anyscale.com/configuration/storage/#private-storage-buckets) but Anyscale offers a [default storage bucket](https://docs.anyscale.com/configuration/storage/#anyscale-default-storage-bucket) to make things even easier. We also have plenty of other [storage options](https://docs.anyscale.com/configuration/storage/) as well (shared at the cluster, user and cloud levels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Anyscale default storage bucket\n",
    "echo $ANYSCALE_ARTIFACT_STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Save fine-tuning artifacts to cloud storage\n",
    "aws s3 rm $ANYSCALE_ARTIFACT_STORAGE/viggo --recursive --quiet\n",
    "aws s3 cp /mnt/cluster_storage/viggo/outputs $ANYSCALE_ARTIFACT_STORAGE/viggo/outputs --recursive --quiet\n",
    "aws s3 cp $2 /mnt/cluster_storage/viggo/saves $ANYSCALE_ARTIFACT_STORAGE/viggo/saves --recursive --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"p\">{</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;epoch&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">4.864</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.11676677316427231</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_runtime&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">19.8306</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_samples_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">36.005</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;eval_viggo-val_steps_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">9.026</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;total_flos&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">4.662888690089984e+16</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_loss&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.1828844992744346</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_runtime&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">432.7067</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_samples_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">11.555</span><span class=\"p\">,</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">&quot;train_steps_per_second&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.358</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{p}{\\PYZob{}}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}epoch\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{4.864}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}loss\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.11676677316427231}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}runtime\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{19.8306}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}samples\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{36.005}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}eval\\PYZus{}viggo\\PYZhy{}val\\PYZus{}steps\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{9.026}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}total\\PYZus{}flos\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{4.662888690089984e+16}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}loss\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.1828844992744346}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}runtime\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{432.7067}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}samples\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{11.555}\\PY{p}{,}\n",
       "\\PY{+w}{    }\\PY{n+nt}{\\PYZdq{}train\\PYZus{}steps\\PYZus{}per\\PYZus{}second\\PYZdq{}}\\PY{p}{:}\\PY{+w}{ }\\PY{l+m+mf}{0.358}\n",
       "\\PY{p}{\\PYZcb{}}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "{\n",
       "    \"epoch\": 4.864,\n",
       "    \"eval_viggo-val_loss\": 0.11676677316427231,\n",
       "    \"eval_viggo-val_runtime\": 19.8306,\n",
       "    \"eval_viggo-val_samples_per_second\": 36.005,\n",
       "    \"eval_viggo-val_steps_per_second\": 9.026,\n",
       "    \"total_flos\": 4.662888690089984e+16,\n",
       "    \"train_loss\": 0.1828844992744346,\n",
       "    \"train_runtime\": 432.7067,\n",
       "    \"train_samples_per_second\": 11.555,\n",
       "    \"train_steps_per_second\": 0.358\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Code(filename=\"/mnt/cluster_storage/viggo/outputs/all_results.json\", language=\"json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(filename=\"/mnt/cluster_storage/viggo/outputs/training_loss.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchTrainer_6ffa1_00000_0_2025-04-06_15-06-49\n",
      "basic-variant-state-2025-04-06_15-06-49.json\n",
      "experiment_state-2025-04-06_15-06-49.json\n",
      "trainer.pkl\n",
      "tuner.pkl\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls /mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_6ffa1_00000_0_2025-04-06_15-06-49/checkpoint_000000/checkpoint\n",
      "s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_6ffa1_00000_0_2025-04-06_15-06-49/checkpoint_000000/checkpoint\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora\")\n",
    "trainer_dirs = [d for d in save_dir.iterdir() if d.name.startswith(\"TorchTrainer_\") and d.is_dir()]\n",
    "latest_trainer = max(trainer_dirs, key=lambda d: d.stat().st_mtime, default=None)\n",
    "lora_path = f\"{latest_trainer}/checkpoint_000000/checkpoint\"\n",
    "s3_lora_path = os.path.join(os.getenv(\"ANYSCALE_ARTIFACT_STORAGE\"), lora_path.split(\"/mnt/cluster_storage/\")[-1])\n",
    "print (lora_path)\n",
    "print (s3_lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "adapter_config.json\n",
      "adapter_model.safetensors\n",
      "optimizer.pt\n",
      "rng_state_0.pth\n",
      "rng_state_1.pth\n",
      "rng_state_2.pth\n",
      "rng_state_3.pth\n",
      "scheduler.pt\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "trainer_state.json\n",
      "training_args.bin\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$lora_path\"\n",
    "ls $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference \n",
    "[`Overview`](https://docs.ray.io/en/latest/data/working-with-llms.html) |  [`API reference`](https://docs.ray.io/en/latest/data/api/llm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ray.data.llm` module integrates with key large language model (LLM) inference engines and deployed models to enable LLM batch inference. These llm modules use [Ray Data](https://docs.ray.io/en/latest/data/data.html) under the hood, which makes it extremely easy to distribute our workloads but also ensures that they happen:\n",
    "- **efficiently**: minimize CPU/GPU idletime with hetergenous resource scheduling.\n",
    "- **at scale**: streaming execution to petabyte-scale datasets (especially when [working with LLMs](https://docs.ray.io/en/latest/data/working-with-llms.html))\n",
    "- **reliably** by checkpointing processes, especially when running workloads on spot instanes (with on-demand fallback).\n",
    "- **flexiblibly**: connect to data from any source, apply your transformations and save to any format/location for your next workload.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/ray_data_solution.png\" width=800>\n",
    "\n",
    "[RayTurbo Data](https://docs.anyscale.com/rayturbo/rayturbo-data) has even more functionality on top of Ray Data:\n",
    "- **accelerated metadata fetching** to improve reading first time from large datasets \n",
    "- **optimized autoscaling** where Jobs can kick off before waiting for the entire cluster to start\n",
    "- **high reliabilty** where entire fails jobs (head node, cluster, uncaptured exceptions, etc.) can resume from checkpoints (OSS Ray can only recover from worker node failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the [vLLM engine processor config](https://docs.ray.io/en/latest/data/api/doc/ray.data.llm.vLLMEngineProcessorConfig.html#ray.data.llm.vLLMEngineProcessorConfig) where we can select the model we want to use and the [engine behavior](https://docs.vllm.ai/en/stable/serving/engine_args.html). The model can come from [HuggingFace (HF) Hub](https://huggingface.co/models) or a local model path `/path/to/your/model` (GPTQ, GGUF, or LoRA model formats supported).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/data_llm.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-06 15:22:00 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = vLLMEngineProcessorConfig(\n",
    "    model_source=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    runtime_env={\"env_vars\": {\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}},\n",
    "    engine_kwargs={\n",
    "        \"enable_lora\": True,\n",
    "        \"max_lora_rank\": 8,\n",
    "        \"max_loras\": 1,\n",
    "        \"pipeline_parallel_size\": 1, \n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"max_num_batched_tokens\": 4096,\n",
    "        # complete list: https://docs.vllm.ai/en/stable/serving/engine_args.html\n",
    "    },\n",
    "    concurrency=1,\n",
    "    batch_size=16,\n",
    "    accelerator_type=\"A10G\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll pass our config to an [llm processor](https://docs.ray.io/en/master/data/api/doc/ray.data.llm.build_llm_processor.html#ray.data.llm.build_llm_processor) where we can define the preprocessing and postprocessing steps around inference. With our base model defined in the processor config, we can define the lora adapter layers as part of the preprocessing step of the llm processor itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 15:22:01,282\tINFO worker.py:1660 -- Connecting to existing Ray cluster at address: 10.0.47.147:6379...\n",
      "2025-04-06 15:22:01,292\tINFO worker.py:1843 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-zt5t77xa58pyp3uy28glg2g24d.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-04-06 15:22:01,296\tINFO packaging.py:367 -- Pushing file package 'gcs://_ray_pkg_e28850f3e30418125898e8349c048b7ea11faee6.zip' (1.21MiB) to Ray cluster...\n",
      "2025-04-06 15:22:01,301\tINFO packaging.py:380 -- Successfully pushed file package 'gcs://_ray_pkg_e28850f3e30418125898e8349c048b7ea11faee6.zip'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f6e3ba85cf4f83bd555607824629b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=10235)\u001b[0m INFO 04-06 15:22:07 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    }
   ],
   "source": [
    "processor = build_llm_processor(\n",
    "    config,\n",
    "    preprocess=lambda row: dict(\n",
    "        model=lora_path,  # REMOVE this line if doing inference with just the base model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": row[\"input\"]}\n",
    "        ],\n",
    "        sampling_params={\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 250,\n",
    "            # complete list: https://docs.vllm.ai/en/stable/api/inference_params.html\n",
    "        },\n",
    "    ),\n",
    "    postprocess=lambda row: {\n",
    "        **row,  # all contents\n",
    "        \"generated_output\": row[\"generated_text\"],\n",
    "        # add additional outputs\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"batch_uuid\": \"af410ea03e304120a33df0571e5fef0f\",\n",
      "  \"embeddings\": null,\n",
      "  \"generated_text\": \"request(specifier[weirdest])\",\n",
      "  \"generated_tokens\": [2079, 39309, 3125, 58, 906, 404, 5086, 2526, 128009],\n",
      "  \"input\": \"What do you think is the weirdest game you've ever played?\",\n",
      "  \"instruction\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"content\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
      "      \"role\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"What do you think is the weirdest game you've ever played?\",\n",
      "      \"role\": \"user\"\n",
      "    }\n",
      "  ],\n",
      "  \"metrics\": {\n",
      "    \"arrival_time\": 1743978286.1226327,\n",
      "    \"finished_time\": 1743978293.194894,\n",
      "    \"first_scheduled_time\": 1743978286.9711804,\n",
      "    \"first_token_time\": 1743978288.479632,\n",
      "    \"last_token_time\": 1743978293.1929276,\n",
      "    \"model_execute_time\": null,\n",
      "    \"model_forward_time\": null,\n",
      "    \"scheduler_time\": 0.04725466399997913,\n",
      "    \"time_in_queue\": 0.8485476970672607\n",
      "  },\n",
      "  \"model\": \"/mnt/cluster_storage/viggo/saves/llama3_8b_sft_lora/TorchTrainer_6ffa1_00000_0_2025-04-06_15-06-49/checkpoint_000000/checkpoint\",\n",
      "  \"num_generated_tokens\": 9,\n",
      "  \"num_input_tokens\": 170,\n",
      "  \"output\": \"request(specifier[weirdest])\",\n",
      "  \"params\": \"SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.3, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=250, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None)\",\n",
      "  \"prompt\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What do you think is the weirdest game you've ever played?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\",\n",
      "  \"prompt_token_ids\": [128000],\n",
      "  \"request_id\": 12,\n",
      "  \"time_taken_llm\": 7.160030726999992,\n",
      "  \"generated_output\": \"request(specifier[weirdest])\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test dataset\n",
    "ds = ray.data.read_json(\"/mnt/cluster_storage/viggo/test.jsonl\")  # complete list: https://docs.ray.io/en/latest/data/api/input_output.html\n",
    "ds = processor(ds)\n",
    "results = ds.take_all()\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7996306555863343"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +7m36s)\u001b[0m [autoscaler] Downscaling node i-0325269f43c140a67 (node IP: 10.0.18.165) due to node idle termination.\n"
     ]
    }
   ],
   "source": [
    "# Exact match (strict!)\n",
    "matches = 0\n",
    "for item in results:\n",
    "    if item[\"output\"] == item[\"generated_output\"]:\n",
    "        matches += 1\n",
    "matches / float(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can observe the individual steps in our our batch inference workload through the Anyscale Ray Data dashboard:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/data_dashboard.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "💡 For more advanced guides on topics like optimized model loading, multi-lora, openai-compatible endpoints, etc. check out [more examples](https://docs.ray.io/en/latest/data/working-with-llms.html) and the [API reference](https://docs.ray.io/en/latest/data/api/llm.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online serving\n",
    "[`Overview`](https://docs.ray.io/en/latest/serve/llm/serving-llms.html) | [`API reference`](https://docs.ray.io/en/latest/serve/api/index.html#llm-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/ray_serve.png\" width=600>\n",
    "\n",
    "`ray.serve.llm` APIs allow users to deploy multiple LLM models together with a familiar Ray Serve API, while providing compatibility with the OpenAI API.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/serve_llm.png\" width=500>\n",
    "\n",
    "Ray Serve LLM is designed with the following features:\n",
    "- Automatic scaling and load balancing\n",
    "- Unified multi-node multi-model deployment\n",
    "- OpenAI compatibility\n",
    "- Multi-LoRA support with shared base models\n",
    "- Deep integration with inference engines (vLLM to start)\n",
    "- Composable multi-model LLM pipelines\n",
    "\n",
    "[RayTurbo Serve](https://docs.anyscale.com/rayturbo/rayturbo-serve) on Anyscale has even more functionality on top of Ray Serve:\n",
    "- **fast autoscaling and model loading** to get our services up and running even faster ([5x improvements](https://www.anyscale.com/blog/autoscale-large-ai-models-faster) even for LLMs)\n",
    "- 54% **higher QPS** and up-to 3x **streaming tokens per second** for high traffic serving use-cases\n",
    "- **replica compaction** into fewer nodes where possible to reduce resource fragmentation and improve hardware utilization\n",
    "- **zero-downtime** [incremental rollouts](https://docs.anyscale.com/platform/services/update-a-service/#resource-constrained-updates) so your service is never interrupted\n",
    "- [**different environments**](https://docs.anyscale.com/platform/services/multi-app/#multiple-applications-in-different-containers) for each service in a multi-serve application\n",
    "- **multi availability-zone** aware scheduling of Ray Serve replicas to provide higher redundancy to availability zone failures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI  # to use openai api format\n",
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, LLMServer, LLMRouter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an [LLM config](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) where we can define where our model comes from, it's [autoscaling behavior](https://docs.ray.io/en/latest/serve/autoscaling-guide.html#serve-autoscaling), what hardware to use and [engine arguments](https://docs.vllm.ai/en/stable/serving/engine_args.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config\n",
    "model_id = \"llama-3-8b-instruct\"  # call it whatever you want\n",
    "model_source = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # HF model ID, S3 mirror config, or GCS mirror config\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config={\n",
    "        \"model_id\": model_id,\n",
    "        \"model_source\": model_source\n",
    "    },\n",
    "    lora_config={  # REMOVE this if you are only using a base model\n",
    "        \"dynamic_lora_loading_path\": s3_lora_path,\n",
    "        \"max_num_adapters_per_replica\": 16,  # we only have 1\n",
    "    },\n",
    "    runtime_env={\"env_vars\": {\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}},\n",
    "    deployment_config={\n",
    "        \"autoscaling_config\": {\n",
    "            \"min_replicas\": 1, \n",
    "            \"max_replicas\": 2,\n",
    "            # complete list: https://docs.ray.io/en/latest/serve/autoscaling-guide.html#serve-autoscaling\n",
    "        }\n",
    "    },\n",
    "    accelerator_type=\"A10G\",\n",
    "    engine_kwargs={\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        # complete list: https://docs.vllm.ai/en/stable/serving/engine_args.html\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll deploy our llm config as an application. And since this is all built on top of [Ray Serve](https://docs.ray.io/en/latest/serve/index.html), we can have advanvced service logic around composing models together, deploying multiple applications, model multiplexing, observability, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ProxyActor pid=22873)\u001b[0m INFO 2025-04-06 14:45:54,295 proxy 10.0.23.207 -- Proxy starting on node 4e6eb0bd757d7fbd341bf47e4259871a0b6dfd27e02443d8088a9cae (HTTP port: 8000).\n",
      "INFO 2025-04-06 14:45:54,384 serve 14736 -- Started Serve in namespace \"serve\".\n",
      "\u001b[36m(ProxyActor pid=22873)\u001b[0m INFO 2025-04-06 14:45:54,351 proxy 10.0.23.207 -- Got updated endpoints: {}.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m INFO 2025-04-06 14:45:54,415 controller 22811 -- Deploying new version of Deployment(name='VLLM:llama-3-8b-instruct', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m INFO 2025-04-06 14:45:54,416 controller 22811 -- Deploying new version of Deployment(name='LLMRouter', app='default') (initial target replicas: 2).\n",
      "\u001b[36m(ProxyActor pid=22873)\u001b[0m INFO 2025-04-06 14:45:54,419 proxy 10.0.23.207 -- Got updated endpoints: {Deployment(name='LLMRouter', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=22873)\u001b[0m INFO 2025-04-06 14:45:54,437 proxy 10.0.23.207 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x74acfc5baba0>.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m INFO 2025-04-06 14:45:54,519 controller 22811 -- Adding 1 replica to Deployment(name='VLLM:llama-3-8b-instruct', app='default').\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m INFO 2025-04-06 14:45:54,521 controller 22811 -- Adding 2 replicas to Deployment(name='LLMRouter', app='default').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +9m54s)\u001b[0m [autoscaler] [4xA10G:48CPU-192GB] Attempting to add 1 node(s) to the cluster (increasing from 0 to 1).\n",
      "\u001b[36m(autoscaler +9m54s)\u001b[0m [autoscaler] [4xA10G:48CPU-192GB] Launched 1 instances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:46:24,592 controller 22811 -- Deployment 'VLLM:llama-3-8b-instruct' in application 'default' has 1 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: [{\"CPU\": 1.0}, {\"GPU\": 1.0, \"accelerator_type:A10G\": 0.001}], total resources available: {}. Use `ray status` for more details.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:46:24,593 controller 22811 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:46:54,628 controller 22811 -- Deployment 'VLLM:llama-3-8b-instruct' in application 'default' has 1 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: [{\"CPU\": 1.0}, {\"GPU\": 1.0, \"accelerator_type:A10G\": 0.001}], total resources available: {}. Use `ray status` for more details.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:46:54,629 controller 22811 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {\"CPU\": 45.0}. Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMRouter pid=3146, ip=10.0.109.239)\u001b[0m INFO 04-06 14:46:55 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:46:56,711 default_VLLM:llama-3-8b-instruct pxm9ni61 -- No cloud storage mirror configured\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:46:56,711 default_VLLM:llama-3-8b-instruct pxm9ni61 -- Downloading the tokenizer for meta-llama/Meta-Llama-3-8B-Instruct\n",
      "\u001b[36m(ProxyActor pid=3148, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:46:57,636 proxy 10.0.109.239 -- Proxy starting on node 578d2be5fea1addd112065824ea02cfc6cd374ada91da022e4dc5409 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=3148, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:46:57,732 proxy 10.0.109.239 -- Got updated endpoints: {Deployment(name='LLMRouter', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=3148, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:46:57,747 proxy 10.0.109.239 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7227c88a66c0>.\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:11 config.py:542] This model supports multiple tasks: {'generate', 'embed', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 04-06 14:46:55 __init__.py:190] Automatically detected platform cuda.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:11,744 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Getting the server ready ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:16 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:17,257 serve 4023 -- Clearing the current platform cache ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:17 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m Connecting to existing Ray cluster at address: 10.0.23.207:6379...\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:18 ray_distributed_executor.py:149] use_ray_spmd_worker: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:21,792 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Waiting for engine process ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=4117, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:23 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:47:24,653 controller 22811 -- Deployment 'VLLM:llama-3-8b-instruct' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:47:24,654 controller 22811 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:24 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:25 model_runner.py:1110] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:26 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:32,842 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Waiting for engine process ...\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:43,892 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Waiting for engine process ...\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.47it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.24it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.19it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.52it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.41it/s]\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:48 model_runner.py:1115] Loading model weights took 14.9595 GB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:50 worker.py:267] Memory profiling takes 2.36 seconds\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:50 worker.py:267] the current vLLM instance can use total_gpu_memory (21.99GiB) x gpu_memory_utilization (0.90) = 19.79GiB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:50 worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 3.54GiB.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:51 executor_base.py:110] # CUDA blocks: 1812, # CPU blocks: 2048\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:51 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 3.54x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:47:54 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:47:54,704 controller 22811 -- Deployment 'VLLM:llama-3-8b-instruct' in application 'default' has 1 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m WARNING 2025-04-06 14:47:54,705 controller 22811 -- Deployment 'LLMRouter' in application 'default' has 2 replicas that have taken more than 30s to initialize.\n",
      "\u001b[36m(ServeController pid=22811)\u001b[0m This may be caused by a slow __init__ or reconfigure method.\n",
      "Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:19,  1.70it/s]\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:47:54,923 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Waiting for engine process ...\n",
      "Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:18,  1.74it/s]\n",
      "Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:18,  1.77it/s]\n",
      "Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:17,  1.78it/s]\n",
      "Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:16,  1.79it/s]\n",
      "Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:16,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:15,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:14,  1.81it/s]\n",
      "Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:14,  1.84it/s]\n",
      "Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:05<00:13,  1.86it/s]\n",
      "Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:12,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:06<00:12,  1.89it/s]\n",
      "Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:07<00:11,  1.90it/s]\n",
      "Capturing CUDA graph shapes:  40%|████      | 14/35 [00:07<00:11,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:08<00:10,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:08<00:10,  1.89it/s]\n",
      "Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:09<00:09,  1.93it/s]\n",
      "Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:09<00:08,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:10<00:08,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:10<00:07,  2.00it/s]\n",
      "Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:11<00:06,  2.01it/s]\n",
      "Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:11<00:06,  2.03it/s]\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:05,954 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Waiting for engine process ...\n",
      "Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:12<00:05,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:12<00:05,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:13<00:04,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:13<00:04,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:14<00:03,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:14<00:03,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:15<00:02,  2.07it/s]\n",
      "Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:15<00:02,  2.08it/s]\n",
      "Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:15<00:01,  2.09it/s]\n",
      "Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:16<00:01,  2.09it/s]\n",
      "Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:16<00:00,  2.09it/s]\n",
      "Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:17<00:00,  2.10it/s]\n",
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:17<00:00,  1.96it/s]\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:12,146 default_VLLM:llama-3-8b-instruct pxm9ni61 -- [STATUS] Server is ready.\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:12,146 default_VLLM:llama-3-8b-instruct pxm9ni61 -- Started vLLM engine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:12 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.26 GiB\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:12 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 23.63 seconds\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 4 PYTHON worker processes have been started on node: 4e6eb0bd757d7fbd341bf47e4259871a0b6dfd27e02443d8088a9cae with address: 10.0.23.207. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[36m(pid=23638)\u001b[0m INFO 04-06 14:48:18 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:19,750 default_VLLM:llama-3-8b-instruct pxm9ni61 bae18a66-746b-4309-a973-1ee6ba112ac8 -- CALL llm_config OK 294.7ms\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:19,756 default_VLLM:llama-3-8b-instruct pxm9ni61 d9ddf5ed-6eb0-4732-8402-27f14522523e -- CALL llm_config OK 300.1ms\n",
      "INFO 2025-04-06 14:48:21,786 serve 14736 -- Application 'default' is ready at http://127.0.0.1:8000/.\n",
      "INFO 2025-04-06 14:48:21,794 serve 14736 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7284b02a6930>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeploymentHandle(deployment='LLMRouter')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy\n",
    "deployment = LLMServer.as_deployment(llm_config.get_serve_options(name_prefix=\"VLLM:\")).bind(llm_config)\n",
    "llm_app = LLMRouter.as_deployment().bind([deployment])\n",
    "serve.run(llm_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:21,937 default_VLLM:llama-3-8b-instruct pxm9ni61 4d96b857-3f09-4a88-9f7d-0a7c362aa16d -- Received streaming request 4d96b857-3f09-4a88-9f7d-0a7c362aa16d\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:21,941 default_VLLM:llama-3-8b-instruct pxm9ni61 4d96b857-3f09-4a88-9f7d-0a7c362aa16d -- Request 4d96b857-3f09-4a88-9f7d-0a7c362aa16d started. Prompt: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m Tell me a joke.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m \n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m \n",
      "{\"asctime\": \"2025-04-06 14:48:21,997\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"job_id\": \"04000000\", \"worker_id\": \"04000000ffffffffffffffffffffffffffffffffffffffffffffffff\", \"node_id\": \"4e6eb0bd757d7fbd341bf47e4259871a0b6dfd27e02443d8088a9cae\", \"timestamp_ns\": 1743976101997094933}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:21 engine.py:275] Added request 4d96b857-3f09-4a88-9f7d-0a7c362aa16d.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:21 metrics.py:455] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you smile!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:22 engine.py:293] Aborted request 4d96b857-3f09-4a88-9f7d-0a7c362aa16d.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:LLMRouter pid=3145, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:22,909 default_LLMRouter bgqxqemo 4d96b857-3f09-4a88-9f7d-0a7c362aa16d -- POST /v1/chat/completions 200 979.5ms\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:22,905 default_VLLM:llama-3-8b-instruct pxm9ni61 4d96b857-3f09-4a88-9f7d-0a7c362aa16d -- Request 4d96b857-3f09-4a88-9f7d-0a7c362aa16d finished (stop). Total time: 0.963570107999999s, Queue time: 0.0028231143951416016s, Generation+async time: 0.9607469936048574s, Input tokens: 16, Generated tokens: 28, tokens/s: 45.797697305203975, generated tokens/s: 29.143989194220712.\n",
      "\u001b[36m(ServeReplica:default:VLLM:llama-3-8b-instruct pid=3147, ip=10.0.109.239)\u001b[0m INFO 2025-04-06 14:48:22,907 default_VLLM:llama-3-8b-instruct pxm9ni61 4d96b857-3f09-4a88-9f7d-0a7c362aa16d -- CALL /v1/chat/completions OK 970.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:32 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(_EngineBackgroundProcess pid=4023, ip=10.0.109.239)\u001b[0m INFO 04-06 14:48:43 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "# Initialize client\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"fake-key\")\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n",
    "    stream=True\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can observe our running service (deployments and metrics like QPS, latency, etc.) through the [Ray Dashboard](https://docs.ray.io/en/latest/ray-observability/getting-started.html)'s [Serve view](https://docs.ray.io/en/latest/ray-observability/getting-started.html#dash-serve-view):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/serve_dashboard.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "💡 For more advanced guides on topics like structured outputs (ex. json), vision LMs, multi-lora on shared base models, using other inference engines (ex. sglang), etc. fast model loading, etc. check out [more examples](https://docs.ray.io/en/latest/serve/llm/overview.html) and the [API reference](https://docs.ray.io/en/latest/serve/llm/api.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production\n",
    "\n",
    "Seamlessly integrate with your existing CI/CD pipelines by leveraging the Anyscale [CLI](https://docs.anyscale.com/reference/quickstart-cli) or [SDK](https://docs.anyscale.com/reference/quickstart-sdk) to run [reliable batch jobs](https://docs.anyscale.com/platform/jobs) and deploy [highly available services](https://docs.anyscale.com/platform/services). Given we've been developing in an environment that's almost identical to production (multinode cluster), this should drastically speed up our dev → prod velocity.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/cicd.png\" width=600>\n",
    "\n",
    "[Anyscale Jobs](https://docs.anyscale.com/platform/jobs/) ([API ref](https://docs.anyscale.com/reference/job-api/)) allows us to execute discrete workloads in production such as batch inference, embeddings generation, or model fine-tuning.\n",
    "- [define and manage](https://docs.anyscale.com/platform/jobs/manage-jobs) our Jobs in many different ways (CLI, Python SDK)\n",
    "- set up [queues](https://docs.anyscale.com/platform/jobs/job-queues) and [schedules](https://docs.anyscale.com/platform/jobs/schedules)\n",
    "- set up all the [observability, alerting, etc.](https://docs.anyscale.com/platform/jobs/monitoring-and-debugging) around our Jobs\n",
    "\n",
    "[Anyscale Services](https://docs.anyscale.com/platform/services/) ([API ref](https://docs.anyscale.com/reference/service-api/)) offers an extremely fault tolerant, scalable and optimized way to serve our Ray Serve applications.\n",
    "- we can [rollout and update](https://docs.anyscale.com/platform/services/update-a-service) our services with canary deployment (zero-downtime upgrades)\n",
    "- [monitor](https://docs.anyscale.com/platform/services/monitoring) our Services through a dedicated Service page, unified log viewer, tracing, set up alerts, etc.\n",
    "- scale a service (`num_replicas=auto`) and utilize replica compaction to consolidate nodes that are fractionally utilized\n",
    "- [head node fault tolerance](https://docs.anyscale.com/platform/services/production-best-practices#head-node-ft) (OSS Ray recovers from failed workers and replicas but not head node crashes)\n",
    "- serving [muliple applications](https://docs.anyscale.com/platform/services/multi-app) in a single Service\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/canary.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_EngineBackgroundProcess pid=4054, ip=10.0.150.21)\u001b[0m INFO 04-05 19:35:53 engine.py:293] Aborted request b96829c1-808c-4aa8-b1d5-304f98361560.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=73936)\u001b[0m INFO 2025-04-05 19:45:56,754 controller 73936 -- Downscaling Deployment(name='LLMRouter', app='default') from 2 to 0 replicas. Current ongoing requests: 0.00, current running replicas: 2.\n",
      "\u001b[36m(ServeController pid=73936)\u001b[0m INFO 2025-04-05 19:45:56,755 controller 73936 -- Removing 2 replicas from Deployment(name='LLMRouter', app='default').\n",
      "\u001b[36m(ServeController pid=73936)\u001b[0m INFO 2025-04-05 19:45:58,782 controller 73936 -- Replica(id='ur53jp9k', deployment='LLMRouter', app='default') is stopped.\n",
      "\u001b[36m(ServeController pid=73936)\u001b[0m INFO 2025-04-05 19:45:58,782 controller 73936 -- Replica(id='df9zz1e9', deployment='LLMRouter', app='default') is stopped.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# clean up\n",
    "rm -rf /mnt/cluster_storage/viggo  # clean up\n",
    "aws s3 rm $ANYSCALE_ARTIFACT_STORAGE/viggo --recursive --quiet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
